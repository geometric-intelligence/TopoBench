"""
DAWN temporal hypergraph dataset loader (TopoBench-compatible).

This module provides a PyTorch Geometric InMemoryDataset implementation that
parses raw node, simplex, and label files and produces a processed dataset
ready for graph learning tasks.
"""

import gzip
import os
import shutil
import warnings

import torch
from torch_geometric.data import Data, InMemoryDataset


class DawnDataset(InMemoryDataset):
    """
    TopoBench-compatible loader for the DAWN temporal hypergraph dataset from Cornell.

    Handles raw files (gz or txt) located in `raw_dir`:

    - `simplices.txt.gz` / `simplices.txt` : timestamped simplices
    - `nodes.txt.gz` / `nodes.txt`         : optional node features
    - `labels.txt.gz` / `labels.txt`       : optional node labels

    Produces a single processed file: `data.pt`.

    Parameters
    ----------
    root : str
        Root directory where the dataset should be saved.
    transform : callable, optional
        A function/transform that takes in a `Data` object and returns a
        transformed version.
    pre_transform : callable, optional
        A function/transform applied before saving the processed data.
    pre_filter : callable, optional
        A function that decides whether a `Data` object should be included.
    """

    def __init__(
        self, root: str, transform=None, pre_transform=None, pre_filter=None
    ):
        """
        Initialize the DAWN dataset.

        Parameters
        ----------
        root : str
            Root directory where the dataset should be saved.
        transform : callable, optional
            A function/transform that takes in a `Data` object and returns a
            transformed version.
        pre_transform : callable, optional
            A function/transform applied before saving the processed data.
        pre_filter : callable, optional
            A function that decides whether a `Data` object should be included.
        """
        super().__init__(root, transform, pre_transform, pre_filter)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self) -> list:
        """
        List of raw files expected in `raw_dir`.

        Returns
        -------
        list
            Names of raw files to check for extraction.
        """
        return ["simplices.txt", "nodes.txt", "labels.txt"]

    @property
    def processed_file_names(self) -> list:
        """
        List of processed files generated by this loader.

        Returns
        -------
        list
            Names of processed files.
        """
        return ["data.pt"]

    def download(self) -> None:
        """
        Extract gzipped raw files if they exist.

        Checks for `.gz` files in `raw_dir` and extracts them to `.txt`.
        """
        for fname in self.raw_file_names:
            gz_path = os.path.join(self.raw_dir, fname + ".gz")
            txt_path = os.path.join(self.raw_dir, fname)
            if os.path.exists(gz_path) and not os.path.exists(txt_path):
                print(f"Extracting {gz_path} → {txt_path}")
                with (
                    gzip.open(gz_path, "rb") as f_in,
                    open(txt_path, "wb") as f_out,
                ):
                    shutil.copyfileobj(f_in, f_out)

    def validate_and_normalize(
        self,
        num_nodes: int,
        x: torch.Tensor | None,
        y: torch.Tensor | None,
        edge_index: torch.LongTensor,
    ) -> tuple[int, torch.Tensor | None, torch.Tensor | None]:
        """
        Validate and normalize inputs.

        Ensures edge_index is non-empty, infers num_nodes from edges,
        validates x and y shapes, and converts 1-indexed labels to 0-indexed.

        Parameters
        ----------
        num_nodes : int
            Initial estimate of number of nodes.
        x : torch.Tensor, optional
            Node feature tensor of shape (num_nodes, num_features).
        y : torch.Tensor, optional
            Node label tensor of shape (num_nodes,).
        edge_index : torch.LongTensor
            Edge index tensor in COO format.

        Returns
        -------
        tuple[int, Optional[torch.Tensor], Optional[torch.Tensor]]
            Validated and normalized (num_nodes, x, y).

        Raises
        ------
        ValueError
            If edge_index is empty, if x/y dimensions are invalid, if x/y have
            fewer entries than required, or if labels contain negative values.
        """
        if edge_index.numel() == 0:
            raise ValueError(
                "Parsed edge_index is empty — no hyperedges found in simplices.txt."
            )

        max_node_id = int(edge_index[0].max().item())
        inferred_num_nodes = max_node_id + 1
        num_nodes = max(num_nodes, inferred_num_nodes)

        # Validate features
        if x is not None:
            if x.dim() != 2:
                raise ValueError(
                    f"Node features must be 2D (num_nodes x feat_dim), got {tuple(x.shape)}"
                )
            if x.size(0) < num_nodes:
                raise ValueError(
                    f"Node feature rows ({x.size(0)}) < inferred num_nodes ({num_nodes}) from simplices."
                )
            if x.size(0) > num_nodes:
                warnings.warn(
                    f"Node features ({x.size(0)}) contain more rows than inferred num_nodes ({num_nodes}). "
                    "Extra rows will be kept but may be ignored downstream.",
                    stacklevel=2,
                )

        # Validate and normalize labels
        if y is not None:
            if y.dim() != 1:
                raise ValueError(
                    f"Labels tensor must be 1D, got shape {tuple(y.shape)}"
                )
            if y.size(0) < num_nodes:
                raise ValueError(
                    f"Labels ({y.size(0)}) contain fewer entries than inferred num_nodes ({num_nodes})."
                )
            if y.size(0) > num_nodes:
                warnings.warn(
                    f"Labels ({y.size(0)}) contain more entries than inferred num_nodes ({num_nodes}). "
                    "Extra entries will be ignored.",
                    stacklevel=2,
                )
                y = y[:num_nodes]

            # Normalize common 1-indexed labels → 0-indexed
            y_min = int(y.min().item())
            if y_min == 1:
                warnings.warn(
                    "Detected labels starting at 1 — converting to 0-indexed labels.",
                    stacklevel=2,
                )
                y = y - 1

            if int(y.min().item()) < 0:
                raise ValueError(
                    "Labels contain negative values after normalization."
                )

            # Optional: remap arbitrary labels to contiguous 0..C-1
            # If labels are not contiguous, uncomment remapping below:
            # unique = torch.unique(y)
            # if unique[0] != 0 or unique[-1] != unique.numel() - 1:
            #     mapping = {int(v): i for i, v in enumerate(unique.tolist())}
            #     y = torch.tensor([mapping[int(v)] for v in y.tolist()], dtype=torch.long)

        return num_nodes, x, y

    def process(self) -> None:
        """
        Convert the raw DAWN dataset into a PyG `Data` object.

        Reads nodes, simplices, and optional labels, constructs edge_index
        and timestamps, applies pre-filter and pre-transform, and saves a single
        processed file.

        Raises
        ------
        ValueError
            If the number of nodes in features or labels does not match the
            number of nodes inferred from simplices.
        """
        # Ensure .gz files are extracted
        self.download()

        raw_nodes_path = os.path.join(self.raw_dir, "nodes.txt")
        raw_simplices_path = os.path.join(self.raw_dir, "simplices.txt")
        raw_labels_path = os.path.join(self.raw_dir, "labels.txt")

        # Load node features
        if os.path.exists(raw_nodes_path):
            with open(raw_nodes_path) as f:
                node_feats = [
                    [1.0]
                    if len(line.strip().split()) == 1
                    else [float(x) for x in line.strip().split()[1:]]
                    for line in f
                ]
        else:
            node_feats = []
            print("No nodes.txt found. Using default features for nodes.")

        x = torch.tensor(node_feats, dtype=torch.float) if node_feats else None
        num_nodes = x.size(0) if x is not None else 0

        # Load labels if present
        if os.path.exists(raw_labels_path):
            with open(raw_labels_path) as f:
                labels = [int(line.strip()) for line in f]
            y = torch.tensor(labels, dtype=torch.long)
        else:
            y = None

        # Load simplices
        edge_index = []
        edge_timestamps = []
        with open(raw_simplices_path) as f:
            for e_idx, line in enumerate(f):
                parts = line.strip().split()
                timestamp = float(parts[0])
                node_ids = [int(x) for x in parts[1:]]
                if not num_nodes:
                    num_nodes = max(node_ids) + 1
                edge_index.extend([[n, e_idx] for n in node_ids])
                edge_timestamps.append(timestamp)

        edge_index = (
            torch.tensor(edge_index, dtype=torch.long).t().contiguous()
        )
        edge_timestamps = torch.tensor(edge_timestamps, dtype=torch.float)

        # Validate and normalize using helper method
        num_nodes, x, y = self.validate_and_normalize(
            num_nodes, x, y, edge_index
        )

        if x is None:
            x = torch.ones((num_nodes, 1), dtype=torch.float)

        data = Data(
            x=x,
            y=y,
            edge_index=edge_index,
            edge_timestamps=edge_timestamps,
            num_nodes=num_nodes,
        )

        # Apply pre-filter and pre-transform if provided
        if self.pre_filter is not None and not self.pre_filter(data):
            return
        if self.pre_transform is not None:
            data = self.pre_transform(data)

        # Save processed data
        torch.save(self.collate([data]), self.processed_paths[0])
        print(f"Processed DAWN dataset saved to {self.processed_paths[0]}")
        return
