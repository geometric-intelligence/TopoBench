{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Disk Transductive Learning: Extended Context Batching\n",
    "\n",
    "Enhance your existing node samplers with **structure-aware context expansion**!\n",
    "\n",
    "**What you'll learn:**\n",
    "- üîß How to enhance ANY existing node sampler\n",
    "- üìà Controlled context expansion for complete structures\n",
    "- üéØ Core vs context nodes (for selective loss)\n",
    "- üîÑ Backward compatibility with existing samplers\n",
    "\n",
    "**üìö Prerequisites:** Complete `tutorial_ondisk_transductive_intro.ipynb` first!  \n",
    "That tutorial covers: transductive learning, node samplers (Random, ClusterAware), cluster-aware sampling, structure loss problem, and on-disk indexing.\n",
    "\n",
    "**‚è±Ô∏è Time:** 15-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Extended Context Batching?\n",
    "\n",
    "**Building on Tutorial 1:** We learned that cluster-aware node samplers lose structures at boundaries.\n",
    "\n",
    "**This approach: Enhance, don't replace!**\n",
    "- Keep your existing node sampler (Louvain, METIS, etc.)\n",
    "- **Add context nodes** to complete structures\n",
    "- **Control expansion** with max_expansion_ratio\n",
    "- **Result:** 95-100% completeness with modest memory increase\n",
    "\n",
    "**When to use:**\n",
    "- Have existing node sampler you like\n",
    "- Want **backward compatibility**\n",
    "- Can afford 20-50% memory increase per batch\n",
    "- Using enumerable structures (cliques, cycles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Topological Structures Benefit?\n",
    "\n",
    "### ‚úÖ Benefits from Extended Context:\n",
    "- **SimplicialCliqueLifting**: Cliques/triangles enumerated from graph\n",
    "- **CellCycleLifting**: Cycles discovered in graph\n",
    "- **Any enumerable structure** from the graph topology\n",
    "\n",
    "### ‚ùå Does NOT benefit:\n",
    "- **HypergraphKHopLifting**: Neighborhoods generated per node (already complete)\n",
    "- **KernelLifting**: Structures from kernels, not graph enumeration\n",
    "- **Feature-based** transforms that don't rely on graph structure\n",
    "\n",
    "**Why?** Extended context helps when structures **cross batch boundaries**. If structures are generated per-node (like k-hop), they're already complete within the sampled nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Building the On-Disk Index](#index)\n",
    "3. [Using Existing Node Samplers](#samplers)\n",
    "4. [Extended Context Enhancement](#context)\n",
    "5. [Core vs Context Nodes](#core-context)\n",
    "6. [Training with TBModel](#training)\n",
    "7. [Do Liftings Need Modification?](#liftings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "Following TopoBench conventions, we define dataset and loader classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Dataset Class (TopoBench Style)\n",
    "\n",
    "**Important: Why `InMemoryDataset` is fine here**\n",
    "\n",
    "You might wonder: \"If we're doing on-disk processing, why use `InMemoryDataset`?\"\n",
    "\n",
    "**Answer:** We keep the **graph** in memory, but **structures** on disk!\n",
    "\n",
    "- **Graph data** (nodes, edges, features): ~50-200 MB for 8K nodes ‚Üí **fits in RAM** ‚úÖ\n",
    "- **Topological structures** (triangles, etc.): 1-10 GB ‚Üí **stored on disk** ‚úÖ\n",
    "\n",
    "**Why this works:**\n",
    "1. The base graph must be in memory anyway (for subgraph extraction)\n",
    "2. The **bottleneck** is structures, not the graph itself\n",
    "3. Our on-disk index handles the structures (SQLite)\n",
    "4. Result: Constant memory training on large graphs!\n",
    "\n",
    "**Would on-demand graph loading help?** No, because:\n",
    "- We need full graph in memory to extract subgraphs\n",
    "- Preprocessor needs full graph to query edges\n",
    "- Graph itself is not the memory problem\n",
    "- Structures are the problem (and we solve that!)\n",
    "\n",
    "**Bottom line:** `InMemoryDataset` for graph + on-disk index for structures = perfect combo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset Class (TopoBench Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunityGraphDataset(InMemoryDataset):\n",
    "    \"\"\"Large graph with clear community structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, root, name, parameters: DictConfig):\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        super().__init__(root)\n",
    "        \n",
    "        out = fs.torch_load(self.processed_paths[0])\n",
    "        if len(out) == 4:\n",
    "            data, self.slices, self.sizes, data_cls = out\n",
    "            self.data = data_cls.from_dict(data) if isinstance(data, dict) else data\n",
    "        else:\n",
    "            data, self.slices, self.sizes = out\n",
    "            self.data = data\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return \"data.pt\"\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Generate graph with community structure.\"\"\"\n",
    "        from networkx.generators.community import stochastic_block_model\n",
    "        \n",
    "        n = self.parameters.num_nodes\n",
    "        num_communities = self.parameters.num_communities\n",
    "        nodes_per_comm = n // num_communities\n",
    "        sizes = [nodes_per_comm] * num_communities\n",
    "        \n",
    "        # High intra-community, low inter-community edges\n",
    "        p_in = 0.3\n",
    "        p_out = 0.02\n",
    "        probs = [[p_in if i == j else p_out for j in range(num_communities)] \n",
    "                 for i in range(num_communities)]\n",
    "        \n",
    "        G = stochastic_block_model(sizes, probs, seed=42)\n",
    "        \n",
    "        # Convert to PyG Data\n",
    "        edges = list(G.edges())\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "        edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)\n",
    "        \n",
    "        x = torch.randn(n, self.parameters.num_features)\n",
    "        y = torch.randint(0, self.parameters.num_classes, (n,))\n",
    "        \n",
    "        # Transductive splits\n",
    "        train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n, dtype=torch.bool)\n",
    "        \n",
    "        train_mask[:int(0.6 * n)] = True\n",
    "        val_mask[int(0.6 * n):int(0.8 * n)] = True\n",
    "        test_mask[int(0.8 * n):] = True\n",
    "        \n",
    "        data = Data(\n",
    "            x=x, edge_index=edge_index, y=y, num_nodes=n,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask\n",
    "        )\n",
    "        \n",
    "        self.data, self.slices = self.collate([data])\n",
    "        fs.torch_save(\n",
    "            (self._data.to_dict(), self.slices, {}, self._data.__class__),\n",
    "            self.processed_paths[0]\n",
    "        )\n",
    "\n",
    "print(\"‚úì Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loader Class (TopoBench Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunityGraphLoader(AbstractLoader):\n",
    "    \"\"\"Loader for community-structured graphs.\"\"\"\n",
    "    \n",
    "    def __init__(self, parameters: DictConfig):\n",
    "        super().__init__(parameters)\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        return CommunityGraphDataset(\n",
    "            str(self.root_data_dir),\n",
    "            self.parameters.data_name,\n",
    "            self.parameters\n",
    "        )\n",
    "\n",
    "print(\"‚úì Loader class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.io import fs\n",
    "import lightning as pl\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# TopoBench imports\n",
    "from topobench.data.loaders.base import AbstractLoader\n",
    "from topobench.data.preprocessor import OnDiskTransductivePreprocessor\n",
    "from topobench.dataloader import TBDataloader\n",
    "from topobench.model import TBModel\n",
    "from topobench.nn.backbones.simplicial import SCCNNCustom\n",
    "from topobench.nn.readouts.simplicial_readout import SimplicialReadout\n",
    "from topobench.loss import TBLoss\n",
    "from topobench.optimizer import TBOptimizer\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = OmegaConf.create({\n",
    "    \"data_dir\": \"./data/\",\n",
    "    \"data_name\": \"CommunityGraph\",\n",
    "    \"num_nodes\": 8000,\n",
    "    \"num_communities\": 8,\n",
    "    \"num_features\": 32,\n",
    "    \"num_classes\": 4\n",
    "})\n",
    "\n",
    "# Load dataset\n",
    "loader = CommunityGraphLoader(config)\n",
    "dataset, _ = loader.load()\n",
    "graph_data = dataset[0]\n",
    "\n",
    "print(f\"\\n‚úì Graph loaded: {graph_data.num_nodes:,} nodes\")\n",
    "print(f\"  Edges: {graph_data.edge_index.size(1):,}\")\n",
    "print(f\"  Communities: 8 (clear cluster structure)\")\n",
    "print(f\"  Train nodes: {graph_data.train_mask.sum().item():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='samplers'></a>\n",
    "## 3. Using Existing Node Samplers\n",
    "\n",
    "**Key Feature:** Extended context works with **any node sampler from Tutorial 1**!\n",
    "\n",
    "**Recap from Tutorial 1:** Node samplers yield batches of node IDs.  \n",
    "Options: `louvain`, `metis`, `leiden`, `label_propagation`, or custom samplers\n",
    "\n",
    "Let's use the `ClusterAwareNodeSampler` with Louvain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure transform\n",
    "transforms_config = OmegaConf.create({\n",
    "    \"clique_lifting\": {\n",
    "        \"transform_type\": \"lifting\",\n",
    "        \"transform_name\": \"SimplicialCliqueLifting\",\n",
    "        \"complex_dim\": 2\n",
    "    }\n",
    "})\n",
    "\n",
    "# Create preprocessor (no need to build_index manually - it's automatic!)\n",
    "preprocessor = OnDiskTransductivePreprocessor(\n",
    "    graph_data=graph_data,\n",
    "    data_dir=\"./index/extended_context_demo\",\n",
    "    transforms_config=transforms_config,\n",
    "    max_structure_size=3\n",
    ")\n",
    "\n",
    "print(\"‚úì Preprocessor created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataloader'></a>\n",
    "## 3. Load Dataset Splits (TopoBench Style)\n",
    "\n",
    "**High-level API:** Exactly like inductive learning!\n",
    "\n",
    "**What happens under the hood:**\n",
    "1. Builds structure index (if not exists) ‚Üí saved to disk\n",
    "2. Creates train/val/test datasets based on masks\n",
    "3. Each dataset wraps a loader for extended context sampling\n",
    "\n",
    "**Result:** Train, val, test datasets ready for TBDataloader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split configuration (like inductive learning!)\n",
    "split_config = OmegaConf.create({\n",
    "    \"strategy\": \"extended_context\",\n",
    "    \"nodes_per_batch\": 1000,       # Core nodes per batch\n",
    "    \"max_expansion_ratio\": 1.5,    # Allow up to 50% expansion\n",
    "    \"sampler_method\": \"louvain\",   # Community detection\n",
    "})\n",
    "\n",
    "print(\"üóÑÔ∏è Loading dataset splits (builds index if needed)...\\n\")\n",
    "\n",
    "# Load splits - EXACTLY like inductive learning!\n",
    "train, val, test = preprocessor.load_dataset_splits(split_config)\n",
    "\n",
    "print(f\"\\n‚úì Dataset splits loaded!\")\n",
    "print(f\"  Train: {len(train)} batches\")\n",
    "print(f\"  Val: {len(val)} batches\")\n",
    "print(f\"  Test: {len(test)} batches\")\n",
    "print(f\"  Strategy: Extended context batching\")\n",
    "\n",
    "# Inspect a sample batch\n",
    "sample_batch = next(iter(train))\n",
    "print(f\"\\nüì¶ Sample training batch:\")\n",
    "print(f\"  Total nodes: {sample_batch.num_nodes}\")\n",
    "print(f\"  Core nodes: {sample_batch.core_mask.sum().item()}\")\n",
    "print(f\"  Context nodes: {sample_batch.num_nodes - sample_batch.core_mask.sum().item()}\")\n",
    "print(f\"  Expansion ratio: {sample_batch.expansion_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datamodule (TopoBench Style)\n",
    "\n",
    "**Exactly like inductive learning:** Use TBDataloader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datamodule - EXACTLY like inductive learning!\n",
    "datamodule = TBDataloader(\n",
    "    dataset_train=train,\n",
    "    dataset_val=val,\n",
    "    dataset_test=test,\n",
    "    batch_size=1,  # Already batched by dataset\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"‚úì Datamodule created (TopoBench style)\")\n",
    "print(\"  This is IDENTICAL to inductive learning!\")\n",
    "print(\"  Same API, same workflow, just different sampling strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "## 5. Training with TBModel\n",
    "\n",
    "**Integration:** Works seamlessly with TopoBench's TBModel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"\\nüì¶ Sample Batch Analysis:\")\n",
    "print(f\"  Total nodes (with context): {sample_batch.num_nodes}\")\n",
    "print(f\"  Core nodes (sampled): {sample_batch.core_mask.sum().item()}\")\n",
    "print(f\"  Context nodes (added): {sample_batch.num_nodes - sample_batch.core_mask.sum().item()}\")\n",
    "print(f\"  Expansion ratio: {sample_batch.expansion_ratio:.2f}x\")\n",
    "\n",
    "print(f\"\\n  Edges: {sample_batch.edge_index.size(1)}\")\n",
    "print(f\"  Structures: {sample_batch.num_structures}\")\n",
    "\n",
    "print(f\"\\n  Core mask: {sample_batch.core_mask.sum().item()} True values\")\n",
    "print(f\"  üí° Use core_mask to compute loss only on sampled nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "## 6. Training with TBModel\n",
    "\n",
    "**Integration:** Works seamlessly with TopoBench's TBModel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "HIDDEN_DIM = 64\n",
    "OUT_CHANNELS = 4\n",
    "IN_CHANNELS = 32\n",
    "\n",
    "model = TBModel(\n",
    "    backbone=SCCNNCustom(\n",
    "        in_channels_all=(IN_CHANNELS, HIDDEN_DIM, HIDDEN_DIM),\n",
    "        hidden_channels_all=(HIDDEN_DIM, HIDDEN_DIM, HIDDEN_DIM),\n",
    "        conv_order=1,\n",
    "        sc_order=2,\n",
    "        n_layers=2\n",
    "    ),\n",
    "    readout=SimplicialReadout(\n",
    "        HIDDEN_DIM, OUT_CHANNELS, task_level=\"node\"\n",
    "    ),\n",
    "    loss=TBLoss(\n",
    "        dataset_loss={\"task\": \"classification\", \"loss_type\": \"cross_entropy\"}\n",
    "    ),\n",
    "    optimizer=TBOptimizer(\n",
    "        optimizer_id=\"Adam\", parameters={\"lr\": 0.01}\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úì TBModel created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - EXACTLY like inductive learning!\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training with extended context...\\n\")\n",
    "print(\"  Note: In production, consider using core_mask for loss\")\n",
    "print(\"  For this demo, we train on all nodes\\n\")\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"  Trained on {graph_data.train_mask.sum().item():,} train nodes\")\n",
    "print(f\"  Validated on {graph_data.val_mask.sum().item():,} val nodes\")\n",
    "print(\"  Standard node sampler + extended context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='liftings'></a>\n",
    "## 6. Do Liftings Need Modification?\n",
    "\n",
    "**Short Answer:** No! Existing liftings work as-is.\n",
    "\n",
    "**What's different:**\n",
    "- **Index building:** Enumerates structures from full graph once\n",
    "- **Batch-time:** Lifting applied to expanded mini-batch (as normal)\n",
    "- **Advantage:** Structures queried from index + context nodes added\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topobench.transforms.liftings.graph2simplicial import SimplicialCliqueLifting\n",
    "\n",
    "# Your existing lifting works unchanged!\n",
    "lifting = SimplicialCliqueLifting(complex_dim=2)\n",
    "\n",
    "print(\"‚úì Existing SimplicialCliqueLifting works as-is\")\n",
    "print(\"\\n  What happens:\")\n",
    "print(\"  1. Sampler provides core nodes from one community\")\n",
    "print(\"  2. Extended context adds neighbors for complete structures\")\n",
    "print(\"  3. Lifting applied to expanded batch (standard process)\")\n",
    "print(\"  4. Result: More complete structures in mini-batch\")\n",
    "print(\"\\n  No lifting changes needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "1. ‚úÖ **On-disk indexing:** SQLite database, not RAM\n",
    "2. ‚úÖ **Backward compatible:** Works with existing node samplers\n",
    "3. ‚úÖ **Controlled expansion:** Set `max_expansion_ratio`\n",
    "4. ‚úÖ **Core vs context:** Distinguish sampled vs added nodes\n",
    "5. ‚úÖ **Which structures benefit:** Enumerable structures (cliques, cycles)\n",
    "6. ‚úÖ **No lifting changes:** Existing liftings work as-is\n",
    "\n",
    "**When to use this approach:**\n",
    "- ‚úÖ You have existing node samplers you like (Louvain, METIS, etc.)\n",
    "- ‚úÖ Using SimplicialCliqueLifting or cycle-based liftings\n",
    "- ‚úÖ Want better structure completeness without changing workflow\n",
    "- ‚úÖ Can afford 20-50% memory increase per batch\n",
    "\n",
    "**When NOT to use:**\n",
    "- ‚ùå Using k-hop or kernel-based liftings (structures not enumerable)\n",
    "- ‚ùå Memory is extremely constrained\n",
    "- ‚ùå Transforms don't rely on graph structure enumeration\n",
    "\n",
    "**Alternative:** See `tutorial_ondisk_transductive_structure_centric.ipynb` for a structure-first approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "preprocessor.close()\n",
    "print(\"‚úì Tutorial complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
