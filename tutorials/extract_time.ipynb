{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "today = date.today()\n",
    "api = wandb.Api()\n",
    "\n",
    "# # Find all csv files in the current directory\n",
    "csv_files = glob.glob(\"*.csv\")\n",
    "# # Collect all the names of the csv files without the extension\n",
    "csv_names = [csv_file[:-4] for csv_file in csv_files]\n",
    "project_name = \"graph_baselines_graph_baselines\"  # rebutal_CWN_ogbg-molhiv fix_gnn_rebuttal_cell_NCI109  fix_gnn_rebuttal_cell_MUTAG fix_gnn_rebuttal_cell_PROTEINS fix_gnn_rebuttal_cell_ZINC\n",
    "user = \"levsap\" #\"telyatnikov_sap\"\n",
    "\n",
    "if project_name not in csv_names:\n",
    "    runs = api.runs(f\"{user}/{project_name}\")\n",
    "\n",
    "    summary_list, config_list, name_list = [], [], []\n",
    "    for run in runs:\n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files\n",
    "        summary_list.append(run.summary._json_dict)\n",
    "\n",
    "        # .config contains the hyperparameters.\n",
    "        #  We remove special values that start with _.\n",
    "        config_list.append(\n",
    "            {k: v for k, v in run.config.items() if not k.startswith(\"_\")}\n",
    "        )\n",
    "\n",
    "        # .name is the human-readable name of the run.\n",
    "        name_list.append(run.name)\n",
    "\n",
    "    runs_df = pd.DataFrame(\n",
    "        {\"summary\": summary_list, \"config\": config_list, \"name\": name_list}\n",
    "    )\n",
    "\n",
    "    runs_df.to_csv(f\"{project_name}.csv\")\n",
    "else:\n",
    "    runs_df = pd.read_csv(f\"{project_name}.csv\", index_col=0)\n",
    "\n",
    "    for row in runs_df.iloc:\n",
    "        row[\"summary\"] = ast.literal_eval(row[\"summary\"])\n",
    "        row[\"config\"] = ast.literal_eval(row[\"config\"])\n",
    "\n",
    "\n",
    "for row in runs_df.iloc:\n",
    "    row[\"summary\"].update(row[\"config\"])\n",
    "\n",
    "lst = [i[\"summary\"] for i in runs_df.iloc]\n",
    "df = pd.DataFrame.from_dict(lst)\n",
    "\n",
    "df_init = df.copy()\n",
    "\n",
    "# Get average epoch run time\n",
    "#df[\"epoch_run_time\"] = df[\"_runtime\"] / df[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"epoch_run_time\"] = df[\"_runtime\"] / df[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7679, 104)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(df, column_to_normalize):\n",
    "    # Use json_normalize to flatten the nested dictionaries into separate columns\n",
    "    flattened_df = pd.json_normalize(df[column_to_normalize])\n",
    "    # Rename columns to include 'nested_column' prefix\n",
    "    flattened_df.columns = [\n",
    "        f\"{column_to_normalize}.{col}\" for col in flattened_df.columns\n",
    "    ]\n",
    "    # Concatenate the flattened DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([df, flattened_df], axis=1)\n",
    "    # Get new columns names\n",
    "    new_columns = flattened_df.columns\n",
    "    # Drop the original nested column if needed\n",
    "    result_df.drop(column_to_normalize, axis=1, inplace=True)\n",
    "    return result_df, new_columns\n",
    "\n",
    "\n",
    "# Config columns to normalize\n",
    "columns_to_normalize = [\"model\", \"dataset\", \"callbacks\", \"paths\"]\n",
    "\n",
    "# Keep track of config columns added\n",
    "config_columns = []\n",
    "for column in columns_to_normalize:\n",
    "    df, columns = normalize_column(df, column)\n",
    "    config_columns.extend(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns: ['callbacks.model_checkpoint.dirpath']\n"
     ]
    }
   ],
   "source": [
    "# Remove columns that are not needed (we shouldn't vary them or their variation is not interesting)\n",
    "remove_cols = [\n",
    "    #\"dataset.transforms.data_manipulations.selected_fields\",\n",
    "    \"callbacks.model_checkpoint.dirpath\", 'model.feature_encoder.selected_dimensions','callbacks.model_checkpoint.dirpath',\n",
    "]\n",
    "removed_columns = []\n",
    "for remove_col in remove_cols:\n",
    "    if remove_col in df.columns:\n",
    "        df = df.drop([remove_col], axis=1)\n",
    "        removed_columns.append(remove_col)\n",
    "\n",
    "print(\"Removed columns:\", removed_columns)\n",
    "\n",
    "# Ensure that removed columns are not present in config_columns\n",
    "config_columns = [col for col in config_columns if col not in removed_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_epoch',\n",
       " 'best_epoch/checkpoint',\n",
       " 'best_epoch/train/accuracy',\n",
       " 'best_epoch/train/auroc',\n",
       " 'best_epoch/train/average_precision',\n",
       " 'best_epoch/train/cohen_kappa',\n",
       " 'best_epoch/train/f1_score',\n",
       " 'best_epoch/train/jaccard',\n",
       " 'best_epoch/train/loss',\n",
       " 'best_epoch/train/mcc',\n",
       " 'best_epoch/train/precision',\n",
       " 'best_epoch/train/recall',\n",
       " 'best_epoch/val/accuracy',\n",
       " 'best_epoch/val/auroc',\n",
       " 'best_epoch/val/average_precision',\n",
       " 'best_epoch/val/cohen_kappa',\n",
       " 'best_epoch/val/f1_score',\n",
       " 'best_epoch/val/jaccard',\n",
       " 'best_epoch/val/loss',\n",
       " 'best_epoch/val/mcc',\n",
       " 'best_epoch/val/precision',\n",
       " 'best_epoch/val/recall',\n",
       " 'best_epoch/train/mae',\n",
       " 'best_epoch/train/mse',\n",
       " 'best_epoch/train/r2',\n",
       " 'best_epoch/val/mae',\n",
       " 'best_epoch/val/mse',\n",
       " 'best_epoch/val/r2',\n",
       " 'callbacks.best_epoch_metrics.mode',\n",
       " 'callbacks.best_epoch_metrics.monitor',\n",
       " 'callbacks.best_epoch_metrics._target_']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in list(df.columns) if 'best' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15      0.0\n",
       "16      0.0\n",
       "17      0.0\n",
       "18      0.0\n",
       "19      0.0\n",
       "       ... \n",
       "7514    4.0\n",
       "7515    4.0\n",
       "7516    4.0\n",
       "7517    4.0\n",
       "7518    4.0\n",
       "Name: epoch, Length: 502, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First get rid of the runs that have less than 50 epochs (minimum amount of epochs is 50)\n",
    "MIN_EPOCHS = 50\n",
    "df = df[df[\"epoch\"] >= MIN_EPOCHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with model.backbone._target_ = nan is 1\n",
      "Number of rows with callbacks.early_stopping.monitor = nan is 0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Number of rows with model.backbone._target_ = nan is {sum(df['model.backbone._target_'].isna())}\"\n",
    ")\n",
    "# Drop na values if there are\n",
    "df = df.dropna(subset=[\"model.backbone._target_\"])\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Drop rows that 'callbacks.early_stopping.monitor' isna\n",
    "print(\n",
    "    f\"Number of rows with callbacks.early_stopping.monitor = nan is {sum(df['callbacks.early_stopping.monitor'].isna())}\"\n",
    ")\n",
    "\n",
    "# print(\"Because of SCCN and CWN false runs there were 96 such runs on 13/03/24\")\n",
    "\n",
    "df = df.dropna(subset=[\"callbacks.early_stopping.monitor\"])\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Get correct names for the models\n",
    "df[\"model.backbone._target_\"] = df[\"model.backbone._target_\"].apply(\n",
    "    lambda x: x.split(\".\")[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SANN'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"model.backbone._target_\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention the columns: model.feature_encoder.in_channels, has issues with unique values\n",
      "Attention the columns: model.feature_encoder.dataset_in_channels, has issues with unique values\n",
      "Attention the columns: model.feature_encoder.selected_dimensions, has issues with unique values\n",
      "Attention the columns: dataset.parameters.num_features, has issues with unique values\n",
      "Attention the columns: callbacks.model_checkpoint.dirpath, has issues with unique values\n",
      "Unique values for each config column for SANN on ogbg-molhiv:\n",
      "model.readout.max_hop: [5. 4. 2.]\n",
      "\n",
      "model.readout.hidden_dim: [128. 256.]\n",
      "\n",
      "model.backbone.max_hop: [5. 4. 2.]\n",
      "\n",
      "model.backbone.in_channels: [128. 256.]\n",
      "\n",
      "model.backbone.hidden_channels: [128. 256.]\n",
      "\n",
      "model.feature_encoder.max_hop: [5. 4. 2.]\n",
      "\n",
      "model.feature_encoder.out_channels: [128. 256.]\n",
      "\n",
      "model.feature_encoder.proj_dropout: [0.   0.25]\n",
      "\n",
      "model.backbone_wrapper.max_hop: [5. 4. 2.]\n",
      "\n",
      "model.backbone_wrapper.out_channels: [128. 256.]\n",
      "\n",
      "dataset.split_params.data_seed: [0. 3. 5. 7. 9.]\n",
      "\n",
      "paths.output_dir: ['/home/levtel/projects/TopoBenchmark/logs/train/runs/2025-09-18_00-43-57'\n",
      " '/home/levtel/projects/TopoBenchmark/logs/train/runs/2025-09-18_00-44-02'\n",
      " '/home/levtel/projects/TopoBenchmark/logs/train/runs/2025-09-18_00-44-07'\n",
      " ...\n",
      " '/home/levtel/projects/TopoBenchmark/logs/train/runs/2025-09-19_06-57-47'\n",
      " '/home/levtel/projects/TopoBenchmark/logs/train/runs/2025-09-19_06-58-39'\n",
      " '/home/levtel/projects/TopoBenchmark/logs/train/runs/2025-09-19_06-59-11']\n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Identify unique models in DataFrame\n",
    "unique_models = df[\"model.backbone._target_\"].unique()\n",
    "\n",
    "# Identify unique datasets in DataFrame\n",
    "unique_datasets = df[\"dataset.loader.parameters.data_name\"].unique()\n",
    "\n",
    "\n",
    "collected_results = defaultdict(dict)\n",
    "collected_results_time = defaultdict(dict)\n",
    "collected_results_time_run = defaultdict(dict)\n",
    "\n",
    "collected_aggregated_results = defaultdict(dict)\n",
    "collected_non_aggregated_results = defaultdict(dict)\n",
    "\n",
    "# Got over each dataset and model and find the best result\n",
    "for dataset in unique_datasets:\n",
    "    for model in unique_models:\n",
    "        # Get the subset of the DataFrame for the current dataset and model\n",
    "        subset = df[\n",
    "            (df[\"dataset.loader.parameters.data_name\"] == dataset)\n",
    "            & (df[\"model.backbone._target_\"] == model)\n",
    "        ]\n",
    "\n",
    "        if subset.empty:\n",
    "            print(\"---------\")\n",
    "            print(f\"No results for {model} on {dataset}\")\n",
    "            print(\"---------\")\n",
    "            continue\n",
    "        # Suppress all warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        subset[\"Model\"] = model\n",
    "        warnings.filterwarnings(\"default\")\n",
    "\n",
    "        def get_metric(df):\n",
    "            metric_ = df[\"callbacks.early_stopping.monitor\"].unique()\n",
    "            assert len(metric_) == 1, \"There should be only one metric to optimize\"\n",
    "            metric = metric_[0]\n",
    "            return metric.split(\"/\")[-1]\n",
    "\n",
    "        # Cols to get statistics later\n",
    "        # TODO: log maximum validation value for optimized metric\n",
    "        performance_cols = [f\"test/{get_metric(subset)}\"]\n",
    "\n",
    "        # Get the unique values for each config column\n",
    "        unique_colums_values = {}\n",
    "        for col in config_columns:\n",
    "            try:\n",
    "                unique_colums_values[col] = subset[col].unique()\n",
    "            except:\n",
    "                print(f\"Attention the columns: {col}, has issues with unique values\")\n",
    "\n",
    "        # Keep only those keys that have more than one unique value\n",
    "        unique_colums_values = {\n",
    "            k: v for k, v in unique_colums_values.items() if len(v) > 1\n",
    "        }\n",
    "\n",
    "        # Print the unique values for each config column\n",
    "\n",
    "        print(f\"Unique values for each config column for {model} on {dataset}:\")\n",
    "        for col, unique in unique_colums_values.items():\n",
    "            print(f\"{col}: {unique}\")\n",
    "            print()\n",
    "        print(\"---------\")\n",
    "\n",
    "        # Check if \"special colums\" are not in unique_colums_values\n",
    "        # For example dataset.parameters.data_seed should not be in aggregation columns\n",
    "        # If it is, then we should remove it from the list\n",
    "        special_columns = [\"dataset.parameters.data_seed\"]\n",
    "\n",
    "        for col in special_columns:\n",
    "            if col in unique_colums_values:\n",
    "                unique_colums_values.pop(col)\n",
    "\n",
    "        # Obtain the aggregation columns\n",
    "        aggregation_columns = [\"Model\"] + list(unique_colums_values.keys())\n",
    "\n",
    "        collected_non_aggregated_results[dataset][model] = {\n",
    "            \"df\": subset.copy(),\n",
    "            \"aggregation_columns\": aggregation_columns,\n",
    "            \"performance_cols\": performance_cols,\n",
    "        }\n",
    "\n",
    "        # Aggregate the subset by the aggregation columns and get the best result for each group\n",
    "        aggregated = subset.groupby(aggregation_columns).agg(\n",
    "            {col: [\"mean\", \"std\"] for col in performance_cols}\n",
    "        )\n",
    "\n",
    "        # Go from MultiIndex to Index\n",
    "        aggregated = aggregated.reset_index()\n",
    "\n",
    "        assert (\n",
    "            len(subset[\"callbacks.early_stopping.mode\"].unique()) == 1\n",
    "        ), \"There should be only one mode for early stopping\"\n",
    "        # Identify the mode of the early stopping mode\n",
    "\n",
    "        if subset[\"callbacks.early_stopping.mode\"].unique()[0] == \"max\":\n",
    "            ascending = False\n",
    "            final_best_ = aggregated.sort_values(\n",
    "                by=(f\"test/{get_metric(subset)}\", \"mean\"), ascending=ascending\n",
    "            ).head(1)\n",
    "            final_best_ = (final_best_ * 100).round(2)\n",
    "        else:\n",
    "            ascending = True\n",
    "            final_best_ = aggregated.sort_values(\n",
    "                by=(f\"test/{get_metric(subset)}\", \"mean\"), ascending=ascending\n",
    "            ).head(1)\n",
    "\n",
    "        collected_results[dataset][model] = {\n",
    "            \"mean\": final_best_[(f\"test/{get_metric(subset)}\", \"mean\")].values[0],\n",
    "            \"std\": final_best_[(f\"test/{get_metric(subset)}\", \"std\")].values[0],\n",
    "        }\n",
    "\n",
    "        # Get average epoch run time\n",
    "        collected_results_time[dataset][model] = {\n",
    "            \"mean\": subset['AvgTime/train_epoch_mean'].mean(),\n",
    "            \"std\": subset['AvgTime/train_epoch_mean'].std(),\n",
    "        }\n",
    "\n",
    "        collected_results_time_run[dataset][model] = {\n",
    "            \"mean\": subset[\"_runtime\"].mean(),\n",
    "            \"std\": subset[\"_runtime\"].std(),\n",
    "        }\n",
    "\n",
    "        collected_aggregated_results[dataset][model] = aggregated.sort_values(\n",
    "            by=(f\"test/{get_metric(subset)}\", \"mean\"), ascending=ascending\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_results_time\n",
    "# Convert nested dictionary to DataFrame\n",
    "nested_dict = dict(collected_results_time)\n",
    "result_dict = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        (i, j): nested_dict[i][j]\n",
    "        for i in nested_dict\n",
    "        for j in nested_dict[i].keys()\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "\n",
    "result_dict = result_dict.round(2)\n",
    "result_dict[\"performance\"] = result_dict.apply(\n",
    "    lambda x: f\"{x['mean']} ± {x['std']}\", axis=1\n",
    ")\n",
    "result_dict = result_dict.drop([\"mean\", \"std\"], axis=1)\n",
    "\n",
    "# Reset multiindex\n",
    "result_dict = result_dict.reset_index()\n",
    "# rename columns\n",
    "result_dict.columns = [\"Dataset\", \"Model\", \"Average Time per Epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Time per Epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ogbg-molhiv</td>\n",
       "      <td>SANN</td>\n",
       "      <td>15.75 ± 2.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dataset Model Average Time per Epoch\n",
       "0  ogbg-molhiv  SANN           15.75 ± 2.78"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataset</th>\n",
       "      <th>ogbg-molhiv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SANN</th>\n",
       "      <td>15.75 ± 2.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dataset   ogbg-molhiv\n",
       "Model                \n",
       "SANN     15.75 ± 2.78"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict.pivot_table(\n",
    "    index=\"Model\", columns=\"Dataset\", values=\"Average Time per Epoch\", aggfunc=\"first\"\n",
    ")[['ogbg-molhiv']] #[['MUTAG', 'NCI1','NCI109','PROTEINS','ZINC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_results_time_run\n",
    "# Convert nested dictionary to DataFrame\n",
    "nested_dict = dict(collected_results_time_run)\n",
    "result_dict = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        (i, j): nested_dict[i][j]\n",
    "        for i in nested_dict\n",
    "        for j in nested_dict[i].keys()\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "\n",
    "result_dict = result_dict.round(2)\n",
    "result_dict[\"performance\"] = result_dict.apply(\n",
    "    lambda x: f\"{x['mean']} ± {x['std']}\", axis=1\n",
    ")\n",
    "result_dict = result_dict.drop([\"mean\", \"std\"], axis=1)\n",
    "\n",
    "# Reset multiindex\n",
    "result_dict = result_dict.reset_index()\n",
    "# rename columns\n",
    "result_dict.columns = [\"Dataset\", \"Model\", \"Average Training Time\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataset</th>\n",
       "      <th>ogbg-molhiv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SANN</th>\n",
       "      <td>1378.66 ± 306.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dataset       ogbg-molhiv\n",
       "Model                    \n",
       "SANN     1378.66 ± 306.25"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict.pivot_table(\n",
    "    index=\"Model\", columns=\"Dataset\", values=\"Average Training Time\", aggfunc=\"first\"\n",
    ")[['ogbg-molhiv']] #[['MUTAG', 'NCI1','NCI109','PROTEINS','ZINC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topobench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
