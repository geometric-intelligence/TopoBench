{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we show how to implement your own model and test it on a dataset. \n",
    "\n",
    "This particular example uses the MUTAG dataset, uses an hypergraph lifting to create hypergraphs, and defines a model to work on them. \n",
    "\n",
    "We train the model using the appropriate training and validation datasets, and finally test it on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>Table of contents<font><a class='anchor' id='top'></a>\n",
    "&emsp;[1. Imports](##sec1)\n",
    "\n",
    "&emsp;[2. Configurations and utilities](##sec2)\n",
    "\n",
    "&emsp;[3. Loading the data](##sec3)\n",
    "\n",
    "&emsp;[4. Backbone definition](##sec4)\n",
    "\n",
    "&emsp;[5. Model initialization](##sec5)\n",
    "\n",
    "&emsp;[6. Training](##sec6)\n",
    "\n",
    "&emsp;[7. Testing the model](##sec7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports <a class=\"anchor\" id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning as pl\n",
    "# Hydra related imports\n",
    "from omegaconf import OmegaConf\n",
    "# Data related imports\n",
    "from topobench.data.loaders.graph import TUDatasetLoader\n",
    "from topobench.dataloader.dataloader import TBDataloader\n",
    "from topobench.data.preprocessor import PreProcessor\n",
    "# Model related imports\n",
    "from topobench.model.model import TBModel\n",
    "from topomodelx.nn.simplicial.scn2 import SCN2\n",
    "from topobench.nn.wrappers.simplicial import SCNWrapper\n",
    "from topobench.nn.encoders import AllCellFeatureEncoder\n",
    "from topobench.nn.readouts import PropagateSignalDown\n",
    "# Optimization related imports\n",
    "from topobench.loss.loss import TBLoss\n",
    "from topobench.optimizer import TBOptimizer\n",
    "from topobench.evaluator.evaluator import TBEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurations and utilities <a class=\"anchor\" id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations can be specified using yaml files or directly specified in your code like in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_config = {\n",
    "    \"data_domain\": \"graph\",\n",
    "    \"data_type\": \"TUDataset\",\n",
    "    \"data_name\": \"MUTAG\",\n",
    "    \"data_dir\": \"./data/MUTAG/\"}\n",
    "\n",
    "transform_config = { \"khop_lifting\":\n",
    "    {\"transform_type\": \"lifting\",\n",
    "    \"transform_name\": \"HypergraphKHopLifting\",\n",
    "    \"k_value\": 1,}\n",
    "}\n",
    "\n",
    "split_config = {\n",
    "    \"learning_setting\": \"inductive\",\n",
    "    \"split_type\": \"random\",\n",
    "    \"data_seed\": 0,\n",
    "    \"data_split_dir\": \"./data/MUTAG/splits/\",\n",
    "    \"train_prop\": 0.5,\n",
    "}\n",
    "\n",
    "in_channels = 7\n",
    "out_channels = 2\n",
    "dim_hidden = 16\n",
    "\n",
    "readout_config = {\n",
    "    \"readout_name\": \"PropagateSignalDown\",\n",
    "    \"num_cell_dimensions\": 1,\n",
    "    \"hidden_dim\": dim_hidden,\n",
    "    \"out_channels\": out_channels,\n",
    "    \"task_level\": \"graph\",\n",
    "    \"pooling_type\": \"sum\",\n",
    "}\n",
    "\n",
    "loss_config = {\n",
    "    \"dataset_loss\": \n",
    "        {\n",
    "            \"task\": \"classification\", \n",
    "            \"loss_type\": \"cross_entropy\"\n",
    "        }\n",
    "}\n",
    "\n",
    "evaluator_config = {\"task\": \"classification\",\n",
    "                    \"num_classes\": out_channels,\n",
    "                    \"metrics\": [\"accuracy\", \"precision\", \"recall\"]}\n",
    "\n",
    "optimizer_config = {\"optimizer_id\": \"Adam\",\n",
    "                    \"parameters\":\n",
    "                        {\"lr\": 0.001,\"weight_decay\": 0.0005}\n",
    "                    }\n",
    "\n",
    "loader_config = OmegaConf.create(loader_config)\n",
    "transform_config = OmegaConf.create(transform_config)\n",
    "split_config = OmegaConf.create(split_config)\n",
    "readout_config = OmegaConf.create(readout_config)\n",
    "loss_config = OmegaConf.create(loss_config)\n",
    "evaluator_config = OmegaConf.create(evaluator_config)\n",
    "optimizer_config = OmegaConf.create(optimizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the data <a class=\"anchor\" id=\"sec3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use the MUTAG dataset. It is a graph dataset and we use the k-hop lifting to transform the graphs into hypergraphs. \n",
    "\n",
    "We invite you to check out the README of the [repository](https://github.com/pyt-team/TopoBenchX) to learn more about the various liftings offered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: data\\MUTAG\\MUTAG\\khop_lifting\\1116229528\n"
     ]
    }
   ],
   "source": [
    "graph_loader = TUDatasetLoader(loader_config)\n",
    "\n",
    "dataset, dataset_dir = graph_loader.load()\n",
    "\n",
    "preprocessor = PreProcessor(dataset, dataset_dir, transform_config)\n",
    "dataset_train, dataset_val, dataset_test = preprocessor.load_dataset_splits(split_config)\n",
    "datamodule = TBDataloader(dataset_train, dataset_val, dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backbone definition <a class=\"anchor\" id=\"sec4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a new model we only need to define the forward method.\n",
    "\n",
    "With a hypergraph with $n$ nodes and $m$ hyperedges this model simply calculates the hyperedge features as $X_1 = B_1 \\cdot X_0$ where $B_1 \\in \\mathbb{R}^{n \\times m}$ is the incidence matrix, where $B_{ij}=1$ if node $i$ belongs to hyperedge $j$ and is 0 otherwise.\n",
    "\n",
    "Then the outputs are computed as $X^{'}_0=\\text{ReLU}(W_0 \\cdot X_0 + B_0)$ and $X^{'}_1=\\text{ReLU}(W_1 \\cdot X_1 + B_1)$, by simply using two linear layers with ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel(pl.LightningModule):\n",
    "    def __init__(self, dim_hidden):\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.linear_0 = torch.nn.Linear(dim_hidden, dim_hidden)\n",
    "        self.linear_1 = torch.nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x_0 = batch.x_0\n",
    "        incidence_hyperedges = batch.incidence_hyperedges\n",
    "        x_1 = torch.sparse.mm(incidence_hyperedges, x_0)\n",
    "        \n",
    "        x_0 = self.linear_0(x_0)\n",
    "        x_0 = torch.relu(x_0)\n",
    "        x_1 = self.linear_1(x_1)\n",
    "        x_1 = torch.relu(x_1)\n",
    "        \n",
    "        model_out = {\"labels\": batch.y, \"batch_0\": batch.batch_0}\n",
    "        model_out[\"x_0\"] = x_0\n",
    "        model_out[\"hyperedge\"] = x_1\n",
    "        return model_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model initialization <a class=\"anchor\" id=\"sec5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is defined we can create the TBModel, which takes care of implementing everything else that is needed to train the model. \n",
    "\n",
    "First we need to implement a few classes to specify the behaviour of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = myModel(dim_hidden)\n",
    "\n",
    "readout = PropagateSignalDown(**readout_config)\n",
    "loss = TBLoss(**loss_config)\n",
    "feature_encoder = AllCellFeatureEncoder(in_channels=[in_channels], out_channels=dim_hidden)\n",
    "\n",
    "evaluator = TBEvaluator(**evaluator_config)\n",
    "optimizer = TBOptimizer(**optimizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the TBModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TBModel(backbone=backbone,\n",
    "                 backbone_wrapper=None,\n",
    "                 readout=readout,\n",
    "                 loss=loss,\n",
    "                 feature_encoder=feature_encoder,\n",
    "                 evaluator=evaluator,\n",
    "                 optimizer=optimizer,\n",
    "                 compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training <a class=\"anchor\" id=\"sec6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `lightning` trainer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type                  | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | feature_encoder | AllCellFeatureEncoder | 448    | train\n",
      "1 | backbone        | myModel               | 544    | train\n",
      "2 | readout         | PropagateSignalDown   | 34     | train\n",
      "3 | val_acc_best    | MeanMetric            | 0      | train\n",
      "------------------------------------------------------------------\n",
      "1.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\giova\\anaconda3\\envs\\tb\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\giova\\anaconda3\\envs\\tb\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\giova\\anaconda3\\envs\\tb\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\giova\\anaconda3\\envs\\tb\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\giova\\anaconda3\\envs\\tb\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "# Increase the number of epochs to get better results\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator=\"cpu\", enable_progress_bar=False, log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "train_metrics = trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training metrics\n",
      " --------------------------\n",
      "train/accuracy:       0.7660\n",
      "train/precision:      0.7476\n",
      "train/recall:         0.6943\n",
      "val/loss:             0.5238\n",
      "val/accuracy:         0.7447\n",
      "val/precision:        0.7321\n",
      "val/recall:           0.6354\n",
      "train/loss:           0.4471\n"
     ]
    }
   ],
   "source": [
    "print('      Training metrics\\n', '-'*26)\n",
    "for key in train_metrics:\n",
    "    print('{:<21s} {:>5.4f}'.format(key+':', train_metrics[key].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the model <a class=\"anchor\" id=\"sec7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test the model and obtain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giova\\anaconda3\\envs\\tb\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7234042286872864     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4870447516441345     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7339743375778198     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test/recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6431372761726379     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7234042286872864    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4870447516441345    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7339743375778198    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test/recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6431372761726379    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(model, datamodule)\n",
    "test_metrics = trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Testing metrics\n",
      " -------------------------\n",
      "test/loss:           0.4870\n",
      "test/accuracy:       0.7234\n",
      "test/precision:      0.7340\n",
      "test/recall:         0.6431\n"
     ]
    }
   ],
   "source": [
    "print('      Testing metrics\\n', '-'*25)\n",
    "for key in test_metrics:\n",
    "    print('{:<20s} {:>5.4f}'.format(key+':', test_metrics[key].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch_geometric.nn as pygnn\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import Linear as Linear_pyg\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from topobench.nn.backbones.graph.gatedgcn_layer import GatedGCNLayer\n",
    "from topobench.nn.backbones.graph.gine_conv_layer import GINEConvESLapPE\n",
    "\n",
    "class GraphGPSModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_h,\n",
    "        local_gnn_type='GINE',\n",
    "        global_model_type='Transformer',\n",
    "        num_heads=8,\n",
    "        act='relu',\n",
    "        dropout=0.0,\n",
    "        attn_dropout=0.0,\n",
    "        layer_norm=False,\n",
    "        batch_norm=True,\n",
    "        equivstable_pe=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_h = dim_h\n",
    "        self.num_heads = num_heads\n",
    "        self.layer_norm = layer_norm\n",
    "        self.batch_norm = batch_norm\n",
    "        self.equivstable_pe = equivstable_pe\n",
    "        self.activation = nn.ReLU if act == 'relu' else nn.SiLU\n",
    "        # --- Local GNN (GINE or CustomGatedGCN) ---\n",
    "        if local_gnn_type == 'GINE':\n",
    "            gin_nn = nn.Sequential(\n",
    "                Linear_pyg(dim_h, dim_h),\n",
    "                self.activation(),\n",
    "                Linear_pyg(dim_h, dim_h),\n",
    "            )\n",
    "            if equivstable_pe:\n",
    "                self.local_model = GINEConvESLapPE(gin_nn)\n",
    "            else:\n",
    "                self.local_model = pygnn.GINEConv(gin_nn)\n",
    "        elif local_gnn_type == 'CustomGatedGCN':\n",
    "            self.local_model = GatedGCNLayer(\n",
    "                dim_h,\n",
    "                dim_h,\n",
    "                dropout=dropout,\n",
    "                residual=True,\n",
    "                act=act,\n",
    "                equivstable_pe=equivstable_pe\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported local GNN: {local_gnn_type}. \"\n",
    "                \"Choose 'GINE' or 'CustomGatedGCN'.\"\n",
    "            )\n",
    "        self.local_gnn_type = local_gnn_type\n",
    "        self.dropout_local = nn.Dropout(dropout)\n",
    "\n",
    "        if global_model_type == 'Transformer':\n",
    "            self.self_attn = nn.MultiheadAttention(\n",
    "                dim_h,\n",
    "                num_heads,\n",
    "                dropout=attn_dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported global model: {global_model_type}. \"\n",
    "                \"Choose 'Transformer'.\"\n",
    "            )\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "\n",
    "        if layer_norm and batch_norm:\n",
    "            raise ValueError(\"Cannot use both layer_norm and batch_norm\")\n",
    "        Norm = pygnn.norm.LayerNorm if layer_norm else nn.BatchNorm1d\n",
    "        self.norm1_local = Norm(dim_h)\n",
    "        self.norm1_attn = Norm(dim_h)\n",
    "        self.norm2 = Norm(dim_h)\n",
    "\n",
    "        # --- Feed-forward block ---\n",
    "        self.ff_linear1 = nn.Linear(dim_h, dim_h * 2)\n",
    "        self.ff_linear2 = nn.Linear(dim_h * 2, dim_h)\n",
    "        self.ff_activation = self.activation()\n",
    "        self.ff_dropout1 = nn.Dropout(dropout)\n",
    "        self.ff_dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        h = batch.x\n",
    "        h_in1 = h\n",
    "        outputs = []\n",
    "\n",
    "        # Local pass\n",
    "        if self.local_gnn_type == 'CustomGatedGCN':\n",
    "            es = batch.pe_EquivStableLapPE if self.equivstable_pe else None\n",
    "            local_out = self.local_model(\n",
    "                Batch(\n",
    "                    batch=batch,\n",
    "                    x=h,\n",
    "                    edge_index=batch.edge_index,\n",
    "                    edge_attr=batch.edge_attr,\n",
    "                    pe_EquivStableLapPE=es\n",
    "                )\n",
    "            ).x\n",
    "        else:  # GINE\n",
    "            pe = batch.pe_EquivStableLapPE if self.equivstable_pe else None\n",
    "            local_out = (\n",
    "                self.local_model(h, batch.edge_index, batch.edge_attr, pe)\n",
    "                if hasattr(self.local_model, 'equivstable_pe')\n",
    "                else self.local_model(h, batch.edge_index, batch.edge_attr)\n",
    "            )\n",
    "        local = self.dropout_local(local_out)\n",
    "        local = h_in1 + local\n",
    "        local = (\n",
    "            self.norm1_local(local, batch.batch)\n",
    "            if hasattr(self.norm1_local, 'normalized_shape')\n",
    "            else self.norm1_local(local)\n",
    "        )\n",
    "        outputs.append(local)\n",
    "\n",
    "        # Global Transformer pass\n",
    "        h_dense, mask = to_dense_batch(h, batch.batch)\n",
    "        attn_out = self.self_attn(\n",
    "            h_dense,\n",
    "            h_dense,\n",
    "            h_dense,\n",
    "            key_padding_mask=~mask\n",
    "        )[0]\n",
    "        attn = attn_out[mask]\n",
    "        attn = self.dropout_attn(attn)\n",
    "        attn = h_in1 + attn\n",
    "        attn = (\n",
    "            self.norm1_attn(attn, batch.batch)\n",
    "            if hasattr(self.norm1_attn, 'normalized_shape')\n",
    "            else self.norm1_attn(attn)\n",
    "        )\n",
    "        outputs.append(attn)\n",
    "\n",
    "        # Combine + FF\n",
    "        h = sum(outputs)\n",
    "        ff = self.ff_dropout1(self.ff_activation(self.ff_linear1(h)))\n",
    "        ff = self.ff_dropout2(self.ff_linear2(ff))\n",
    "        h = h + ff\n",
    "        h = (\n",
    "            self.norm2(h, batch.batch)\n",
    "            if hasattr(self.norm2, 'normalized_shape')\n",
    "            else self.norm2(h)\n",
    "        )\n",
    "\n",
    "        batch.x = h\n",
    "        return {\"labels\": batch.y, \"batch\": batch.batch, \"x\": h}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.GraphGPSModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TBModel`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(backbone))\n\u001b[0;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, enable_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\tb\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:554\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    509\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    552\u001b[0m \n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 554\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    556\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\tb\\Lib\\site-packages\\pytorch_lightning\\utilities\\compile.py:111\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    110\u001b[0m _check_mixed_imports(model)\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TBModel`"
     ]
    }
   ],
   "source": [
    "backbone = GraphGPSModel(dim_hidden)\n",
    "\n",
    "readout = PropagateSignalDown(**readout_config)\n",
    "loss = TBLoss(**loss_config)\n",
    "feature_encoder = AllCellFeatureEncoder(in_channels=[in_channels], out_channels=dim_hidden)\n",
    "\n",
    "evaluator = TBEvaluator(**evaluator_config)\n",
    "optimizer = TBOptimizer(**optimizer_config)\n",
    "\n",
    "model = TBModel(backbone=backbone,\n",
    "                 backbone_wrapper=None,\n",
    "                 readout=readout,\n",
    "                 loss=loss,\n",
    "                 feature_encoder=feature_encoder,\n",
    "                 evaluator=evaluator,\n",
    "                 optimizer=optimizer,\n",
    "                 compile=False)\n",
    "print(type(backbone))\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator=\"cpu\", enable_progress_bar=False, log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "train_metrics = trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topobench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
