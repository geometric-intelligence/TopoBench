{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Adding a Custom Dataset Tutorial\n",
    "\n",
    "## üéØ Tutorial Overview\n",
    "\n",
    "This comprehensive guide walks you through the process of integrating your custom dataset into our library. The process is divided into three main steps:\n",
    "\n",
    "1.  **Dataset Creation** üî®\n",
    "    * Implement a new dataset class (inheriting from `InMemoryDataset`).\n",
    "    * Define the `download` logic to get your raw data files.\n",
    "    * Define the `process` logic to convert raw data into `Data` objects.\n",
    "\n",
    "2.  **Integrate with Dataset APIs** üîÑ\n",
    "    * Implement a `DatasetLoader` class to make your dataset accessible to the library.\n",
    "    * Place your new Python files in the correct directories for auto-registration.\n",
    "\n",
    "3.  **Configuration Setup** ‚öôÔ∏è\n",
    "    * Create a `.yaml` config file for your dataset.\n",
    "    * Define all dataset parameters, such as task type and feature dimensions.\n",
    "    * Configure data paths, splitting strategies, and batching settings.\n",
    "\n",
    "## üìã How This Tutorial is Structured\n",
    "\n",
    "This tutorial is presented in two complementary parts:\n",
    "\n",
    "> üí° **1. The Main Guide (This Notebook)**\n",
    "> * Provides a high-level, step-by-step walkthrough of the entire **custom dataset integration** process.\n",
    "> * Explains the core concepts and demonstrates how the integrated dataset can be used with the TopoBench library, complete with runnable code.\n",
    "\n",
    "> üìÅ **2. The Reference Templates (Library Files)**\n",
    "> * The guide refers to the actual `.py` and `.yaml` implementation files within the library.\n",
    "> * These files are intended to serve as starting points, which can be copied and modified for a new custom dataset.\n",
    "\n",
    "## üõ†Ô∏è Technical Framework\n",
    "\n",
    "Adding a new dataset involves \"plugging it in\" to TopoBench's existing architecture. This tutorial's process connects three key components:\n",
    "\n",
    "* **`torch_geometric.data.InMemoryDataset`**: This is the standard **PyTorch Geometric (PyG)** base class your new dataset must inherit from. We use it because our datasets are expected to fit and be processed entirely in memory.\n",
    "\n",
    "* **TopoBench's `AbstractLoader`**: This is the TopoBench-specific class that makes the framework aware of your dataset. You will implement a new loader that inherits from this, which tells TopoBench *how* to find and instantiate your dataset class.\n",
    "\n",
    "* **Hydra Configuration**: This is how you define your dataset's parameters (like its name, task, and paths). TopoBench uses **Hydra** to manage all experiment configurations in `.yaml` files. This system makes it simple to compose experiments and override any setting from the command line.\n",
    "\n",
    "## üéì Important Notes\n",
    "\n",
    "* To make the learning process concrete, this guide uses the existing **`US-county-demos`** dataset as a reference.\n",
    "* When following the instructions to create a new dataset, all template paths and class names use the placeholder **`<dataset_name>`**. This placeholder must be replaced with the actual name of the new dataset.\n",
    "\n",
    "**For example:**\n",
    "\n",
    "| If the instruction shows a template like: | And the new dataset is named \"MyDataset\": |\n",
    "| :--- | :--- |\n",
    "| `configs/dataset/<dataset_name>.yaml` | The new file should be: `configs/dataset/MyDataset.yaml` |\n",
    "| `class <DatasetName>Dataset:` | The new class should be: `class MyDatasetDataset:` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a Dataset Class üõ†Ô∏è\n",
    "\n",
    "## The Goal\n",
    "\n",
    "The first step is to create a new **Python class** that defines your dataset. This class tells TopoBench how to get, process, and save your data.\n",
    "\n",
    "We will inherit from **`torch_geometric.data.InMemoryDataset`**. This is a powerful base class from PyTorch Geometric that does most of the heavy lifting. For example, it automatically:\n",
    "* Checks if the dataset is already downloaded or processed.\n",
    "* Handles the logic of `self.raw_dir` and `self.processed_dir`.\n",
    "\n",
    "Our only job is to provide the logic for the two methods it needs.\n",
    "\n",
    "## Key Methods to Implement\n",
    "\n",
    "To get our new class working, we only need to implement two core methods:\n",
    "\n",
    "* **`download()`**\n",
    "    * **What it does:** Gets your *raw* data files (like `.zip`, `.csv`, `.json`, etc.) from their source.\n",
    "    * **Your Task:** Download from a URL or copy from a local directory.\n",
    "    * **Output:** All raw files must be saved in the `self.raw_dir` (the `raw/` folder).\n",
    "\n",
    "* **`process()`**\n",
    "    * **What it does:** Converts the *raw* files into the final, ready-to-use PyTorch Geometric format.\n",
    "    * **Your Task:** Load the raw files from `self.raw_dir`, perform all preprocessing, and build one or more `torch_geometric.data.Data` objects.\n",
    "    * **Output:** The final dataset (a list of `Data` objects) must be saved as a single `.pt` file in the `self.processed_dir` (the `processed/` folder).\n",
    "\n",
    "> üí° **Reference Implementation**: For a complete example, see the directory `topobench/data/datasets/` and files like `us_county_demos_dataset.py`, `mantra_dataset.py`, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive: The Download Method\n",
    "\n",
    "> üí° **Reference Implementation:**\n",
    "> The code discussed in this section is adapted from:\n",
    "> `topobench/data/datasets/us_county_demos_dataset.py`\n",
    "\n",
    "The `download()` method's only job is to get the raw data files (like `.zip`, `.csv`, etc.) and save them in the `self.raw_dir` folder.\n",
    "\n",
    "Let's examine the implementation from our `US-county-demos` example.\n",
    "\n",
    "### 1. Defining URLs (Class Attributes)\n",
    "\n",
    "First, it's a best practice to define URLs as class attributes, so they are easy to find and change. In the reference file, these are defined at the top of the class:\n",
    "\n",
    "```python\n",
    "class USCountyDemosDataset(InMemoryDataset):\n",
    "    URLS = {\n",
    "        'US-county-demos': '10-3W-P-1m-R_r-Z-L3S6_G1-hZk-m'\n",
    "    }\n",
    "    FILE_FORMAT = {\n",
    "        'US-county-demos': 'zip'\n",
    "    }\n",
    "\n",
    "    def __init__(self, ...):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implementation Steps & Code\n",
    "\n",
    "With that context, the `download` method is straightforward. The process is:\n",
    "\n",
    "1.  **Get URL & Download** üì•\n",
    "    * Look up the dataset's URL and file format from the class attributes.\n",
    "    * Call `download_file_from_drive` to fetch the data and save it to `self.raw_dir`.\n",
    "\n",
    "2.  **Extract Content** üì¶\n",
    "    * Call `extract_zip` to unzip the downloaded file (e.g., `US-county-demos.zip`) into the `self.raw_dir`.\n",
    "    * Delete the original `.zip` file to save space.\n",
    "\n",
    "3.  **Organize Files** üìÇ\n",
    "    * Often, unzipping a file creates an extra sub-folder (e.g., the zip extracts to `raw/US-county-demos/`).\n",
    "    * This step \"flattens\" the directory by moving all files from that sub-folder directly into `self.raw_dir`.\n",
    "    * Finally, it removes the now-empty sub-folder.\n",
    "\n",
    "#### Code Implementation\n",
    "\n",
    "```python\n",
    "def download(self) -> None:\n",
    "    r\"\"\"Download the dataset from a URL and saves it to the raw directory.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the dataset URL is not found.\n",
    "    \"\"\"\n",
    "    # Step 1: Download data from the source\n",
    "    self.url = self.URLS[self.name]\n",
    "    self.file_format = self.FILE_FORMAT[self.name]\n",
    "    download_file_from_drive(\n",
    "        file_link=self.url,\n",
    "        path_to_save=self.raw_dir,\n",
    "        dataset_name=self.name,\n",
    "        file_format=self.file_format,\n",
    "    )\n",
    "    \n",
    "    # Step 2: Extract zip file\n",
    "    folder = self.raw_dir\n",
    "    filename = f\"{self.name}.{self.file_format}\"\n",
    "    path = osp.join(folder, filename)\n",
    "    extract_zip(path, folder)\n",
    "    # Delete zip file\n",
    "    os.unlink(path)\n",
    "    \n",
    "    # Step 3: Organize files\n",
    "    # Move files from the unzipped sub-folder up one level\n",
    "    source_folder = osp.join(folder, self.name)\n",
    "    for file in os.listdir(source_folder):\n",
    "        shutil.move(osp.join(source_folder, file), folder)\n",
    "    # Delete the now-empty sub-folder\n",
    "    shutil.rmtree(source_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive: The Process Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> üí° **Reference Implementation:**\n",
    "> The code discussed in this section is the *actual* implementation from:\n",
    "> `topobench/data/datasets/us_county_demos_dataset.py`\n",
    "\n",
    "Complete `process()` method:\n",
    "```python\n",
    "def process(self) -> None:\n",
    "        r\"\"\"Handle the data for the dataset.\n",
    "\n",
    "        This method loads the US county demographics data, applies any pre-\n",
    "        processing transformations if specified, and saves the processed data\n",
    "        to the appropriate location.\n",
    "        \"\"\"\n",
    "        # Step 1: extract the data\n",
    "        data = read_us_county_demos(\n",
    "            self.raw_dir, self.year, self.task_variable\n",
    "        )\n",
    "        data_list = [data]\n",
    "\n",
    "        # Step 2: collate the graphs\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "        self._data_list = None  # Reset cache.\n",
    "\n",
    "        # Step 3: save processed data\n",
    "        fs.torch_save(\n",
    "            (self._data.to_dict(), self.slices, {}, self._data.__class__),\n",
    "            self.processed_paths[0],\n",
    "        )\n",
    "```\n",
    "\n",
    "\n",
    "The `process()` method is the heart of your dataset. Its job is to load the raw files you downloaded in `self.raw_dir` and convert them into the final `Data` objects that PyTorch Geometric can use.\n",
    "\n",
    "The method has three responsibilities:\n",
    "\n",
    "1.  **Load Raw Data:** Read your `.csv`, `.json`, `.txt`, or other files from `self.raw_dir`.\n",
    "2.  **Build `Data` Objects:** Create one or more `torch_geometric.data.Data` objects holding your node features (`x`), connectivity (`edge_index`), and targets (`y`).\n",
    "3.  **Collate & Save:** Combine all `Data` objects and save them to `self.processed_dir`.\n",
    "\n",
    "In the `US-county-demos` implementation, the complex logic of Step 1 is hidden inside a helper function called `read_us_county_demos`. This is a very common and clean way to structure your code.\n",
    "\n",
    "### 1. Step 1: Extract the Data (via Helper)\n",
    "\n",
    "Instead of writing all the `pandas` and `torch` logic directly in the `process` method, the code calls a custom function `read_us_county_demos`.\n",
    "\n",
    "* **What it does:** This helper function (which is in the same file) is responsible for loading the raw `.csv` and `.txt` files, creating the `x`, `edge_index`, and `y` tensors, and returning a single, fully-formed `Data` object.\n",
    "* **Parameters:** It passes `self.raw_dir` (to find the files) and parameters like `self.year` and `self.task_variable` so it can build the *correct* version of the dataset.\n",
    "\n",
    "```python\n",
    "        # Step 1: extract the data\n",
    "        data = read_us_county_demos(\n",
    "            self.raw_dir, self.year, self.task_variable\n",
    "        )\n",
    "        data_list = [data]\n",
    "```\n",
    "\n",
    "### 2. Step 2: Collate the Graphs\n",
    "\n",
    "This step is standard for `InMemoryDataset`.\n",
    "\n",
    "* **`self.collate(data_list)`**: This helper method takes the list of `Data` objects (in this case, a list with just one item) and formats it into an efficient, collated `BaseData` object. This is the standard storage format PyG uses.\n",
    "\n",
    "```python\n",
    "        # Step 2: collate the graphs\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "```\n",
    "\n",
    "### 3. Step 3: Save Processed Data \n",
    "\n",
    "This step saves the collated data to the `self.processed_dir`. The `US-county-demos` dataset uses a specific PyG file system utility (`fs.torch_save`) for this.\n",
    "\n",
    "* **`fs.torch_save(...)`**: This is the TopoBench-specific way to save the processed data. It serializes the data object's dictionary representation (`self._data.to_dict()`) and the slices, saving them to the path specified in `self.processed_paths[0]`.\n",
    "\n",
    "```python\n",
    "        # Step 3: save processed data\n",
    "        fs.torch_save(\n",
    "            (self._data.to_dict(), self.slices, {}, self._data.__class__),\n",
    "            self.processed_paths[0],\n",
    "        )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Integrate with Dataset APIs üîÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Goal\n",
    "\n",
    "At this point, the `Dataset` class defined in Step 1 is just an \"offline\" Python file. TopoBench has no way of finding or using it.\n",
    "\n",
    "The goal of Step 2 is to \"plug in\" our new dataset so the framework can **discover and load it**. This requires two actions:\n",
    "1.  Placing the dataset file in the correct directory for auto-registration.\n",
    "2.  Creating a **Loader Class** to act as the bridge between TopoBench's configuration system and our dataset class.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 1: Place Your Dataset File\n",
    "\n",
    "This is the easiest step.\n",
    "\n",
    "1.  **Move your file:** Place the `<dataset_name>.py` file (e.g., `us_county_demos_dataset.py`) created in Step 1 into the `topobench/data/datasets/` directory.\n",
    "\n",
    "    ```yaml\n",
    "    topobench/\n",
    "    ‚îú‚îÄ‚îÄ data/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ datasets/\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # <-- This file handles auto-registration\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ <dataset_name>.py   # <-- Your file from Step 1 goes here\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ loaders/\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ```\n",
    "\n",
    "2.  **That's it!** The `topobench/data/datasets/__init__.py` file is designed to **automatically discover** and register any new dataset class in this directory. There is no need to edit `__init__.py` manually.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Create a Dataset Loader\n",
    "\n",
    "Next, we must create a **Loader**. This is a simple class that inherits from `AbstractLoader` and tells TopoBench *how* to instantiate your dataset class and pass in the correct parameters (like `year` or `task_variable`) from the config file.\n",
    "\n",
    "1.  **Create a new loader file:** Create a new file named `<dataset_name>_loader.py` (e.g., `us_county_demos_dataset_loader.py`) and place it in the appropriate `loaders` subdirectory. For standard graphs, this is `topobench/data/loaders/graph/`.\n",
    "\n",
    "    ```yaml\n",
    "    topobench/\n",
    "    ‚îú‚îÄ‚îÄ data/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ datasets/\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ loaders/\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py       # <-- This file also handles auto-registration\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py           # <-- Contains the AbstractLoader\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph/\n",
    "    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ <dataset_name>_loader.py  # <-- Your new loader file goes here\n",
    "    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hypergraph/\n",
    "    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ```\n",
    "\n",
    "2.  **Implement the Loader Class:** The loader is mostly boilerplate. Its main job is to implement the `load_dataset` method, which simply creates an instance of your dataset class from Step 1.\n",
    "\n",
    "    Here is the complete template from `us_county_demos_dataset_loader.py` (including the necessary imports):\n",
    "\n",
    "    ```python\n",
    "    from pathlib import Path\n",
    "    from omegaconf import DictConfig\n",
    "\n",
    "    from topobench.data.datasets import USCountyDemosDataset  # <-- Import class from Step 1\n",
    "    from topobench.data.loaders.base import AbstractLoader    # <-- Import base class\n",
    "\n",
    "\n",
    "    class USCountyDemosDatasetLoader(AbstractLoader):\n",
    "        \"\"\"Load US County Demos dataset with configurable year and task variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters : DictConfig\n",
    "            Configuration parameters containing:\n",
    "                - data_dir: Root directory for data\n",
    "                - data_name: Name of the dataset\n",
    "                - year: Year of the dataset (if applicable)\n",
    "                - task_variable: Task variable for the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, parameters: DictConfig) -> None:\n",
    "            super().__init__(parameters)\n",
    "\n",
    "        def load_dataset(self) -> USCountyDemosDataset:\n",
    "            \"\"\"Load the US County Demos dataset.\n",
    "\n",
    "            This is the main method called by TopoBench.\n",
    "            It initializes the dataset and returns it.\n",
    "            \"\"\"\n",
    "            dataset = self._initialize_dataset()\n",
    "            self.data_dir = self._redefine_data_dir(dataset)\n",
    "            return dataset\n",
    "\n",
    "        def _initialize_dataset(self) -> USCountyDemosDataset:\n",
    "            \"\"\"Helper method to instantiate the dataset class.\"\"\"\n",
    "            \n",
    "            # This is the key line: it creates an instance\n",
    "            # of the class from Step 1, passing in parameters\n",
    "            # from the config file (accessed via self.parameters).\n",
    "            return USCountyDemosDataset(\n",
    "                root=str(self.root_data_dir),\n",
    "                name=self.parameters.data_name,\n",
    "                parameters=self.parameters,\n",
    "            )\n",
    "\n",
    "        def _redefine_data_dir(self, dataset: USCountyDemosDataset) -> Path:\n",
    "            \"\"\"Helper method to get the final processed data path.\"\"\"\n",
    "            return dataset.processed_root\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùó Key Integration Rules (Auto-Registration)\n",
    "\n",
    "For the TopoBench framework to find your new loader class automatically, you **must** follow these naming conventions:\n",
    "\n",
    "* **Inheritance:** Your loader class (e.g., `USCountyDemosDatasetLoader`) must inherit from `AbstractLoader`.\n",
    "* **Method:** Your loader class must implement the `load_dataset` method, which must return a `torch.utils.data.Dataset` object (our `InMemoryDataset` class qualifies).\n",
    "* **Naming Convention:** This is the most important part. The class name **must** end with the suffix **`DatasetLoader`**.\n",
    "    * ‚úÖ `MyCoolDatasetLoader`\n",
    "    * ‚úÖ `USCountyDemosDatasetLoader`\n",
    "    * ‚ùå `MyCoolDataset`\n",
    "    * ‚ùå `MyCoolLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Configuration ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "\n",
    "The goal of this step is to create a `.yaml` configuration file. This file is the \"control panel\" for your dataset. It tells TopoBench (and the **Hydra** framework) exactly:\n",
    "1.  **What to load:** Using the `_target_` key to point to your `Loader` class from Step 2.\n",
    "2.  **How to load it:** By passing parameters (like `year` or `task_variable`) to your loader.\n",
    "3.  **What its properties are:** By defining parameters like `num_features`, `task`, and `task_level`.\n",
    "\n",
    "### File Location\n",
    "\n",
    "First, create your new configuration file in the appropriate sub-directory. The path is critical for TopoBench to find it when you reference it (e.g., `dataset=graph/US-county-demos`).\n",
    "\n",
    "**Create the file at:**\n",
    "`configs/dataset/<data_domain>/<dataset_name>.yaml`\n",
    "\n",
    "**For our example:**\n",
    "`configs/dataset/graph/US-county-demos.yaml`\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration File Structure\n",
    "\n",
    "Your new `.yaml` file must define four main sections:\n",
    "\n",
    "### 1. Loader Configuration\n",
    "\n",
    "This section tells Hydra which `Loader` class to instantiate.\n",
    "\n",
    "* `_target_`: This is the most important key. It's a special **Hydra** command that tells the framework to import and create an instance of this *exact* Python class (the one created in Step 2).\n",
    "* `parameters`: This nested dictionary contains all the arguments that will be passed to your `Loader`'s `__init__` method.\n",
    "* `data_dir: ${...}`: Note the `${...}` syntax. This is **Hydra's** way of *interpolating variables*‚Äîit re-uses values from other parts of the configuration (in this case, the global `paths.data_dir`).\n",
    "\n",
    "```yaml\n",
    "# Dataset loader config\n",
    "loader:\n",
    "  _target_: topobench.data.loaders.graph.us_county_demos_dataset_loader.USCountyDemosDatasetLoader\n",
    "  parameters: \n",
    "    data_domain: graph             # Primary data domain. Options: ['graph', 'hypergraph', 'cell', 'simplicial']\n",
    "    data_type: cornel              # Data type. String emphasizing from where dataset come from. \n",
    "    data_name: US-county-demos     # Name of the dataset\n",
    "    year: 2012                     # In the case of US-county-demos there are multiple version of this dataset. Options:[2012, 2016]\n",
    "    task_variable: 'Election'      # Different target variable used as target. Options: ['Election', 'MedianIncome', 'MigraRate', 'BirthRate', 'DeathRate', 'BachelorRate', 'UnemploymentRate']\n",
    "    data_dir: ${paths.data_dir}/${dataset.loader.parameters.data_domain}/${dataset.loader.parameters.data_type}\n",
    "```\n",
    "\n",
    "### 2. Dataset Parameters\n",
    "\n",
    "This section defines the objective properties of your dataset. These values are used by other parts of the framework (like models and trainers) to configure themselves correctly.\n",
    "\n",
    "```yaml\n",
    "# Dataset parameters\n",
    "parameters:\n",
    "  num_features: 6         # Number of features in the dataset\n",
    "  num_classes: 1          # Dimension of the target variable\n",
    "  task: regression        # Dataset task. Options: [classification, regression]\n",
    "  loss_type: mse          # Task-specific loss function\n",
    "  monitor_metric: mae     # Metric to monitor during training\n",
    "  task_level: node        # Task level. Options: [node, edge, graph]\n",
    "```\n",
    "\n",
    "### 3. Split Parameters\n",
    "\n",
    "This section controls all aspects of data splitting for training, validation, and testing.\n",
    "\n",
    "```yaml\n",
    "# Splits\n",
    "split_params:\n",
    "  learning_setting: transductive      # Type of learning. Options:['transductive', 'inductive']\n",
    "  data_seed: 0                        # Seed for data splitting\n",
    "  split_type: random                  # Type of splitting. Options: ['k-fold', 'random']\n",
    "  k: 10                               # Number of folds in case of \"k-fold\" cross-validation\n",
    "  train_prop: 0.5                     # Training proportion in case of 'random' splitting strategy\n",
    "  standardize: True                   # Standardize the data or not. Options: [True, False]\n",
    "  data_split_dir: ${paths.data_dir}/data_splits/${dataset.loader.parameters.data_name}\n",
    "```\n",
    "\n",
    "### 4. Dataloader Parameters\n",
    "\n",
    "Finally, these parameters are passed directly to the PyTorch `DataLoader` wrapper that handles batching.\n",
    "\n",
    "```yaml\n",
    "# Dataloader parameters\n",
    "dataloader_params:\n",
    "  batch_size: 1       # Number of graphs per batch. In case of transductive always 1 as there is only one graph. \n",
    "  num_workers: 0      # Number of workers for data loading\n",
    "  pin_memory: False   # Pin memory for data loading\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "* The `paths` object (used in `${paths.data_dir}`) is automatically resolved by Hydra from a global configuration file.\n",
    "* It is critical to include all these parameters, even if they seem redundant. Other configuration files (like for models) dynamically read these values (e.g., `model.in_channels=${dataset.parameters.num_features}`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations! The Dataset is Integrated!\n",
    "\n",
    "That's it! By completing these three steps, the new dataset is now fully integrated into the TopoBench framework.\n",
    "\n",
    "### Summary of What Was Accomplished\n",
    "\n",
    "1.  **Dataset Class:** A new `.py` file was created (e.g., `us_county_demos_dataset.py`) that inherits from `InMemoryDataset` and implements the `download` and `process` logic.\n",
    "2.  **Loader Class:** A second `.py` file was created (e.g., `us_county_demos_dataset_loader.py`) that inherits from `AbstractLoader` to \"plug\" the dataset into the framework.\n",
    "3.  **Config File:** A new `.yaml` file (e.g., `US-county-demos.yaml`) was created to define all the dataset's parameters and tell Hydra how to load it.\n",
    "\n",
    "### Next Steps: Run It!\n",
    "\n",
    "Thanks to this setup, the dataset is now available to the entire framework. It can be loaded, preprocessed, and used in any experiment simply by referencing its name.\n",
    "\n",
    "The following part of this notebook will demonstrate how to load the newly integrated dataset and even run a full training and evaluation pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Load the Custom Dataset: Understanding Configuration Imports\n",
    "\n",
    "\n",
    "## Step 4: Run It! (Understanding Config Resolvers)\n",
    "\n",
    "We are now ready to load and use our new dataset.\n",
    "\n",
    "Before we run the code, we must understand one advanced feature of Hydra that TopoBench configuration system relies on: **Resolvers**.\n",
    "\n",
    "### What is a Resolver?\n",
    "\n",
    "A Resolver is a Python function that can be called *from within a `.yaml` file*. This allows for powerful, dynamic configuration.\n",
    "\n",
    "In Step 3, we saw simple **variable substitution**:\n",
    "```yaml\n",
    "# This just copies a value\n",
    "data_dir: ${paths.data_dir}/${...}\n",
    "```\n",
    "\n",
    "But TopoBench also uses function resolvers for automated logic:\n",
    "\n",
    "```yaml\n",
    "# This calls a Python function\n",
    "transforms: ${get_default_transform:${dataset},${model}}\n",
    "```\n",
    "\n",
    "When Hydra sees this line, it will **call the `get_default_transform` function**, passing it the `dataset` and `model` configs. This function then *dynamically* figures out the correct data transformations, which is a key part of TopoBench's automation.\n",
    "\n",
    "### Why This Matters in a Notebook\n",
    "\n",
    "For Hydra to find and use functions like `get_default_transform`, they must be **registered** first.\n",
    "\n",
    "In a normal command-line run, TopoBench handles this automatically. But when running in a notebook, **we must register them manually**. We do this by simply **importing them** in a code cell *before* we initialize Hydra.\n",
    "\n",
    "The next code cells will show the full list of imports from `topobench.utils.config_resolvers` needed for a complete run. This \"registration\" step unlocks the framework's full power:\n",
    "\n",
    "* **Automatic Parameter Inference** (e.g., inferring model input channels from data)\n",
    "* **Dynamic Configuration** (e.g., selecting the right transforms for a given model/dataset)\n",
    "* **Reduced Manual Setup** (less boilerplate to write in config files)\n",
    "\n",
    "Now, let's see the complete code in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/kdqr0d7n3y1cyygtb_y421tc0000gn/T/ipykernel_2107/1604224440.py:21: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../configs\", job_name=\"job\"):\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "\n",
    "\n",
    "from topobench.utils.config_resolvers import (\n",
    "    get_default_metrics,\n",
    "    get_default_trainer,\n",
    "    get_default_transform,\n",
    "    get_flattened_channels,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    get_non_relational_out_channels,\n",
    "    get_required_lifting,\n",
    "    infer_in_channels,\n",
    "    infer_num_cell_dimensions,\n",
    ")\n",
    "\n",
    "\n",
    "with initialize(config_path=\"../configs\", job_name=\"job\"):\n",
    "\n",
    "    cfg = compose(\n",
    "        config_name=\"run.yaml\",\n",
    "        overrides=[\n",
    "            \"model=hypergraph/unignn2\",\n",
    "            \"dataset=graph/US-county-demos\",\n",
    "        ], \n",
    "        return_hydra_config=True\n",
    "    )\n",
    "loader = instantiate(cfg.dataset.loader)\n",
    "\n",
    "\n",
    "dataset, dataset_dir = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-county-demos(self.root=/Users/leone/Desktop/PhD/projects/projects/TopoBench/datasets/graph/cornel, self.name=US-county-demos, self.parameters={'data_domain': 'graph', 'data_type': 'cornel', 'data_name': 'US-county-demos', 'year': 2012, 'task_variable': 'Election', 'data_dir': '/Users/leone/Desktop/PhD/projects/projects/TopoBench/datasets/graph/cornel'}, self.force_reload=False)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3224, 6], edge_index=[2, 18966], y=[3224])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the default transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform name: dict_keys(['graph2hypergraph_lifting'])\n"
     ]
    }
   ],
   "source": [
    "print('Transform name:', cfg.transforms.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from topobench.data.preprocessor import PreProcessor\n",
    "preprocessed_dataset = PreProcessor(dataset, dataset_dir, cfg['transforms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3224, 6], edge_index=[2, 18966], y=[3224], incidence_hyperedges=[3224, 3224], num_hyperedges=3224, x_0=[3224, 6], x_hyperedges=[3224, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is integrated, lets run some model: \n",
    "\n",
    "CLI command: `python -m topobench model=hypergraph/unignn2 dataset=graph/US-county-demos`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/kdqr0d7n3y1cyygtb_y421tc0000gn/T/ipykernel_2107/1017559150.py:65: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../configs\", job_name=\"job\"):\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: /Users/leone/Desktop/PhD/projects/projects/TopoBench/datasets/graph/cornel/US-county-demos/graph2hypergraph_lifting/304036748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:44: Attribute 'backbone_wrapper' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['backbone_wrapper'])`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                                   </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                  </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>‚îÇ feature_encoder                        ‚îÇ AllCellFeatureEncoder ‚îÇ    914 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>‚îÇ feature_encoder.encoder_0              ‚îÇ BaseEncoder           ‚îÇ    914 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>‚îÇ feature_encoder.encoder_0.BN           ‚îÇ GraphNorm             ‚îÇ     18 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>‚îÇ feature_encoder.encoder_0.linear       ‚îÇ Linear                ‚îÇ    896 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>‚îÇ feature_encoder.encoder_0.relu         ‚îÇ ReLU                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>‚îÇ feature_encoder.encoder_0.dropout      ‚îÇ Dropout               ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>‚îÇ backbone                               ‚îÇ HypergraphWrapper     ‚îÇ 82.3 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>‚îÇ backbone.backbone                      ‚îÇ UniGCNII              ‚îÇ 82.0 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>‚îÇ backbone.backbone.input_drop           ‚îÇ Dropout               ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>‚îÇ backbone.backbone.layer_drop           ‚îÇ Dropout               ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>‚îÇ backbone.backbone.initial_linear_layer ‚îÇ Linear                ‚îÇ 16.5 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>‚îÇ backbone.backbone.layers               ‚îÇ ModuleList            ‚îÇ 65.5 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span>‚îÇ backbone.backbone.layers.0             ‚îÇ UniGCNIILayer         ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>‚îÇ backbone.backbone.layers.0.linear      ‚îÇ Linear                ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span>‚îÇ backbone.backbone.layers.0.conv        ‚îÇ Conv                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span>‚îÇ backbone.backbone.layers.1             ‚îÇ UniGCNIILayer         ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span>‚îÇ backbone.backbone.layers.1.linear      ‚îÇ Linear                ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span>‚îÇ backbone.backbone.layers.1.conv        ‚îÇ Conv                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 18 </span>‚îÇ backbone.backbone.layers.2             ‚îÇ UniGCNIILayer         ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span>‚îÇ backbone.backbone.layers.2.linear      ‚îÇ Linear                ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span>‚îÇ backbone.backbone.layers.2.conv        ‚îÇ Conv                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 </span>‚îÇ backbone.backbone.layers.3             ‚îÇ UniGCNIILayer         ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 </span>‚îÇ backbone.backbone.layers.3.linear      ‚îÇ Linear                ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 23 </span>‚îÇ backbone.backbone.layers.3.conv        ‚îÇ Conv                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 </span>‚îÇ backbone.ln_0                          ‚îÇ LayerNorm             ‚îÇ    256 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 </span>‚îÇ readout                                ‚îÇ PropagateSignalDown   ‚îÇ    129 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 </span>‚îÇ readout.linear                         ‚îÇ Linear                ‚îÇ    129 ‚îÇ train ‚îÇ\n",
       "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 27 </span>‚îÇ val_acc_best                           ‚îÇ MeanMetric            ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mName                                  \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mType                 \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m‚îÇ feature_encoder                        ‚îÇ AllCellFeatureEncoder ‚îÇ    914 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m‚îÇ feature_encoder.encoder_0              ‚îÇ BaseEncoder           ‚îÇ    914 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m‚îÇ feature_encoder.encoder_0.BN           ‚îÇ GraphNorm             ‚îÇ     18 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m‚îÇ feature_encoder.encoder_0.linear       ‚îÇ Linear                ‚îÇ    896 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m‚îÇ feature_encoder.encoder_0.relu         ‚îÇ ReLU                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m‚îÇ feature_encoder.encoder_0.dropout      ‚îÇ Dropout               ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m‚îÇ backbone                               ‚îÇ HypergraphWrapper     ‚îÇ 82.3 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone                      ‚îÇ UniGCNII              ‚îÇ 82.0 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.input_drop           ‚îÇ Dropout               ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layer_drop           ‚îÇ Dropout               ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.initial_linear_layer ‚îÇ Linear                ‚îÇ 16.5 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers               ‚îÇ ModuleList            ‚îÇ 65.5 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.0             ‚îÇ UniGCNIILayer         ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.0.linear      ‚îÇ Linear                ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m14\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.0.conv        ‚îÇ Conv                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m15\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.1             ‚îÇ UniGCNIILayer         ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m16\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.1.linear      ‚îÇ Linear                ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m17\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.1.conv        ‚îÇ Conv                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m18\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.2             ‚îÇ UniGCNIILayer         ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m19\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.2.linear      ‚îÇ Linear                ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m20\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.2.conv        ‚îÇ Conv                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m21\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.3             ‚îÇ UniGCNIILayer         ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m22\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.3.linear      ‚îÇ Linear                ‚îÇ 16.4 K ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m23\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.backbone.layers.3.conv        ‚îÇ Conv                  ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m24\u001b[0m\u001b[2m \u001b[0m‚îÇ backbone.ln_0                          ‚îÇ LayerNorm             ‚îÇ    256 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m25\u001b[0m\u001b[2m \u001b[0m‚îÇ readout                                ‚îÇ PropagateSignalDown   ‚îÇ    129 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m26\u001b[0m\u001b[2m \u001b[0m‚îÇ readout.linear                         ‚îÇ Linear                ‚îÇ    129 ‚îÇ train ‚îÇ\n",
       "‚îÇ\u001b[2m \u001b[0m\u001b[2m27\u001b[0m\u001b[2m \u001b[0m‚îÇ val_acc_best                           ‚îÇ MeanMetric            ‚îÇ      0 ‚îÇ train ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 83.3 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 83.3 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 28                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 83.3 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 83.3 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 28                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f89f5fd94b84264b9975da586dd6e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f053c9f2ace48b186d563b8951d868e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/mae improved. New best score: 0.649\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Restoring states from the checkpoint path at ./checkpoints/epoch=4-step=5.ckpt\n",
      "Loaded model weights from the checkpoint at ./checkpoints/epoch=4-step=5.ckpt\n",
      "/Users/leone/miniconda3/envs/tb_challenge/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8592f8b3633e4f3f8a87800d702718d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\">        Test metric        </span>‚îÉ<span style=\"font-weight: bold\">       DataLoader 0        </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">     0.767922043800354     </span>‚îÇ\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.6569849252700806     </span>‚îÇ\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">     0.767922043800354     </span>‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m    0.767922043800354    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m   0.6569849252700806    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m    0.767922043800354    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import DictConfig\n",
    "from topobench.dataloader import TBDataloader\n",
    "from topobench.utils import instantiate_callbacks\n",
    "\n",
    "# It's good practice to clear Hydra's state in a notebook \n",
    "# in case you run the cell multiple times.\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "\n",
    "def run(cfg: DictConfig) -> DictConfig:\n",
    "    \"\"\"Run pipeline with given configuration.\"\"\"\n",
    "    # Instantiate and load dataset\n",
    "    dataset_loader = hydra.utils.instantiate(cfg.dataset.loader)\n",
    "    dataset, dataset_dir = dataset_loader.load()\n",
    "\n",
    "    # Preprocess dataset and load the splits\n",
    "    transform_config = cfg.get(\"transforms\", None)\n",
    "    preprocessor = PreProcessor(dataset, dataset_dir, transform_config)\n",
    "    dataset_train, dataset_val, dataset_test = (\n",
    "        preprocessor.load_dataset_splits(cfg.dataset.split_params)\n",
    "    )\n",
    "    \n",
    "    # Prepare datamodule\n",
    "    if cfg.dataset.parameters.task_level in [\"node\", \"graph\"]:\n",
    "        datamodule = TBDataloader(\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_val,\n",
    "            dataset_test=dataset_test,\n",
    "            **cfg.dataset.get(\"dataloader_params\", {}),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task_level\")\n",
    "\n",
    "    # Model for us is Network + logic: inputs backbone, readout, losses\n",
    "    model = hydra.utils.instantiate(\n",
    "        cfg.model,\n",
    "        evaluator=cfg.evaluator,\n",
    "        optimizer=cfg.optimizer,\n",
    "        loss=cfg.loss,\n",
    "    )\n",
    "    callbacks = instantiate_callbacks(cfg.get(\"callbacks\"))\n",
    "\n",
    "    # === FIX IS HERE ===\n",
    "    # Manually set the output directory for the Trainer since we are in a notebook\n",
    "    trainer = hydra.utils.instantiate(\n",
    "        cfg.trainer,\n",
    "        callbacks=callbacks,\n",
    "        logger=False,\n",
    "        num_sanity_val_steps=0,\n",
    "        default_root_dir=\".\"  # Explicitly set the output directory\n",
    "    )\n",
    "    \n",
    "    trainer.fit(\n",
    "        model=model, datamodule=datamodule, ckpt_path=cfg.get(\"ckpt_path\")\n",
    "    )\n",
    "    ckpt_path = trainer.checkpoint_callback.best_model_path\n",
    "    trainer.test(\n",
    "        model=model, datamodule=datamodule, ckpt_path=ckpt_path\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Your calling code remains the same ---\n",
    "with initialize(config_path=\"../configs\", job_name=\"job\"):\n",
    "    cfg = compose(\n",
    "        config_name=\"run.yaml\",\n",
    "        overrides=[\n",
    "            \"model=hypergraph/unignn2\",\n",
    "            \"dataset=graph/US-county-demos\",\n",
    "            \"callbacks=notebook\"\n",
    "        ], \n",
    "        return_hydra_config=True\n",
    "    )\n",
    "    cfg.trainer.max_epochs=5\n",
    "    run(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
