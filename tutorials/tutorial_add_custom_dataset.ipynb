{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Adding a Custom Dataset Tutorial\n",
    "\n",
    "## 🎯 Tutorial Overview\n",
    "\n",
    "This comprehensive guide walks you through the process of integrating your custom dataset into our library. The process is divided into three main steps:\n",
    "\n",
    "1. **Dataset Creation** 🔨\n",
    "   - Implement data loading mechanisms\n",
    "   - Define preprocessing steps\n",
    "   - Structure data in the required format\n",
    "\n",
    "2. **Integrate with Dataset APIs** 🔄\n",
    "   - Add dataset to the library framework\n",
    "   - Ensure compatibility with existing systems\n",
    "   - Set up proper inheritance structure\n",
    "\n",
    "3. **Configuration Setup** ⚙️\n",
    "   - Define dataset parameters\n",
    "   - Specify data paths and formats\n",
    "   - Configure preprocessing options\n",
    "\n",
    "## 📋 Tutorial Structure\n",
    "\n",
    "This tutorial follows a unique structure to provide the clearest possible learning experience:\n",
    "\n",
    "> 💡 **Main Notebook (Current File)**\n",
    "> - High-level concepts and explanations\n",
    "> - Step-by-step workflow description\n",
    "> - References to implementation files\n",
    "\n",
    "> 📁 **Supporting Files**\n",
    "> - Detailed code implementations\n",
    "> - Specific examples and use cases\n",
    "> - Technical documentation\n",
    "\n",
    "### 🛠️ Technical Framework\n",
    "\n",
    "This tutorial demonstrates custom dataset integration using:\n",
    "- `torch_geometric.data.InMemoryDataset` as the base class\n",
    "- <TB_name> library's dataset management system\n",
    "\n",
    "### 🎓 Important Notes\n",
    "\n",
    "- To make the learning process concrete, we'll work with a practical toy \"language\" dataset example:\n",
    "- While we use the \"language\" dataset as an example, all file references use the generic `<dataset_name>` format for better generalization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a Dataset 🛠️\n",
    "\n",
    "## Overview\n",
    "\n",
    "Adding your custom dataset to <TB_name> requires implementing specific loading and preprocessing functionality. We utilize the `torch_geometric.data.InMemoryDataset` interface to make this process straightforward.\n",
    "\n",
    "## Required Methods\n",
    "\n",
    "To implement your dataset, you need to override two key methods from the `torch_geometric.data.InMemoryDataset` class:\n",
    "\n",
    "- `download()`: Handles dataset acquisition\n",
    "- `process()`: Manages data preprocessing\n",
    "\n",
    "> 💡 **Reference Implementation**: For a complete examples, see directory `topobench/data/datasets/` files `us_county_demos_dataset.py`, `mantra_dataset.py`, etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: The Download Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `download()` method is responsible for acquiring dataset files from external resources. Let's examine its implementation using our language dataset example, where we store data in a GoogleDrive-hosted zip file.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. **Download Data** 📥\n",
    "  - Fetch data from the specified source URL\n",
    "  - Save to the raw directory\n",
    "\n",
    "2. **Extract Content** 📦\n",
    "  - Unzip the downloaded file\n",
    "  - Place contents in appropriate directory\n",
    "\n",
    "3. **Organize Files** 📂\n",
    "  - Move extracted files to named folders\n",
    "  - Clean up temporary files and directories\n",
    "\n",
    "#### Code Implementation\n",
    "\n",
    "```python\n",
    "def download(self) -> None:\n",
    "    r\"\"\"Download the dataset from a URL and saves it to the raw directory.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the dataset URL is not found.\n",
    "    \"\"\"\n",
    "    # Step 1: Download data from the source\n",
    "    self.url = self.URLS[self.name]\n",
    "    self.file_format = self.FILE_FORMAT[self.name]\n",
    "    download_file_from_drive(\n",
    "        file_link=self.url,\n",
    "        path_to_save=self.raw_dir,\n",
    "        dataset_name=self.name,\n",
    "        file_format=self.file_format,\n",
    "    )\n",
    "    \n",
    "    # Step 2: extract zip file\n",
    "    folder = self.raw_dir\n",
    "    filename = f\"{self.name}.{self.file_format}\"\n",
    "    path = osp.join(folder, filename)\n",
    "    extract_zip(path, folder)\n",
    "    # Delete zip file\n",
    "    os.unlink(path)\n",
    "    \n",
    "    # Step 3: organize files\n",
    "    # Move files from osp.join(folder, name_download) to folder\n",
    "    for file in os.listdir(osp.join(folder, self.name)):\n",
    "        shutil.move(osp.join(folder, self.name, file), folder)\n",
    "    # Delete osp.join(folder, self.name) dir\n",
    "    shutil.rmtree(osp.join(folder, self.name))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive: The Process Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process()` method handles data preprocessing and organization. Here's the method's structure:\n",
    "\n",
    "```python\n",
    "def process(self) -> None:\n",
    "   r\"\"\"Handle the data for the dataset.\n",
    "   \n",
    "   This method loads the Language dataset, applies preprocessing \n",
    "   transformations, and saves processed data.\"\"\"\n",
    "\n",
    "   # Step 1: extract the data\n",
    "   ...  # Convert raw data to list of torch_geometric.data.Data objects\n",
    "\n",
    "   # Step 2: collate the graphs\n",
    "   self.data, self.slices = self.collate(graph_sentences)\n",
    "\n",
    "   # Step 3: save processed data\n",
    "   fs.torch_save(\n",
    "       (self._data.to_dict(), self.slices, {}, self._data.__class__),\n",
    "       self.processed_paths[0],\n",
    "   )\n",
    "\n",
    "\n",
    "```self.collate``` -- Collates a list of Data or HeteroData objects to the internal storage format; meaning that it transforms a list of torch.data.Data objectis into one torch.data.BaseData.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Integrate with Dataset APIs 🔄\n",
    "\n",
    "Now that we have created a dataset class, we need to integrate it with the library. In this section we describe where to add the dataset files and how to make it available through data loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to structure your files, the files highlighted with ** are going to be updated: \n",
    "```yaml\n",
    "topobench/\n",
    "├── data/\n",
    "│   ├── datasets/\n",
    "│   │   ├── **init.py**\n",
    "│   │   ├── base.py\n",
    "│   │   ├── <dataset_name>.py   # Your dataset file\n",
    "│   │   └── ...\n",
    "│   ├── loaders/\n",
    "│   │   ├── init.py\n",
    "│   │   ├── base.py\n",
    "│   │   ├── graph/\n",
    "│   │   │   ├── <loader_name>.py   # Your loader file\n",
    "│   │   ├── hypergraph/\n",
    "│   │   │   ├── <loader_name>.py   # Your loader file\n",
    "│   │   ├── .../\n",
    "```\n",
    "\n",
    "To make your dataset available to library:\n",
    "\n",
    "The file ```<dataset_name>.py```  has been created during the previous steps (`us_county_demos_dataset.py` in our case) and should be placed in the `topobench/data/datasets/` directory. \n",
    "\n",
    "\n",
    "The registry in `topobench/data/datasets/__init__.py` discovers the files in `topobench/data/datasets` and updates `__all__` variable of `topobench/data/datasets/__init__.py` automatically. Hence there is no need to update the `__init__.py` file manually to allow your dataset to be loaded by the library. Simply creare a file `<dataset_name>.py` and place it in the  `topobench/data/datasets/` directory.\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "Next it is required to update the data loader system. Modify the loader file (`topobench/data/loaders/loaders.py`:) to include your custom dataset:\n",
    "\n",
    "For the example dataset the loader file ```topobench/data/loaders/graph/us_county_demos_dataset_loader.py``` consist of the following:\n",
    "\n",
    "```python\n",
    "class USCountyDemosDatasetLoader(AbstractLoader):\n",
    "    \"\"\"Load US County Demos dataset with configurable year and task variable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : DictConfig\n",
    "        Configuration parameters containing:\n",
    "            - data_dir: Root directory for data\n",
    "            - data_name: Name of the dataset\n",
    "            - year: Year of the dataset (if applicable)\n",
    "            - task_variable: Task variable for the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parameters: DictConfig) -> None:\n",
    "        super().__init__(parameters)\n",
    "\n",
    "    def load_dataset(self) -> USCountyDemosDataset:\n",
    "        \"\"\"Load the US County Demos dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        USCountyDemosDataset\n",
    "            The loaded US County Demos dataset with the appropriate `data_dir`.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If dataset loading fails.\n",
    "        \"\"\"\n",
    "\n",
    "        dataset = self._initialize_dataset()\n",
    "        self.data_dir = self._redefine_data_dir(dataset)\n",
    "        return dataset\n",
    "\n",
    "    def _initialize_dataset(self) -> USCountyDemosDataset:\n",
    "        \"\"\"Initialize the US County Demos dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        USCountyDemosDataset\n",
    "            The initialized dataset instance.\n",
    "        \"\"\"\n",
    "        return USCountyDemosDataset(\n",
    "            root=str(self.root_data_dir),\n",
    "            name=self.parameters.data_name,\n",
    "            parameters=self.parameters,\n",
    "        )\n",
    "\n",
    "    def _redefine_data_dir(self, dataset: USCountyDemosDataset) -> Path:\n",
    "        \"\"\"Redefine the data directory based on the chosen (year, task_variable) pair.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : USCountyDemosDataset\n",
    "            The dataset instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Path\n",
    "            The redefined data directory path.\n",
    "        \"\"\"\n",
    "        return dataset.processed_root\n",
    "```\n",
    "The loader class have to inherit from the `AbstractLoader` where the method ```load_dataset``` is required while other methods are optional used for convenience and structure.\n",
    "\n",
    "## Notes:\n",
    "- The  ```load_dataset``` of ```AbstractLoader``` class requires to return ```torch.utils.data.Dataset``` object. \n",
    "- ### **Important:** to allow the automatic registering of the loader, make sure to include \"DatasetLoader\" into name of loader class (Example: USCountyDemos**DatasetLoader**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Configuration 🔧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've integrated our dataset, we need to define its configuration parameters. In this section, we'll explain how to create and structure the configuration file for your dataset.\n",
    "\n",
    "## Configuration File Structure\n",
    "Create a new YAML file for your dataset in `configs/dataset/<dataset_name>.yaml` with the following structure:\n",
    "\n",
    "\n",
    "### While creating a configuration file, you will need to specify: \n",
    "\n",
    "1) Loader class (`topobench.data.loaders.USCountyDemosDatasetLoader`) for automatic instantialization inside the provided pipeline and the parameters for the loader.\n",
    "```yaml\n",
    "# Dataset loader config\n",
    "loader:\n",
    "  _target_: topobench.data.loaders.USCountyDemosDatasetLoader\n",
    "  parameters: \n",
    "    data_domain: graph             # Primary data domain. Options: ['graph', 'hypergrpah', 'cell, 'simplicial']\n",
    "    data_type: cornel              # Data type. String emphasizing from where dataset come from. \n",
    "    data_name: US-county-demos     # Name of the dataset\n",
    "    year: 2012                     # In the case of US-county-demos there are multiple version of this dataset. Options:[2012, 2016]\n",
    "    task_variable: 'Election'      # Different target variable used as target. Options: ['Election', 'MedianIncome', 'MigraRate', 'BirthRate', 'DeathRate', 'BachelorRate', 'UnemploymentRate']\n",
    "    data_dir: ${paths.data_dir}/${dataset.loader.parameters.data_domain}/${dataset.loader.parameters.data_type}\n",
    "``` \n",
    "\n",
    "2) The dataset parameters: \n",
    "\n",
    "```yaml\n",
    "# Dataset parameters\n",
    "parameters:\n",
    "  num_features: 6         # Number of features in the dataset\n",
    "  num_classes: 1          # Dimentuin of the target variable\n",
    "  task: regression        # Dataset task. Options: [classification, regression]\n",
    "  loss_type: mse          # Task-specific loss function\n",
    "  monitor_metric: mae     # Metric to monitor during training\n",
    "  task_level: node        # Task level. Options: [classification, regression]\n",
    "```\n",
    "\n",
    "3) The dataset split parameters: \n",
    "```yaml\n",
    "#splits\n",
    "split_params:\n",
    "  learning_setting: transductive      # Type of learning. Options:['transductive', 'inductive']\n",
    "  data_seed: 0                        # Seed for data splitting\n",
    "  split_type: random                  # Type of splitting. Options: ['k-fold', 'random']\n",
    "  k: 10                               # Number of folds in case of \"k-fold\" cross-validation\n",
    "  train_prop: 0.5                     # Training proportion in case of 'random' splitting strategy\n",
    "  standardize: True                   # Standardize the data or not. Options: [True, False]\n",
    "  data_split_dir: ${paths.data_dir}/data_splits/${dataset.loader.parameters.data_name}\n",
    "```\n",
    "\n",
    "4) Finally the dataloader parameters:\n",
    "\n",
    "```yaml\n",
    "# Dataloader parameters\n",
    "dataloader_params:\n",
    "  batch_size: 1       # Number of graphs per batch. In sace of transductive always 1 as there is only one graph. \n",
    "  num_workers: 0      # Number of workers for data loading\n",
    "  pin_memory: False   # Pin memory for data loading\n",
    "```\n",
    "\n",
    "### Notes:\n",
    "- The `paths` section in the configuration file is automatically populated with the paths to the data directory and the data splits directory.\n",
    "- Some of the dataset parameters are used to configure the model.yaml and other files. Hence we suggest always include the above parameters in the dataset configuration file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the markdown for easy copying:\n",
    "\n",
    "\n",
    "## Preparing to Load the Custom Dataset: Understanding Configuration Imports\n",
    "\n",
    "Before loading our dataset, it's crucial to understand the configuration imports, particularly those from the `topobench.utils.config_resolvers` module. These utility functions play a key role in dynamically configuring your machine learning pipeline.\n",
    "\n",
    "### Key Imports for Dynamic Configuration\n",
    "\n",
    "Let's import the essential configuration resolver functions:\n",
    "\n",
    "```python\n",
    "from topobench.utils.config_resolvers import (\n",
    "    get_default_transform,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    infer_in_channels,\n",
    ")\n",
    "```\n",
    "\n",
    "### Why These Imports Matter\n",
    "\n",
    "In our previous step, we explored configuration variables that use dynamic lookups, such as:\n",
    "\n",
    "```yaml\n",
    "data_dir: ${paths.data_dir}/${dataset.loader.parameters.data_domain}/${dataset.loader.parameters.data_type}\n",
    "```\n",
    "\n",
    "However, some configurations require more advanced automation, which is where these imported functions become invaluable.\n",
    "\n",
    "### Practical Example: Dynamic Transforms\n",
    "\n",
    "Consider the configuration in `projects/TopoBench/configs/run.yaml`, where the `transforms` parameter uses the `get_default_transform` function:\n",
    "\n",
    "```yaml\n",
    "transforms: ${get_default_transform:${dataset},${model}}\n",
    "```\n",
    "\n",
    "This syntax allows for automatic transformation selection based on the dataset and model, demonstrating the power of these configuration resolver functions.\n",
    "\n",
    "By importing and utilizing these functions, you gain:\n",
    "- Flexible configuration management\n",
    "- Automatic parameter inference\n",
    "- Reduced manual configuration overhead\n",
    "\n",
    "These facilitate seamless dataset loading and preprocessing for multiple topological domains and provide an easy and intuitive interface for incorporating novel functionality.\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2473879/1604224440.py:21: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../configs\", job_name=\"job\"):\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "\n",
    "\n",
    "from topobench.utils.config_resolvers import (\n",
    "    get_default_metrics,\n",
    "    get_default_trainer,\n",
    "    get_default_transform,\n",
    "    get_flattened_channels,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    get_non_relational_out_channels,\n",
    "    get_required_lifting,\n",
    "    infer_in_channels,\n",
    "    infer_num_cell_dimensions,\n",
    ")\n",
    "\n",
    "\n",
    "with initialize(config_path=\"../configs\", job_name=\"job\"):\n",
    "\n",
    "    cfg = compose(\n",
    "        config_name=\"run.yaml\",\n",
    "        overrides=[\n",
    "            \"model=hypergraph/unignn2\",\n",
    "            \"dataset=graph/US-county-demos\",\n",
    "        ], \n",
    "        return_hydra_config=True\n",
    "    )\n",
    "loader = instantiate(cfg.dataset.loader)\n",
    "\n",
    "\n",
    "dataset, dataset_dir = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-county-demos(self.root=/home/levtel/projects/TopoBench/datasets/graph/cornel, self.name=US-county-demos, self.parameters={'data_domain': 'graph', 'data_type': 'cornel', 'data_name': 'US-county-demos', 'year': 2012, 'task_variable': 'Election', 'data_dir': '/home/levtel/projects/TopoBench/datasets/graph/cornel'}, self.force_reload=False)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3224, 6], edge_index=[2, 18966], y=[3224])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the default transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform name: dict_keys(['graph2hypergraph_lifting'])\n"
     ]
    }
   ],
   "source": [
    "print('Transform name:', cfg.transforms.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: /home/levtel/projects/TopoBench/datasets/graph/cornel/US-county-demos/graph2hypergraph_lifting/3613529153\n"
     ]
    }
   ],
   "source": [
    "from topobench.data.preprocessor import PreProcessor\n",
    "preprocessed_dataset = PreProcessor(dataset, dataset_dir, cfg['transforms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3224, 6], edge_index=[2, 18966], y=[3224], incidence_hyperedges=[3224, 3224], num_hyperedges=3224, x_0=[3224, 6], x_hyperedges=[3224, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is integrated, lets run some model: \n",
    "\n",
    "CLI command: `python -m topobench model=hypergraph/unignn2 dataset=graph/US-county-demos`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2473879/1017559150.py:65: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../configs\", job_name=\"job\"):\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:44: Attribute 'backbone_wrapper' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['backbone_wrapper'])`.\n",
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory ./checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: /home/levtel/projects/TopoBench/datasets/graph/cornel/US-county-demos/graph2hypergraph_lifting/3613529153\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                                   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>│ feature_encoder                        │ AllCellFeatureEncoder │ 17.8 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>│ feature_encoder.encoder_0              │ BaseEncoder           │ 17.8 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>│ feature_encoder.encoder_0.linear1      │ Linear                │    896 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>│ feature_encoder.encoder_0.linear2      │ Linear                │ 16.5 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>│ feature_encoder.encoder_0.relu         │ ReLU                  │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>│ feature_encoder.encoder_0.BN           │ GraphNorm             │    384 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>│ feature_encoder.encoder_0.dropout      │ Dropout               │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>│ backbone                               │ HypergraphWrapper     │ 82.3 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>│ backbone.backbone                      │ UniGCNII              │ 82.0 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>│ backbone.backbone.input_drop           │ Dropout               │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>│ backbone.backbone.layer_drop           │ Dropout               │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>│ backbone.backbone.initial_linear_layer │ Linear                │ 16.5 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span>│ backbone.backbone.layers               │ ModuleList            │ 65.5 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>│ backbone.backbone.layers.0             │ UniGCNIILayer         │ 16.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span>│ backbone.backbone.layers.0.linear      │ Linear                │ 16.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span>│ backbone.backbone.layers.0.conv        │ Conv                  │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span>│ backbone.backbone.layers.1             │ UniGCNIILayer         │ 16.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span>│ backbone.backbone.layers.1.linear      │ Linear                │ 16.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 18 </span>│ backbone.backbone.layers.1.conv        │ Conv                  │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span>│ backbone.backbone.layers.2             │ UniGCNIILayer         │ 16.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span>│ backbone.backbone.layers.2.linear      │ Linear                │ 16.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 </span>│ backbone.backbone.layers.2.conv        │ Conv                  │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 </span>│ backbone.backbone.layers.3             │ UniGCNIILayer         │ 16.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 23 </span>│ backbone.backbone.layers.3.linear      │ Linear                │ 16.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 </span>│ backbone.backbone.layers.3.conv        │ Conv                  │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 </span>│ backbone.ln_0                          │ LayerNorm             │    256 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 </span>│ readout                                │ PropagateSignalDown   │    129 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 27 </span>│ readout.linear                         │ Linear                │    129 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 28 </span>│ val_acc_best                           │ MeanMetric            │      0 │ train │\n",
       "└────┴────────────────────────────────────────┴───────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                                  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ feature_encoder                        │ AllCellFeatureEncoder │ 17.8 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ feature_encoder.encoder_0              │ BaseEncoder           │ 17.8 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ feature_encoder.encoder_0.linear1      │ Linear                │    896 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ feature_encoder.encoder_0.linear2      │ Linear                │ 16.5 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ feature_encoder.encoder_0.relu         │ ReLU                  │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ feature_encoder.encoder_0.BN           │ GraphNorm             │    384 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ feature_encoder.encoder_0.dropout      │ Dropout               │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ backbone                               │ HypergraphWrapper     │ 82.3 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ backbone.backbone                      │ UniGCNII              │ 82.0 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.input_drop           │ Dropout               │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layer_drop           │ Dropout               │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.initial_linear_layer │ Linear                │ 16.5 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers               │ ModuleList            │ 65.5 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.0             │ UniGCNIILayer         │ 16.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m14\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.0.linear      │ Linear                │ 16.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m15\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.0.conv        │ Conv                  │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m16\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.1             │ UniGCNIILayer         │ 16.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m17\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.1.linear      │ Linear                │ 16.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m18\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.1.conv        │ Conv                  │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m19\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.2             │ UniGCNIILayer         │ 16.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m20\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.2.linear      │ Linear                │ 16.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m21\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.2.conv        │ Conv                  │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m22\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.3             │ UniGCNIILayer         │ 16.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m23\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.3.linear      │ Linear                │ 16.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m24\u001b[0m\u001b[2m \u001b[0m│ backbone.backbone.layers.3.conv        │ Conv                  │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m25\u001b[0m\u001b[2m \u001b[0m│ backbone.ln_0                          │ LayerNorm             │    256 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m26\u001b[0m\u001b[2m \u001b[0m│ readout                                │ PropagateSignalDown   │    129 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m27\u001b[0m\u001b[2m \u001b[0m│ readout.linear                         │ Linear                │    129 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m28\u001b[0m\u001b[2m \u001b[0m│ val_acc_best                           │ MeanMetric            │      0 │ train │\n",
       "└────┴────────────────────────────────────────┴───────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 100 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 100 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 29                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 100 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 100 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 29                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd72e982eac84a98877c5b5a6bf2f23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73392e171e74f1fab393de2675099a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/mae improved. New best score: 0.735\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Restoring states from the checkpoint path at ./checkpoints/epoch=4-step=5-v1.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loaded model weights from the checkpoint at ./checkpoints/epoch=4-step=5-v1.ckpt\n",
      "/home/levtel/miniconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cb3a86b2174387982e4db48351ee85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0259636640548706     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7735075950622559     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0259636640548706     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0259636640548706    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7735075950622559    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0259636640548706    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import DictConfig\n",
    "from topobench.dataloader import TBDataloader\n",
    "from topobench.utils import instantiate_callbacks\n",
    "\n",
    "# It's good practice to clear Hydra's state in a notebook \n",
    "# in case you run the cell multiple times.\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "\n",
    "def run(cfg: DictConfig) -> DictConfig:\n",
    "    \"\"\"Run pipeline with given configuration.\"\"\"\n",
    "    # Instantiate and load dataset\n",
    "    dataset_loader = hydra.utils.instantiate(cfg.dataset.loader)\n",
    "    dataset, dataset_dir = dataset_loader.load()\n",
    "\n",
    "    # Preprocess dataset and load the splits\n",
    "    transform_config = cfg.get(\"transforms\", None)\n",
    "    preprocessor = PreProcessor(dataset, dataset_dir, transform_config)\n",
    "    dataset_train, dataset_val, dataset_test = (\n",
    "        preprocessor.load_dataset_splits(cfg.dataset.split_params)\n",
    "    )\n",
    "    \n",
    "    # Prepare datamodule\n",
    "    if cfg.dataset.parameters.task_level in [\"node\", \"graph\"]:\n",
    "        datamodule = TBDataloader(\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_val,\n",
    "            dataset_test=dataset_test,\n",
    "            **cfg.dataset.get(\"dataloader_params\", {}),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task_level\")\n",
    "\n",
    "    # Model for us is Network + logic: inputs backbone, readout, losses\n",
    "    model = hydra.utils.instantiate(\n",
    "        cfg.model,\n",
    "        evaluator=cfg.evaluator,\n",
    "        optimizer=cfg.optimizer,\n",
    "        loss=cfg.loss,\n",
    "    )\n",
    "    callbacks = instantiate_callbacks(cfg.get(\"callbacks\"))\n",
    "\n",
    "    # === FIX IS HERE ===\n",
    "    # Manually set the output directory for the Trainer since we are in a notebook\n",
    "    trainer = hydra.utils.instantiate(\n",
    "        cfg.trainer,\n",
    "        callbacks=callbacks,\n",
    "        logger=False,\n",
    "        num_sanity_val_steps=0,\n",
    "        default_root_dir=\".\"  # Explicitly set the output directory\n",
    "    )\n",
    "    \n",
    "    trainer.fit(\n",
    "        model=model, datamodule=datamodule, ckpt_path=cfg.get(\"ckpt_path\")\n",
    "    )\n",
    "    ckpt_path = trainer.checkpoint_callback.best_model_path\n",
    "    trainer.test(\n",
    "        model=model, datamodule=datamodule, ckpt_path=ckpt_path\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Your calling code remains the same ---\n",
    "with initialize(config_path=\"../configs\", job_name=\"job\"):\n",
    "    cfg = compose(\n",
    "        config_name=\"run.yaml\",\n",
    "        overrides=[\n",
    "            \"model=hypergraph/unignn2\",\n",
    "            \"dataset=graph/US-county-demos\",\n",
    "            \"callbacks=notebook\"\n",
    "        ], \n",
    "        return_hydra_config=True\n",
    "    )\n",
    "    cfg.trainer.max_epochs=5\n",
    "    run(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
