{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Adding a Custom Dataset Tutorial\n",
    "\n",
    "## 🎯 Tutorial Overview\n",
    "\n",
    "This comprehensive guide walks you through the process of integrating your custom dataset into our library. The process is divided into three main steps:\n",
    "\n",
    "1. **Dataset Creation** 🔨\n",
    "   - Implement data loading mechanisms\n",
    "   - Define preprocessing steps\n",
    "   - Structure data in the required format\n",
    "\n",
    "2. **Integrate with Dataset APIs** 🔄\n",
    "   - Add dataset to the library framework\n",
    "   - Ensure compatibility with existing systems\n",
    "   - Set up proper inheritance structure\n",
    "\n",
    "3. **Configuration Setup** ⚙️\n",
    "   - Define dataset parameters\n",
    "   - Specify data paths and formats\n",
    "   - Configure preprocessing options\n",
    "\n",
    "## 📋 Tutorial Structure\n",
    "\n",
    "This tutorial follows a unique structure to provide the clearest possible learning experience:\n",
    "\n",
    "> 💡 **Main Notebook (Current File)**\n",
    "> - High-level concepts and explanations\n",
    "> - Step-by-step workflow description\n",
    "> - References to implementation files\n",
    "\n",
    "> 📁 **Supporting Files**\n",
    "> - Detailed code implementations\n",
    "> - Specific examples and use cases\n",
    "> - Technical documentation\n",
    "\n",
    "### 🛠️ Technical Framework\n",
    "\n",
    "This tutorial demonstrates custom dataset integration using:\n",
    "- `torch_geometric.data.InMemoryDataset` as the base class\n",
    "- <TBX_name> library's dataset management system\n",
    "\n",
    "### 🎓 Important Notes\n",
    "\n",
    "- To make the learning process concrete, we'll work with a practical toy \"language\" dataset example:\n",
    "- While we use the \"language\" dataset as an example, all file references use the generic `<dataset_name>` format for better generalization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a Dataset 🛠️\n",
    "\n",
    "## Overview\n",
    "\n",
    "Adding your custom dataset to <TBX_name> requires implementing specific loading and preprocessing functionality. We utilize the `torch_geometric.data.InMemoryDataset` interface to make this process straightforward.\n",
    "\n",
    "## Required Methods\n",
    "\n",
    "To implement your dataset, you need to override two key methods from the `torch_geometric.data.InMemoryDataset` class:\n",
    "\n",
    "- `download()`: Handles dataset acquisition\n",
    "- `process()`: Manages data preprocessing\n",
    "\n",
    "> 💡 **Reference Implementation**: For a complete example, check `topobenchmarkx/data/datasets/language_dataset.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: The Download Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `download()` method is responsible for acquiring dataset files from external resources. Let's examine its implementation using our language dataset example, where we store data in a GoogleDrive-hosted zip file.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. **Download Data** 📥\n",
    "  - Fetch data from the specified source URL\n",
    "  - Save to the raw directory\n",
    "\n",
    "2. **Extract Content** 📦\n",
    "  - Unzip the downloaded file\n",
    "  - Place contents in appropriate directory\n",
    "\n",
    "3. **Organize Files** 📂\n",
    "  - Move extracted files to named folders\n",
    "  - Clean up temporary files and directories\n",
    "\n",
    "#### Code Implementation\n",
    "\n",
    "```python\n",
    "def download(self) -> None:\n",
    "    # Step 1: download data from source\n",
    "    self.url = self.URLS[self.name]\n",
    "    self.file_format = self.FILE_FORMAT[self.name]\n",
    "    download_file_from_drive(file_link=self.url, path_to_save=self.raw_dir, \n",
    "                           dataset_name=self.name, file_format=self.file_format)\n",
    "\n",
    "    # Step 2: extract zip file\n",
    "    folder = self.raw_dir\n",
    "    filename = f\"{self.name}.{self.file_format}\"\n",
    "    path = osp.join(folder, filename)\n",
    "    extract_zip(path, folder)\n",
    "    os.unlink(path)  # Delete zip\n",
    "    \n",
    "    # Step 3: organize files\n",
    "    for file in os.listdir(osp.join(folder, self.name)):\n",
    "        if file.endswith('ipynb'): continue\n",
    "        shutil.move(osp.join(folder, self.name, file), osp.join(folder, file))\n",
    "    shutil.rmtree(osp.join(folder, self.name))  # Cleanup\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: The Process Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process()` method handles data preprocessing and organization. Here's the method's structure:\n",
    "\n",
    "```python\n",
    "def process(self) -> None:\n",
    "   r\"\"\"Handle the data for the dataset.\n",
    "   \n",
    "   This method loads the Language dataset, applies preprocessing \n",
    "   transformations, and saves processed data.\"\"\"\n",
    "\n",
    "   # Step 1: extract the data\n",
    "   ...  # Convert raw data to list of torch_geometric.data.Data objects\n",
    "\n",
    "   # Step 2: collate the graphs\n",
    "   self.data, self.slices = self.collate(graph_sentences)\n",
    "\n",
    "   # Step 3: save processed data\n",
    "   fs.torch_save(\n",
    "       (self._data.to_dict(), self.slices, {}, self._data.__class__),\n",
    "       self.processed_paths[0],\n",
    "   )\n",
    "   self.graph = graph_sentences\n",
    "\n",
    "\n",
    "```self.collate``` - Quote: Collates a list of Data or HeteroData objects to the internal storage format; meaning that it transforms a list of torch.data.Data objectis into one torch.data.BaseData allowing for .. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Integrate with Dataset APIs 🔄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a dataset class, we need to integrate it with the benchmark library. In this section we describe where to add the dataset files and how to make it available through data loaders.\n",
    "\n",
    "\n",
    "Here's how to structure your files, the files highlighted with ** are going to be updated: \n",
    "```\n",
    "topobenchmarkx/\n",
    "├── data/\n",
    "│   ├── datasets/\n",
    "│   │   ├── **init.py**\n",
    "│   │   ├── base.py\n",
    "│   │   ├── <dataset_name>.py   # Your dataset file\n",
    "│   │   └── ...\n",
    "|   ├── loaders\n",
    "│   │   ├── init.py\n",
    "│   │   ├── **loaders.py**\n",
    "```\n",
    "\n",
    "To make your dataset available to library:\n",
    "\n",
    "The file ```<dataset_name>.py```  has been created during the previous steps (`language_dataset.py` in our case) and should be placed in the `topobenchmarkx/data/datasets/` directory. \n",
    "\n",
    "\n",
    "Now it is important Update Registry `topobenchmarkx/data/datasets/__init__.py` to include custom dataset into the library:\n",
    "\n",
    "```python\n",
    "from .<dataset_name> import <dataset_name_class>\n",
    "\n",
    "__all__ = [\n",
    "    # Other datasets...\n",
    "    '<dataset_name_class>',\n",
    "]\n",
    "```\n",
    "\n",
    "Next it is required to update the data loader system. Modify the loader file (`topobenchmarkx/data/loaders/loaders.py`:) to include your dataset:\n",
    "\n",
    "For the the toy example dataset we add the following into the ```load``` method of ```GraphLoader``` class: \n",
    "\n",
    "```python\n",
    "elif self.parameters.data_name in [\"LanguageDataset\"]:\n",
    "   dataset = LanguageDataset(\n",
    "       root=root_data_dir,\n",
    "       name=self.parameters[\"data_name\"],\n",
    "       parameters=self.parameters,\n",
    "   )\n",
    "   \n",
    "   data_dir = dataset.processed_root\n",
    "```\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- In `topobenchmarkx/data/loaders/loaders.py` we additionally provide a template for adding new dataset. \n",
    "- The  ```load``` of class ```GraphLoader``` has to return ```dataset: torch_geometric.data.Dataset``` and ```data_dir: str``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Configuration 🔧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've integrated our dataset, we need to define its configuration parameters. In this section, we'll explain how to create and structure the configuration file for your dataset.\n",
    "\n",
    "## Configuration File Structure\n",
    "Create a new YAML file for your dataset in `configs/dataset/<dataset_name>.yaml` with the following structure:\n",
    "\n",
    "```yaml\n",
    "# Dataset loader config\n",
    "loader:\n",
    " _target_: topobenchmarkx.data.loaders.GraphLoader\n",
    " parameters: \n",
    "   data_domain: graph\n",
    "   data_type: NLP\n",
    "   data_name: LanguageDataset\n",
    "   data_dir: ${paths.data_dir}/${dataset.loader.parameters.data_domain}/${dataset.loader.parameters.data_type}\n",
    "\n",
    "# Dataset parameters\n",
    "parameters:\n",
    " num_features: 0\n",
    " num_classes: 0\n",
    " task: regression\n",
    " loss_type: mse\n",
    " monitor_metric: mae\n",
    " task_level: node\n",
    "\n",
    "# Splits configuration\n",
    "split_params:\n",
    " learning_setting: transductive\n",
    " data_split_dir: ${paths.data_dir}/data_splits/${dataset.loader.parameters.data_name}\n",
    " data_seed: 0\n",
    " split_type: random  # or 'k-fold'\n",
    " k: 10               # for k-fold Cross-Validation\n",
    " train_prop: 0.5     # for random strategy\n",
    " standardize: True\n",
    "\n",
    "# Dataloader parameters\n",
    "dataloader_params:\n",
    " batch_size: 1       # Fixed\n",
    " num_workers: 0\n",
    " pin_memory: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Custom Data Transformations ⚙️\n",
    "\n",
    "While most datasets can be used directly after integration, some require specific preprocessing transformations. These transformations might vary depending on the task, model, or other conditions.\n",
    "\n",
    "## Example Case: Language Dataset\n",
    "\n",
    "Let's look at our language dataset's structure:\n",
    "- Each graph represents an English sentence\n",
    "- Nodes are tokens i.e. each node is a string and it doesn't have corresponding high dimentional feature embedding\n",
    "- Edges come from transformer attention, hence forming a fully connected graph\n",
    "\n",
    "For this dataset, two default transformations are logical:\n",
    "1. **Graph Sparsification**: Reduce edge density\n",
    "2. **Node Feature Generation**: Create numerical features from tokens\n",
    "\n",
    "\n",
    "Below we provide an quick tutorial on how to create a data transformations and create a sequence of default transformations that will be executed whener you use the defined dataset config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Transform\n",
    "\n",
    "In general any transfom in the library inherits `torch_geometric.transforms.BaseTransform` class, which allow to apply a sequency of transforms to the data. Our inderface requires to implement the `forward` method. The important part of all transforms is that it takes `torch_geometric.data.Data` object and returns updated `torch_geometric.data.Data` object.\n",
    "\n",
    "\n",
    "\n",
    "For language dataset,  we have generated the `attention2graph` transfroms that is a data_manipulation transform hence we place it into `topobenchmarkx/transforms/data_manipulation/` folder. \n",
    "Below you can see the `forward` method of `Attention2Graph` class: \n",
    "\n",
    "\n",
    "```python\n",
    "   def forward(self, data: torch_geometric.data.Data):\n",
    "       # Reshape attention scores\n",
    "       attention_shape = data.attention_shape\n",
    "       attention_scores = data.attention_scores.reshape(attention_shape)\n",
    "       \n",
    "       # Apply threshold\n",
    "       mask = attention_scores > self.parameters[\"threshold\"]\n",
    "       edge_index = torch.stack(torch.where(mask==1))\n",
    "       \n",
    "       # Process edges\n",
    "       edge_index = torch_geometric.utils.remove_self_loops(edge_index)[0]\n",
    "       edge_index = torch_geometric.utils.to_undirected(edge_index)\n",
    "       data.edge_index = edge_index\n",
    "       \n",
    "       return data\n",
    "```\n",
    "\n",
    "Please see the `topobenchmarkx/transforms/data_manipulation/attention2graph.py` file for the precise implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Transform\n",
    "\n",
    "Similarly as adding dataset we have to registed the transform we have created, to do so please follow the procedure below:\n",
    "\n",
    "Update `topobenchmarkx/transforms/data_manipulations/__init__.py`:\n",
    "\n",
    "``` python\n",
    "# Step 1: Import your transform\n",
    "from .attention2graph import Attention2Graph\n",
    "\n",
    "# Step 2: Add to DATA_MANIPULATIONS dictionary\n",
    "DATA_MANIPULATIONS = {\n",
    "    \"Identity\": IdentityTransform,\n",
    "    # ... other transforms ...\n",
    "    \"Attention2Graph\": Attention2Graph,  # Add your transform\n",
    "}\n",
    "\n",
    "# Step 3: Add to __all__\n",
    "__all__ = [\n",
    "    \"DATA_MANIPULATIONS\",\n",
    "    # ... other transforms ...\n",
    "    \"Attention2Graph\"  # Add your transform\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a configuration file \n",
    "Now as we have registered the transform we can finally create the configuration file and use it in the framework: \n",
    "\n",
    "``` yaml\n",
    "_target_: topobenchmarkx.transforms.data_transform.DataTransform\n",
    "transform_name: \"Attention2Graph\"\n",
    "transform_type: \"data manipulation\"\n",
    "threshold: 0.1\n",
    "``` \n",
    "Please refer to `configs/transforms/dataset_defaults/attention2graph.yaml` for the example. \n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- You might notice an interesting key `_target_` in the configuration file. In general for any new transform you the `_target_` is always `topobenchmarkx.transforms.data_transform.DataTransform`.  [For more information please refer to hydra documentation \"Instantiating objects with Hydra\" section.](https://hydra.cc/docs/advanced/instantiate_objects/overview/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default transforms\n",
    "\n",
    "Now when we have crated the transfor we can define a list of default transforms that will be executed always whenwever the dataset under default configuration is used.\n",
    "\n",
    "\n",
    "To configure the deafult transform navigate to `configs/transforms/dataset_defaults` create `<def_transforms.yaml>` and the follwoing `.yaml` file: \n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - transform_1: transform_1\n",
    "  - transform_2: transform_2\n",
    "  - transform_3: transform_3\n",
    "```\n",
    "\n",
    "\n",
    "**Important**\n",
    "There are different types of transforms, including `data_manipulation`, `liftings`, and `feature_liftings`. In case you want to use multiple transforms from the same categoty, let's say from `data_manipulation`, then it is required to stick to a special syntaxis. [See hydra configuration for more information]() or the example below: \n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - data_manipulation@first_usage: transform_1\n",
    "  - data_manipulation@second_usage: transform_2\n",
    "```\n",
    "\n",
    "\n",
    "In the case of the language dataset we have the following `language.yaml` file:\n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - data_manipulations@equal_gaus_features: equal_gaus_features\n",
    "  - data_manipulations@attention2graph: attention2graph\n",
    "  - liftings@_here_: ${get_required_lifting:graph,${model}}\n",
    "\n",
    "equal_gaus_features:\n",
    "  num_features: 10\n",
    "```\n",
    "\n",
    "\n",
    "In our example we have a bunch of interesting aspects: \n",
    "- There are a two transforms from the same catgory `data_manipulations`, hence we use operator `@` to assign new names `equal_gaus_features` and `attention2graph`\n",
    "-  In the case of `equal_gaus_features` we have to override the initial number of features as the `equal_gaus_features.yaml` uses a special \"configuration register\" to infer the feature dimension. In the case of our language dataset, we don't have node's hidden features, hence we need to define the number of features of our own. \n",
    "- We recommend to add `liftings@_here_: ${get_required_lifting:graph,${model}}` so that a default lifting is applied to run any domain-specific topological model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Add language_dataset.py file in topobenchmarkx/data/datasets folder\n",
    "\n",
    "# Step 2\n",
    "# Update __init__.py file in topobenchmarkx/data/datasets\n",
    "# Update loaders.py in topobenchmarkx/data/loaders\n",
    "\n",
    "# Step 3\n",
    "# Create confin file TopoBenchmark/configs/dataset/graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Preprocessing: \n",
    "# Optional: in case you need to have some default transform, create them following 1. and generate corresponding .yaml configuration files and add to configs/transforms/dataset_defaults folder \n",
    "\n",
    "\n",
    "# To create a transform see steps below\n",
    "# Step 1: Crate transform requitred for the dataset add it to appropriate topobenchmarkx/transforms folder\n",
    "# Step 1.1:\n",
    "# Step 1.2:\n",
    "# Step 1.3: \n",
    "# Step 1.4: Add configuration file associated with the transform into transforms/data_manipulations (see file attention2graph.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3178145/2587715312.py:26: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../configs\", job_name=\"job\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting /home/lev/projects/TopoBenchmark/datasets/graph/NLP/LanguageDataset/raw/LanguageDataset.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(\"./\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "import torch_geometric\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from topobenchmarkx.data.preprocessor import PreProcessor\n",
    "from topobenchmarkx.dataloader.dataloader import TBXDataloader\n",
    "from topobenchmarkx.data.loaders import GraphLoader\n",
    "\n",
    "from topobenchmarkx.utils.config_resolvers import (\n",
    "    get_default_transform,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    infer_in_channels,\n",
    ")\n",
    "\n",
    "\n",
    "initialize(config_path=\"../configs\", job_name=\"job\")\n",
    "\n",
    "cfg = compose(config_name=\"run.yaml\", return_hydra_config=True)\n",
    "graph_loader = GraphLoader(cfg.dataset.loader.parameters)\n",
    "dataset, dataset_dir = graph_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['transforms']['equal_gaus_features']['num_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "preprocessed_dataset = PreProcessor(dataset, dataset_dir, cfg['transforms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
