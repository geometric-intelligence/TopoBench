{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Custom Data Transformation and Configuration Tutorial\n",
    "\n",
    "## üéØ Tutorial Overview\n",
    "\n",
    "This guide provides a deep dive into the data transformation and configuration system in **TopoBench**. You will learn how to create, configure, and apply both default and custom preprocessing steps for any dataset.\n",
    "\n",
    "This tutorial covers:\n",
    "\n",
    "1.  **Understanding Dynamic Configuration** üß†\n",
    "    * Learn about configuration resolvers that automate parameter setup.\n",
    "    * See how the framework intelligently selects transforms based on the model and data.\n",
    "\n",
    "2.  **Defining Default Transforms** ‚öôÔ∏è\n",
    "    * Create default transformation pipelines for specific datasets using YAML.\n",
    "    * Override and customize parameters for your default transforms.\n",
    "\n",
    "3.  **Implementing a Custom Transform** üî®\n",
    "    * Write your own transform class from scratch by inheriting from a base class.\n",
    "    * Place the new transform within the project structure to make it accessible.\n",
    "\n",
    "4.  **Configuring and Using New Transforms** üîÑ\n",
    "    * Integrate your custom transform into the framework using Hydra's configuration syntax.\n",
    "    * Apply your new transform as part of a preprocessing pipeline.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Technical Framework\n",
    "\n",
    "This tutorial focuses on:\n",
    "* The **TopoBench** library's data processing pipeline.\n",
    "* **Hydra** for powerful and flexible configuration management.\n",
    "* Inheriting from `torch_geometric.transforms.BaseTransform` to create new transforms.\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Important Notes\n",
    "\n",
    "* To make the learning process concrete, we'll work with practical examples using the **`US-county-demos`** and **`REDDIT-BINARY`** datasets.\n",
    "* The principles shown here are general and can be applied to any dataset you wish to integrate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Load the Custom Dataset: Understanding Configuration Imports\n",
    "\n",
    "Before loading some dataset, it is crucial to understand the configuration imports, particularly those from the `topobench.utils.config_resolvers` module. These utility functions play a key role in dynamically configuring your machine learning pipeline.\n",
    "\n",
    "### Key Imports for Dynamic Configuration\n",
    "\n",
    "Lets  import the essential configuration resolver functions:\n",
    "\n",
    "```python\n",
    "from topobench.utils.config_resolvers import (\n",
    "    get_default_transform,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    infer_in_channels,\n",
    ")\n",
    "```\n",
    "\n",
    "### Why These Imports Matter\n",
    "\n",
    "```yaml\n",
    "data_dir: ${paths.data_dir}/${dataset.loader.parameters.data_domain}/${dataset.loader.parameters.data_type}\n",
    "```\n",
    "Many configuration values can be set by simply referencing other parts of the configuration, as seen in the data_dir example above. This is perfect for constructing file paths or reusing constant values. However, some parameters are not static; their optimal values depend on other components of your experiment, such as the specific dataset or model you are using.\n",
    "\n",
    "For instance, determining the correct data transformations, the metric to monitor for training (e.g., accuracy vs. loss), or the number of input channels for a model or the choice of the lifting depending on the model and dataset domains. The automatization of such situations often requires logic that goes beyond simple variable substitution. Manually adjusting these for every experiment would be tedious and error-prone. This is where custom resolver functions come in‚Äîthey embed this decision-making logic directly into the configuration itself.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Practical Example: Dynamic Transforms\n",
    "\n",
    "Consider the configuration in `projects/TopoBench/configs/run.yaml`, where the `transforms` parameter uses the `get_default_transform` function:\n",
    "\n",
    "```yaml\n",
    "transforms: ${get_default_transform:${dataset},${model}}\n",
    "```\n",
    "\n",
    "This syntax allows for automatic transformation selection based on the dataset and model, demonstrating the power of these configuration resolver functions.\n",
    "\n",
    "By importing and utilizing these functions, you gain:\n",
    "- Flexible configuration management\n",
    "- Automatic parameter inference\n",
    "- Reduced manual configuration overhead\n",
    "\n",
    "These facilitate seamless dataset loading and preprocessing for multiple topological domains and provide an easy and intuitive interface for incorporating novel functionality.\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "\n",
    "\n",
    "from topobench.utils.config_resolvers import (\n",
    "    get_default_transform,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    infer_in_channels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Default Data Transformations ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most datasets can be used directly after integration, some require specific preprocessing transformations. These transformations might vary depending on the task, model, or other conditions.\n",
    "\n",
    "## Example Case: US-county-demos Dataset\n",
    "\n",
    "Let's look at our language dataset's structure the `compose` function. \n",
    "```python\n",
    "cfg = compose(\n",
    "    config_name=\"run.yaml\",\n",
    "    overrides=[\n",
    "        \"model=hypergraph/unignn2\",\n",
    "        \"dataset=graph/US-county-demos\",\n",
    "    ], \n",
    "    return_hydra_config=True\n",
    ")\n",
    "```\n",
    "we can see that the model is `hypergraph/unignn2` from hypergraph domain while the dataset is from graph domain.\n",
    "This implied that the discussed above `get_default_transform` function:\n",
    "\n",
    "```yaml\n",
    "transforms: ${get_default_transform:${dataset},${model}}\n",
    "```\n",
    "Inferred a default transform from graph to hypegraph domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2458422/3549809466.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../configs\", job_name=\"job\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform name: dict_keys(['graph2hypergraph_lifting'])\n",
      "Transform parameters: {'_target_': 'topobench.transforms.data_transform.DataTransform', 'transform_type': 'lifting', 'transform_name': 'HypergraphKHopLifting', 'k_value': 1, 'feature_lifting': 'ProjectionSum', 'preserve_edge_attr': False, 'complex_dim': 1, 'neighborhoods': '${oc.select:model.backbone.neighborhoods,null}'}\n"
     ]
    }
   ],
   "source": [
    "initialize(config_path=\"../configs\", job_name=\"job\")\n",
    "cfg = compose(\n",
    "    config_name=\"run.yaml\",\n",
    "    overrides=[\n",
    "        \"model=hypergraph/unignn2\",\n",
    "        \"dataset=graph/US-county-demos\",\n",
    "    ], \n",
    "    return_hydra_config=True\n",
    ")\n",
    "loader = instantiate(cfg.dataset.loader)\n",
    "\n",
    "\n",
    "dataset, dataset_dir = loader.load()\n",
    "\n",
    "print('Transform name:', cfg.transforms.keys())\n",
    "print('Transform parameters:', cfg.transforms['graph2hypergraph_lifting'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some datasets require might require default transforms which are applied whenever it is nedded to model the data. \n",
    "\n",
    "The topobench library provides a simple way to define custom transformations and apply them to the dataset.\n",
    "Take a look at `TopoBench/configs/transforms/dataset_defaults` folder where you can find some default transformations for different datasets.\n",
    "\n",
    "For example, REDDIT-BINARY does not have initial node features and it is a common practice to define initial features as gaussian noise.\n",
    "Hence the `TopoBench/configs/transforms/dataset_defaults/REDDIT-BINARY.yaml` file incorporates the `gaussian_noise` transform by default. \n",
    "Hence whenver you choose to uplodad the REDDIT-BINARY dataset (and do not modify ```transforms``` parameter), the `gaussian_noise` transform will be applied to the dataset.\n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - data_manipulations: equal_gaus_features\n",
    "  - liftings@_here_: ${get_required_lifting:graph,${model}}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Below we provide an quick tutorial on how to create a data transformations and create a sequence of default transformations that will be executed whener you use the defined dataset config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid override transforms\n",
    "cfg = compose(\n",
    "    config_name=\"run.yaml\",\n",
    "    overrides=[\n",
    "        \"model=hypergraph/unignn2\",\n",
    "        \"dataset=graph/REDDIT-BINARY\",\n",
    "    ], \n",
    "    return_hydra_config=True\n",
    ")\n",
    "loader = instantiate(cfg.dataset.loader)\n",
    "\n",
    "\n",
    "dataset, dataset_dir = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 480], y=[1], num_nodes=218)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the default transforms and the parameters of `equal_gaus_features` transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform name: dict_keys(['equal_gaus_features', 'graph2hypergraph_lifting'])\n",
      "Transform parameters: {'_target_': 'topobench.transforms.data_transform.DataTransform', 'transform_name': 'EqualGausFeatures', 'transform_type': 'data manipulation', 'mean': 0, 'std': 0.1, 'num_features': '${dataset.parameters.num_features}'}\n"
     ]
    }
   ],
   "source": [
    "print('Transform name:', cfg.transforms.keys())\n",
    "print('Transform parameters:', cfg.transforms['equal_gaus_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from topobench.data.preprocessor import PreProcessor\n",
    "preprocessed_dataset = PreProcessor(dataset, dataset_dir, cfg['transforms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[218, 10], edge_index=[2, 480], y=[1], incidence_hyperedges=[218, 218], num_hyperedges=[1], x_0=[218, 10], x_hyperedges=[218, 10], num_nodes=218)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed dataset has the features generated by the preprocessor. And the connectivity of the dataset has been transformed into hypegraph domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your own default transforms\n",
    "\n",
    "Now when we have seen how to add custom dataset and how does the default transform works. One might want to reate your own default transforms for new dataset that will be executed always whenwever the dataset under default configuration is used.\n",
    "\n",
    "\n",
    "**To configure** the deafult transform navigate to `configs/transforms/dataset_defaults` create `<def_transforms.yaml>` and the follwoing `.yaml` file: \n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - transform_1: transform_1\n",
    "  - transform_2: transform_2\n",
    "  - transform_3: transform_3\n",
    "```\n",
    "\n",
    "\n",
    "**Important**\n",
    "There are different types of transforms, including `data_manipulation`, `liftings`, and `feature_liftings`. In case you want to use multiple transforms from the same categoty, let's say from `data_manipulation`, then it is required to stick to a special syntaxis. [See hydra configuration for more information]() or the example below: \n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - data_manipulation@first_usage: transform_1\n",
    "  - data_manipulation@second_usage: transform_2\n",
    "```\n",
    "\n",
    "\n",
    "### Notes: \n",
    "\n",
    "- **Transforms from the same category:** If There are a two transforms from the same catgory, for example, `data_manipulations`, it is required to use operator `@` to assign new diffrerent names `first_usage` and `second_usage` to each transform.\n",
    "\n",
    "-  In the case of `equal_gaus_features` we have to override the initial number of features as the `equal_gaus_features.yaml` which uses a special register to infer the feature dimension (the registed logic descrived in Step 3.) However by some reason we want to specify `num_features` parameter we can override it in the default file without the need to change the transform config file. \n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - data_manipulations@equal_gaus_features: equal_gaus_features\n",
    "  - data_manipulations@some_transform: some_transform\n",
    "  - liftings@_here_: ${get_required_lifting:graph,${model}}\n",
    "\n",
    "equal_gaus_features:\n",
    "  num_features: 100\n",
    "some_transform:\n",
    "  some_param: bla\n",
    "```\n",
    "\n",
    "- We recommend to always add `liftings@_here_: ${get_required_lifting:graph,${model}}` so that a default lifting is applied to run any domain-specific topological model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Custom Data Transformations ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Transform\n",
    "\n",
    "In general any transfom in the library inherits `torch_geometric.transforms.BaseTransform` class, which allow to apply a sequency of transforms to the data. Our inderface requires to implement the `forward` method. The important part of all transforms is that it takes `torch_geometric.data.Data` object and returns updated `torch_geometric.data.Data` object.\n",
    "\n",
    "\n",
    "\n",
    "For language dataset,  we have generated the `equal_gaus_features` transfroms that is a data_manipulation transform hence we place it into `topobench/transforms/data_manipulation/` folder. \n",
    "Below you can see th `EqualGausFeatures` class: \n",
    "\n",
    "\n",
    "```python\n",
    "   class EqualGausFeatures(torch_geometric.transforms.BaseTransform):\n",
    "    r\"\"\"A transform that generates equal Gaussian features for all nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    **kwargs : optional\n",
    "        Additional arguments for the class. It should contain the following keys:\n",
    "        - mean (float): The mean of the Gaussian distribution.\n",
    "        - std (float): The standard deviation of the Gaussian distribution.\n",
    "        - num_features (int): The number of features to generate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.type = \"generate_non_informative_features\"\n",
    "\n",
    "        # Torch generate feature vector from gaus distribution\n",
    "        self.mean = kwargs[\"mean\"]\n",
    "        self.std = kwargs[\"std\"]\n",
    "        self.feature_vector = kwargs[\"num_features\"]\n",
    "        self.feature_vector = torch.normal(\n",
    "            mean=self.mean, std=self.std, size=(1, self.feature_vector)\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(type={self.type!r}, mean={self.mean!r}, std={self.std!r}, feature_vector={self.feature_vector!r})\"\n",
    "\n",
    "    def forward(self, data: torch_geometric.data.Data):\n",
    "        r\"\"\"Apply the transform to the input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : torch_geometric.data.Data\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch_geometric.data.Data\n",
    "            The transformed data.\n",
    "        \"\"\"\n",
    "        data.x = self.feature_vector.expand(data.num_nodes, -1)\n",
    "        return data\n",
    "\n",
    "```\n",
    "\n",
    "As we said above the `forward` function takes as input the `torch_geometric.data.Data` object, modifies it, and returns it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Transform\n",
    "\n",
    "Similarly to adding dataset the transformations you have created and placed at right folder are automatically registered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a configuration file \n",
    "Now as we have registered the transform we can finally create the configuration file and use it in the framework: \n",
    "\n",
    "``` yaml\n",
    "_target_: topobench.transforms.data_transform.DataTransform\n",
    "transform_name: \"EqualGausFeatures\"\n",
    "transform_type: \"data manipulation\"\n",
    "\n",
    "mean: 0\n",
    "std: 0.1\n",
    "num_features: ${dataset.parameters.num_features}\n",
    "``` \n",
    "Please refer to `configs/transforms/dataset_defaults/equal_gaus_features.yaml` for the example. \n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- You might notice an interesting key `_target_` in the configuration file. In general for any new transform you the `_target_` is always `topobench.transforms.data_transform.DataTransform`.  [For more information please refer to hydra documentation \"Instantiating objects with Hydra\" section.](https://hydra.cc/docs/advanced/instantiate_objects/overview/). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
