{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Insert the parent folder of this notebook\n",
    "notebook_path = os.getcwd()\n",
    "sys.path.append(os.path.dirname(notebook_path))\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import lightning as L\n",
    "import json\n",
    "import wandb\n",
    "from topobench.data.preprocessor import PreProcessor\n",
    "from topobench.dataloader import TBDataloader\n",
    "\n",
    "# CONFIGURATION\n",
    "METRIC = 'mae'  # Using MAE for triangle counting (lower is better)\n",
    "PROJECT_NAME = 'graphuniverse/final_triangle_experiments'\n",
    "PARAMETER_TO_VARY = 'degree_separation_range'\n",
    "# Example of how to add checkpoint paths (you'll need to adapt this to your setup)\n",
    "# Assuming you have a checkpoint directory structure like: ../checkpoints/model_name/\n",
    "CHECKPOINT_BASE_DIR = \"../checkpoints\"\n",
    "\n",
    "# Set publication style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'DejaVu Serif'],\n",
    "    'axes.linewidth': 0.8,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.linewidth': 0.5,\n",
    "    'xtick.direction': 'in',\n",
    "    'ytick.direction': 'in',\n",
    "    'errorbar.capsize': 3,\n",
    "    'figure.dpi': 300\n",
    "})\n",
    "\n",
    "def safe_get_nested(obj, keys, default=None):\n",
    "    \"\"\"Safely get nested dictionary values\"\"\"\n",
    "    try:\n",
    "        for key in keys:\n",
    "            obj = obj[key]\n",
    "        return obj\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        return default\n",
    "\n",
    "def load_triangle_experiment_data():\n",
    "    \"\"\"Load data from triangle counting experiments\"\"\"\n",
    "    api = wandb.Api(timeout=100)\n",
    "    \n",
    "    try:\n",
    "        runs = api.runs(PROJECT_NAME)\n",
    "        \n",
    "        all_data = []\n",
    "        for run in runs:\n",
    "            summary = run.summary._json_dict\n",
    "            summary['run_name'] = run.name\n",
    "            summary['run_id'] = run.id\n",
    "            \n",
    "            # Add config parameters with 'config_' prefix\n",
    "            config = run.config\n",
    "            for key, value in config.items():\n",
    "                summary[f'config_{key}'] = value\n",
    "            \n",
    "            all_data.append(summary)\n",
    "        \n",
    "        if len(all_data) == 0:\n",
    "            print(f\"No runs found in {PROJECT_NAME}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(all_data)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading triangle experiments: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def dict_to_sorted_string_no_seed(d, seed_keys=['seed', 'random_seed', 'random_state']):\n",
    "    \"\"\"Convert nested dict to consistent sorted string, excluding seed-related keys\"\"\"\n",
    "    if d is None:\n",
    "        return \"None\"\n",
    "    \n",
    "    def remove_seeds(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: remove_seeds(v) for k, v in obj.items() if k not in seed_keys}\n",
    "        elif isinstance(obj, list):\n",
    "            return [remove_seeds(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    cleaned_dict = remove_seeds(d)\n",
    "    return json.dumps(cleaned_dict, sort_keys=True, separators=(',', ':'))\n",
    "\n",
    "def extract_transform_info(config_transforms, model_name):\n",
    "    \"\"\"Extract relevant transform information based on model type\"\"\"\n",
    "    if pd.isna(config_transforms) or config_transforms is None:\n",
    "        return \"no_transform\"\n",
    "    \n",
    "    if model_name in ['GPS', 'nsd']:\n",
    "        # Extract encodings for GPS and NSD\n",
    "        if 'CombinedPSEs' in config_transforms:\n",
    "            encodings = config_transforms['CombinedPSEs'].get('encodings', [])\n",
    "            if encodings:\n",
    "                return '_'.join(sorted(encodings))\n",
    "        return \"no_encoding\"\n",
    "    \n",
    "    elif model_name == 'topotune':\n",
    "        return \"cell_lifting\"\n",
    "    \n",
    "    else:\n",
    "        return \"no_transform\"\n",
    "\n",
    "def extract_parameter_value(df, parameter_name):\n",
    "    \"\"\"Extract the parameter value from dataset configuration\"\"\"\n",
    "    def extract_param(config_dataset):\n",
    "        if pd.isna(config_dataset) or config_dataset is None:\n",
    "            return None\n",
    "        \n",
    "        # Try different possible locations for the parameter\n",
    "        param_value = safe_get_nested(config_dataset, \n",
    "            ['loader', 'parameters', 'generation_parameters', 'family_parameters', parameter_name])\n",
    "        \n",
    "        if param_value is None:\n",
    "            # Try direct location\n",
    "            param_value = safe_get_nested(config_dataset, \n",
    "                ['loader', 'parameters', parameter_name])\n",
    "        \n",
    "        # Convert to tuple if it's a list for consistency\n",
    "        if isinstance(param_value, list):\n",
    "            param_value = tuple(param_value)\n",
    "        \n",
    "        return param_value\n",
    "    \n",
    "    df[parameter_name] = df['config_dataset'].apply(extract_param)\n",
    "    return df\n",
    "\n",
    "def process_triangle_data(df):\n",
    "    \"\"\"Process triangle counting experiment data with parameter extraction\"\"\"\n",
    "    if df.empty:\n",
    "        return df, None, None\n",
    "    \n",
    "    # Extract model names\n",
    "    df['model_name'] = df['config_model'].apply(\n",
    "        lambda x: x.get('model_name') if isinstance(x, dict) else None\n",
    "    )\n",
    "    \n",
    "    # Extract the parameter to vary\n",
    "    df = extract_parameter_value(df, PARAMETER_TO_VARY)\n",
    "    \n",
    "    # Create model config strings WITHOUT random seeds\n",
    "    df['model_config_str'] = df['config_model'].apply(\n",
    "        lambda x: dict_to_sorted_string_no_seed(x) if isinstance(x, dict) else \"None\"\n",
    "    )\n",
    "    \n",
    "    # Extract transform info\n",
    "    df['transform_info'] = df.apply(\n",
    "        lambda row: extract_transform_info(row.get('config_transforms'), row['model_name']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create enhanced model config string that includes PE info for GPS/NSD\n",
    "    df['enhanced_model_config_str'] = df.apply(\n",
    "        lambda row: (row['model_config_str'] + f\"_PE_{row['transform_info']}\") \n",
    "                    if row['model_name'] in ['GPS', 'nsd'] \n",
    "                    else row['model_config_str'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Define validation and test metrics\n",
    "    VAL_METRIC = f'val/{METRIC}'\n",
    "    TEST_METRIC = f'test/{METRIC}'\n",
    "    \n",
    "    # Filter out rows with missing essential data\n",
    "    df_clean = df.dropna(subset=['model_name', 'enhanced_model_config_str', PARAMETER_TO_VARY, VAL_METRIC, TEST_METRIC])\n",
    "    df_clean = df_clean[df_clean['model_config_str'] != \"None\"]\n",
    "    df_clean = df_clean[df_clean[PARAMETER_TO_VARY].notna()]\n",
    "    \n",
    "    return df_clean, VAL_METRIC, TEST_METRIC\n",
    "\n",
    "# MAIN EXECUTION\n",
    "print(\"Loading triangle counting experiment data...\")\n",
    "df = load_triangle_experiment_data()\n",
    "\n",
    "if df.empty:\n",
    "    print(\"No data found. Please check the project name and ensure runs exist.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loaded {len(df)} runs\")\n",
    "\n",
    "# Process data\n",
    "df_clean, val_metric, test_metric = process_triangle_data(df)\n",
    "\n",
    "if df_clean.empty:\n",
    "    print(\"No clean data available after processing.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Clean data: {len(df_clean)} runs\")\n",
    "\n",
    "# Filter for degree_separation_range == [0.0, 0.1] only\n",
    "target_degree_separation = (0.0, 0.1)\n",
    "df_filtered = df_clean[df_clean[PARAMETER_TO_VARY] == target_degree_separation]\n",
    "\n",
    "if df_filtered.empty:\n",
    "    print(f\"No data found for {PARAMETER_TO_VARY} == {target_degree_separation}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Filtered data for {PARAMETER_TO_VARY} == {target_degree_separation}: {len(df_filtered)} runs\")\n",
    "\n",
    "# Get best model per model type based on validation performance\n",
    "def get_best_model_configs(df, val_metric):\n",
    "    \"\"\"Get best model configuration for each model type based on validation performance.\"\"\"\n",
    "    best_models = {}\n",
    "    \n",
    "    for model_name in df['model_name'].unique():\n",
    "        model_data = df[df['model_name'] == model_name]\n",
    "        \n",
    "        # Calculate mean validation performance per configuration\n",
    "        config_performance = model_data.groupby('enhanced_model_config_str')[val_metric].mean()\n",
    "        \n",
    "        # For MAE, lower is better\n",
    "        best_config = config_performance.idxmin()\n",
    "        best_runs = model_data[model_data['enhanced_model_config_str'] == best_config]\n",
    "        \n",
    "        best_models[model_name] = best_runs\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "best_models = get_best_model_configs(df_filtered, val_metric)\n",
    "\n",
    "print(f\"Found best configurations for {len(best_models)} model types:\")\n",
    "for model_name, runs in best_models.items():\n",
    "    print(f\"  {model_name}: {len(runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf620bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define shift configurations for graph size increases\n",
    "SHIFT_CONFIGS = [\n",
    "    {\n",
    "        'universe_parameters': {},\n",
    "        'family_parameters': {\n",
    "            'min_n_nodes_shift': 200,\n",
    "            'max_n_nodes_shift': 200\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'universe_parameters': {},\n",
    "        'family_parameters': {\n",
    "            'min_n_nodes_shift': 500,\n",
    "            'max_n_nodes_shift': 500\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "class TriangleShiftEvaluator:\n",
    "    def __init__(self, model_df):\n",
    "        \"\"\"Initialize evaluator with model dataframe.\"\"\"\n",
    "        self.model_df = model_df\n",
    "        self._setup_hydra()\n",
    "        \n",
    "    def _setup_hydra(self):\n",
    "        if GlobalHydra().is_initialized():\n",
    "            GlobalHydra().clear()\n",
    "        initialize(config_path=\"../configs\", job_name=\"triangle_shift_evaluation\")\n",
    "    \n",
    "    def shift_dataset_parameters(self, shift_config, number_of_eval_graphs, run_idx):\n",
    "        \"\"\"Apply shifts to dataset generation parameters for triangle counting.\"\"\"\n",
    "        dataset_params = self.model_df.loc[run_idx]['config_dataset']['loader']['parameters']['generation_parameters']\n",
    "        params = copy.deepcopy(dataset_params)\n",
    "        \n",
    "        # Set the number of graphs to the original number of graphs\n",
    "        params['family_parameters']['n_graphs'] = number_of_eval_graphs\n",
    "\n",
    "        # Universe parameters\n",
    "        for param in ['center_variance', 'cluster_variance', 'edge_propensity_variance']:\n",
    "            if f'{param}_shift' in shift_config['universe_parameters']:\n",
    "                params['universe_parameters'][param] += shift_config['universe_parameters'][f'{param}_shift']\n",
    "                params['universe_parameters'][param] = np.clip(params['universe_parameters'][param], 0, 1)\n",
    "        \n",
    "        # Family parameters - integers\n",
    "        for param in ['min_n_nodes', 'max_n_nodes', 'min_communities', 'max_communities']:\n",
    "            if f'{param}_shift' in shift_config['family_parameters']:\n",
    "                params['family_parameters'][param] += shift_config['family_parameters'][f'{param}_shift']\n",
    "                if param.startswith('n_graphs'):\n",
    "                    params['family_parameters'][param] = np.clip(params['family_parameters'][param], 1, 100000)\n",
    "                elif 'nodes' in param:\n",
    "                    params['family_parameters'][param] = np.clip(params['family_parameters'][param], 1, 10000)\n",
    "                elif 'communities' in param:\n",
    "                    params['family_parameters'][param] = np.clip(params['family_parameters'][param], 1, 1000)\n",
    "        \n",
    "        # Family parameters - ranges (including degree_separation_range)\n",
    "        range_params = ['homophily_range', 'avg_degree_range', 'degree_separation_range', 'power_law_exponent_range']\n",
    "        for param in range_params:\n",
    "            for i, bound in enumerate(['min', 'max']):\n",
    "                shift_key = f'{bound}_{param}_shift'\n",
    "                if shift_key in shift_config['family_parameters']:\n",
    "                    params['family_parameters'][param][i] += shift_config['family_parameters'][shift_key]\n",
    "                    \n",
    "                    # Apply appropriate clipping\n",
    "                    if 'homophily' in param:\n",
    "                        params['family_parameters'][param][i] = np.clip(params['family_parameters'][param][i], 0, 1)\n",
    "                    elif 'avg_degree' in param:\n",
    "                        params['family_parameters'][param][i] = np.clip(params['family_parameters'][param][i], 1, 100)\n",
    "                    elif 'degree_separation' in param:\n",
    "                        params['family_parameters'][param][i] = np.clip(params['family_parameters'][param][i], 0, 100)\n",
    "                    elif 'power_law' in param:\n",
    "                        params['family_parameters'][param][i] = np.clip(params['family_parameters'][param][i], 0, 10)\n",
    "        \n",
    "        # Update seed if provided\n",
    "        if 'seed' in shift_config['family_parameters']:\n",
    "            params['family_parameters']['seed'] = shift_config['family_parameters']['seed']\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def config_to_overrides(self, config, prefix=\"\", include_graphuniverse_parameters=False):\n",
    "        \"\"\"Convert config to overrides using recursion with proper dot notation.\"\"\"\n",
    "        overrides = []\n",
    "        \n",
    "        for key, value in config.items():\n",
    "            current_path = f\"{prefix}.{key}\" if prefix else key\n",
    "            \n",
    "            if key == 'generation_parameters' and not include_graphuniverse_parameters:\n",
    "                continue\n",
    "                \n",
    "            if isinstance(value, dict):\n",
    "                overrides.extend(self.config_to_overrides(\n",
    "                    value, \n",
    "                    prefix=current_path, \n",
    "                    include_graphuniverse_parameters=include_graphuniverse_parameters\n",
    "                ))\n",
    "            elif isinstance(value, list):\n",
    "                overrides.append(f\"{current_path}={value}\")\n",
    "            else:\n",
    "                if value is None:\n",
    "                    overrides.append(f\"{current_path}=null\")\n",
    "                else:\n",
    "                    overrides.append(f\"{current_path}={value}\")\n",
    "        \n",
    "        return overrides\n",
    "\n",
    "    def prepare_all_overrides(self, shift_config, run_idx):\n",
    "        \"\"\"Prepare Hydra config with shifts applied.\"\"\"\n",
    "        data_config = self.model_df.loc[run_idx]['config_dataset']\n",
    "        model_config = self.model_df.loc[run_idx]['config_model']\n",
    "        transform_config = self.model_df.loc[run_idx]['config_transforms']\n",
    "\n",
    "        # Calc the original n_graphs for test set \n",
    "        train_prop = data_config['split_params']['train_prop']\n",
    "        eval_prop = (1 - train_prop)\n",
    "        n_graphs = data_config['loader']['parameters']['generation_parameters']['family_parameters']['n_graphs']\n",
    "        n_graphs_eval = int(n_graphs * eval_prop)\n",
    "\n",
    "        shifted_params = self.shift_dataset_parameters(shift_config, n_graphs_eval, run_idx)\n",
    "        \n",
    "        all_overrides = []\n",
    "        model_name = model_config['model_name']\n",
    "\n",
    "        model_config_map = {\n",
    "            'gcn': 'graph/gcn',\n",
    "            'gat': 'graph/gat', \n",
    "            'GPS': 'graph/gps',\n",
    "            'nsd': 'graph/nsd',\n",
    "            'gin': 'graph/gin',\n",
    "            'DeepSet': 'pointcloud/deepset',\n",
    "            'topotune': 'cell/topotune',\n",
    "            'GraphMLP': 'graph/graph_mlp',\n",
    "            'GraphSAGE': 'graph/sage',\n",
    "        }\n",
    "        \n",
    "        all_overrides.append(f\"model={model_config_map[model_name]}\")\n",
    "        all_overrides.append(f\"model.model_name={model_name}\")\n",
    "        \n",
    "        model_overrides = self.config_to_overrides(model_config, prefix=\"model\")\n",
    "        all_overrides.extend(model_overrides)\n",
    "\n",
    "        data_overrides = self.config_to_overrides(data_config, prefix=\"dataset\", include_graphuniverse_parameters=False)\n",
    "        # Find the index of the train_prop override\n",
    "        train_prop_idx = [i for i, override in enumerate(data_overrides) if 'train_prop' in override]\n",
    "        # Replace the train_prop with the new train_prop\n",
    "        data_overrides[train_prop_idx[0]] = f\"dataset.split_params.train_prop=0.0\"\n",
    "\n",
    "        # Now add the data_overrides\n",
    "        all_overrides.extend(data_overrides)\n",
    "\n",
    "        if model_name in ['GPS', 'nsd']:\n",
    "            if pd.isna(transform_config) or transform_config is None:\n",
    "                transform_overrides = ['transforms.CombinedPSEs.encodings=[]']\n",
    "            else:\n",
    "                transform_overrides = self.config_to_overrides(transform_config, prefix=\"transforms\")\n",
    "            all_overrides.extend(transform_overrides)\n",
    "            \n",
    "        shifted_params_overrides = []\n",
    "        for param_group in ['universe_parameters', 'family_parameters']:\n",
    "            for key, value in shifted_params[param_group].items():\n",
    "                shifted_params_overrides.append(\n",
    "                    f\"dataset.loader.parameters.generation_parameters.{param_group}.{key}={value}\"\n",
    "                )\n",
    "        all_overrides.extend(shifted_params_overrides)\n",
    "\n",
    "        return all_overrides\n",
    "         \n",
    "    def evaluate_single_model(self, overrides, run_idx):\n",
    "        \"\"\"Evaluate a single model with given overrides.\"\"\"\n",
    "        cfg = compose(config_name=\"run.yaml\", overrides=overrides)\n",
    "        model = instantiate(cfg.model, evaluator=cfg.evaluator, optimizer=cfg.optimizer, loss=cfg.loss)\n",
    "        \n",
    "        # Note: For triangle counting, we need to get the checkpoint path differently\n",
    "        # Assuming the checkpoint path is stored similar to the community detection case\n",
    "        if 'checkpoint_local' in self.model_df.columns:\n",
    "            checkpoint_path = self.model_df.loc[run_idx]['checkpoint_local']\n",
    "        else:\n",
    "            print(\"No checkpoint_local column found. Cannot evaluate model.\")\n",
    "            return None\n",
    "        \n",
    "        # Add error handling for checkpoint loading\n",
    "        try:\n",
    "            if not os.path.exists(checkpoint_path):\n",
    "                print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "                return None\n",
    "                \n",
    "            checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"], strict=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint {checkpoint_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        loader = instantiate(cfg.dataset.loader)\n",
    "        dataset, dataset_dir = loader.load()\n",
    "        transform_config = cfg.get(\"transforms\", None)\n",
    "        preprocessor = PreProcessor(dataset, dataset_dir, transform_config)\n",
    "        _, _, dataset_test = preprocessor.load_dataset_splits(cfg.dataset.split_params)\n",
    "        \n",
    "        datamodule = TBDataloader(\n",
    "            dataset_train=None,\n",
    "            dataset_val=None, \n",
    "            dataset_test=dataset_test,\n",
    "            **cfg.dataset.get(\"dataloader_params\", {}),\n",
    "        )\n",
    "        \n",
    "        trainer = L.Trainer(\n",
    "            devices=1,\n",
    "            accelerator='auto',\n",
    "            logger=False,\n",
    "            enable_checkpointing=False,\n",
    "            num_sanity_val_steps=0,\n",
    "        )\n",
    "        \n",
    "        test_results = trainer.test(model, datamodule)\n",
    "\n",
    "\n",
    "        # For the triangle counting task, I also want to calc mae/total_triangles\n",
    "        # Calc the average amount of triangles in the test set\n",
    "        test_dataloader = datamodule.test_dataloader()\n",
    "        all_triangles = []\n",
    "        for batch in test_dataloader:\n",
    "            all_triangles.append(batch.y.float().mean())\n",
    "        print(\"average amount of triangles: \", np.mean(all_triangles))\n",
    "\n",
    "        # Add mae/average_num_of_triangles to the test_results\n",
    "        test_results[0]['mae'] = test_results[0]['test/mae']\n",
    "        test_results[0]['average_num_of_triangles'] = np.mean(all_triangles)\n",
    "        test_results[0]['mae/average_num_of_triangles'] = test_results[0]['mae'] / test_results[0]['average_num_of_triangles']\n",
    "        print(\"mae/average_num_of_triangles: \", test_results[0]['mae/average_num_of_triangles'])\n",
    "\n",
    "        return test_results[0] if test_results else {}\n",
    "\n",
    "    def evaluate_multiple_shifts(self, shift_configs, metrics=['test/mae']):\n",
    "        \"\"\"Evaluate model performance across multiple distribution shifts.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Always include a zero shift (baseline)\n",
    "        zero_shift = {\n",
    "            'universe_parameters': {},\n",
    "            'family_parameters': {}\n",
    "        }\n",
    "\n",
    "        # Store results by run_idx to maintain pairing\n",
    "        paired_results = {}\n",
    "        \n",
    "        # Combine zero shift with provided shifts\n",
    "        all_shifts = [zero_shift] + shift_configs\n",
    "        \n",
    "        for run_idx in self.model_df.index:\n",
    "            print(f\"Evaluating run {run_idx}\")\n",
    "            paired_results[run_idx] = {}\n",
    "            for shift_idx, shift_config in enumerate(all_shifts):\n",
    "                shift_name = self.get_shift_title(shift_config)\n",
    "                if shift_name == \"No Shift\":\n",
    "                    shift_name = \"Baseline\"\n",
    "                    \n",
    "                print(f\"Evaluating shift: {shift_name}\")\n",
    "                # Evaluate all models in the DataFrame\n",
    "                try:\n",
    "                    # Prepare overrides for this shift\n",
    "                    overrides = self.prepare_all_overrides(shift_config, run_idx)\n",
    "\n",
    "                    # Evaluate the model\n",
    "                    result = self.evaluate_single_model(overrides, run_idx)\n",
    "                    \n",
    "                    # Check if result is None (checkpoint not found or error)\n",
    "                    if result is None:\n",
    "                        print(f\"Skipping run {run_idx} with shift {shift_name} - checkpoint issue\")\n",
    "                        continue\n",
    "                        \n",
    "                    paired_results[run_idx][shift_name] = result\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating model {run_idx} with shift {shift_name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # If no shifts were successful for this run, remove it\n",
    "            if not paired_results[run_idx]:\n",
    "                print(f\"No successful evaluations for run {run_idx}, removing from results\")\n",
    "                del paired_results[run_idx]\n",
    "        \n",
    "        # Now calculate paired differences AND save baseline values\n",
    "        results = []\n",
    "        for metric in metrics:\n",
    "            # Extract baseline values for this metric\n",
    "            baseline_values = []\n",
    "            run_indices = []\n",
    "            \n",
    "            for run_idx, shifts in paired_results.items():\n",
    "                if ('Baseline' in shifts and \n",
    "                    shifts['Baseline'] is not None and \n",
    "                    metric in shifts['Baseline']):\n",
    "                    baseline_values.append(shifts['Baseline'][metric])\n",
    "                    run_indices.append(run_idx)\n",
    "            \n",
    "            # Check if we have any baseline values\n",
    "            if not baseline_values:\n",
    "                print(f\"No baseline values found for metric {metric}\")\n",
    "                continue\n",
    "            \n",
    "            # First, add baseline results (one row per baseline evaluation)\n",
    "            for i, run_idx in enumerate(run_indices):\n",
    "                baseline_result = paired_results[run_idx]['Baseline']\n",
    "                results.append({\n",
    "                    'shift_name': 'Baseline',\n",
    "                    'metric': metric,\n",
    "                    'run_idx': run_idx,\n",
    "                    'value': baseline_values[i],\n",
    "                    'mae': baseline_result.get('mae', None),\n",
    "                    'average_num_of_triangles': baseline_result.get('average_num_of_triangles', None),\n",
    "                    'mae_per_triangle': baseline_result.get('mae/average_num_of_triangles', None),\n",
    "                    'is_baseline': True,\n",
    "                    'mean_difference': 0.0,  # No difference for baseline\n",
    "                    'se_difference': 0.0,\n",
    "                    'ci_lower': 0.0,\n",
    "                    'ci_upper': 0.0,\n",
    "                    'p_value': 1.0,  # No significance test for baseline\n",
    "                    'n_pairs': 1,\n",
    "                    'baseline_mean': baseline_values[i],\n",
    "                    'shift_mean': baseline_values[i]\n",
    "                })\n",
    "            \n",
    "            # Then, for each shift, calculate paired differences\n",
    "            for shift_name in [s for s in paired_results[list(paired_results.keys())[0]].keys() if s != 'Baseline']:\n",
    "                shift_values = []\n",
    "                valid_pairs = []\n",
    "                shift_detailed_results = []\n",
    "                \n",
    "                for i, run_idx in enumerate(run_indices):\n",
    "                    if (run_idx in paired_results and \n",
    "                        shift_name in paired_results[run_idx] and \n",
    "                        paired_results[run_idx][shift_name] is not None and\n",
    "                        metric in paired_results[run_idx][shift_name]):\n",
    "                        shift_values.append(paired_results[run_idx][shift_name][metric])\n",
    "                        shift_detailed_results.append(paired_results[run_idx][shift_name])\n",
    "                        valid_pairs.append(i)\n",
    "                \n",
    "                if len(valid_pairs) > 0:\n",
    "                    baseline_paired = [baseline_values[i] for i in valid_pairs]\n",
    "                    differences = np.array(shift_values) - np.array(baseline_paired)\n",
    "                    \n",
    "                    mean_diff = np.mean(differences)\n",
    "                    se_diff = np.std(differences) / np.sqrt(len(differences)) if len(differences) > 1 else 0.0\n",
    "                    \n",
    "                    from scipy import stats\n",
    "                    if len(differences) > 1:\n",
    "                        t_val = stats.t.ppf(0.975, len(differences)-1)\n",
    "                        ci_95 = t_val * se_diff\n",
    "                        p_value = stats.ttest_1samp(differences, 0).pvalue\n",
    "                    else:\n",
    "                        ci_95 = 0.0\n",
    "                        p_value = 1.0\n",
    "                    \n",
    "                    # Add individual shift results (one row per run)\n",
    "                    for j, valid_pair_idx in enumerate(valid_pairs):\n",
    "                        run_idx = run_indices[valid_pair_idx]\n",
    "                        shift_result = shift_detailed_results[j]\n",
    "                        results.append({\n",
    "                            'shift_name': shift_name,\n",
    "                            'metric': metric,\n",
    "                            'run_idx': run_idx,\n",
    "                            'value': shift_values[j],\n",
    "                            'mae': shift_result.get('mae', None),\n",
    "                            'average_num_of_triangles': shift_result.get('average_num_of_triangles', None),\n",
    "                            'mae_per_triangle': shift_result.get('mae/average_num_of_triangles', None),\n",
    "                            'is_baseline': False,\n",
    "                            'mean_difference': shift_values[j] - baseline_paired[j],  # Individual difference\n",
    "                            'se_difference': se_diff,  # Group-level SE\n",
    "                            'ci_lower': mean_diff - ci_95,  # Group-level CI\n",
    "                            'ci_upper': mean_diff + ci_95,\n",
    "                            'p_value': p_value,  # Group-level p-value\n",
    "                            'n_pairs': len(differences),\n",
    "                            'baseline_mean': baseline_paired[j],  # Individual baseline\n",
    "                            'shift_mean': shift_values[j]  # Individual shift value\n",
    "                        })\n",
    "                    \n",
    "                    # Also add summary row with group statistics\n",
    "                    results.append({\n",
    "                        'shift_name': f'{shift_name}_SUMMARY',\n",
    "                        'metric': metric,\n",
    "                        'run_idx': -1,  # Indicates summary row\n",
    "                        'value': np.mean(shift_values),\n",
    "                        'mae': np.mean([r.get('mae', 0) for r in shift_detailed_results]),\n",
    "                        'average_num_of_triangles': np.mean([r.get('average_num_of_triangles', 0) for r in shift_detailed_results]),\n",
    "                        'mae_per_triangle': np.mean([r.get('mae/average_num_of_triangles', 0) for r in shift_detailed_results]),\n",
    "                        'is_baseline': False,\n",
    "                        'mean_difference': mean_diff,  # Group mean difference\n",
    "                        'se_difference': se_diff,\n",
    "                        'ci_lower': mean_diff - ci_95,\n",
    "                        'ci_upper': mean_diff + ci_95,\n",
    "                        'p_value': p_value,\n",
    "                        'n_pairs': len(differences),\n",
    "                        'baseline_mean': np.mean(baseline_paired),\n",
    "                        'shift_mean': np.mean(shift_values)\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"No valid pairs found for shift {shift_name} and metric {metric}\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "        \n",
    "    def get_shift_title(self, shift_config):\n",
    "        \"\"\"Generate title from non-zero shift parameters.\"\"\"\n",
    "        shift_parts = []\n",
    "        \n",
    "        for group_name, group_config in shift_config.items():\n",
    "            if group_name == 'universe_parameters':\n",
    "                for param, value in group_config.items():\n",
    "                    if param.endswith('_shift') and value != 0:\n",
    "                        param_name = param.replace('_shift', '')\n",
    "                        shift_parts.append(f\"{param_name}: {value:+.2f}\")\n",
    "            elif group_name == 'family_parameters':\n",
    "                for param, value in group_config.items():\n",
    "                    if param.endswith('_shift') and value != 0:\n",
    "                        param_name = param.replace('_shift', '').replace('_range', '')\n",
    "                        shift_parts.append(f\"{param_name}: {value:+}\")\n",
    "        \n",
    "        if shift_parts == []:\n",
    "            return \"No Shift\"\n",
    "        \n",
    "        return \" | \".join(shift_parts) if shift_parts else \"No Shift\"\n",
    "    \n",
    "    def plot_shift_comparison(self, results_df, metrics=['test/mae'], save_plots=True, group_name=None):\n",
    "        \"\"\"Create publication-quality comparison plot for paired differences.\"\"\"\n",
    "        \n",
    "        # Filter metrics to only those present in the data\n",
    "        available_metrics = results_df['metric'].unique()\n",
    "        metrics = [m for m in metrics if m in available_metrics]\n",
    "        \n",
    "        if not metrics:\n",
    "            print(\"No matching metrics found in results_df\")\n",
    "            return None\n",
    "        \n",
    "        # Set publication-quality style\n",
    "        plt.rcParams.update({\n",
    "            'font.family': 'serif',\n",
    "            'font.size': 12,\n",
    "            'axes.labelsize': 13,\n",
    "        })\n",
    "        \n",
    "        # Create subplots for each metric\n",
    "        fig, axes = plt.subplots(1, len(metrics), figsize=(6*len(metrics), 8), dpi=300)\n",
    "        if len(metrics) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            ax = axes[i]\n",
    "            metric_data = results_df[results_df['metric'] == metric]\n",
    "            \n",
    "            if len(metric_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Create bar plot showing differences from baseline\n",
    "            x_pos = np.arange(len(metric_data))\n",
    "            \n",
    "            # For MAE (lower is better), positive differences are worse (red), negative are better (green)\n",
    "            colors = ['red' if diff > 0 else 'green' for diff in metric_data['mean_difference']]\n",
    "            \n",
    "            bars = ax.bar(x_pos, metric_data['mean_difference'], \n",
    "                        yerr=metric_data['se_difference'],\n",
    "                        color=colors,\n",
    "                        alpha=0.7,\n",
    "                        capsize=5)\n",
    "            \n",
    "            # Add horizontal line at y=0 (no difference)\n",
    "            ax.axhline(y=0, color='black', linestyle='-', alpha=0.8, linewidth=1)\n",
    "            \n",
    "            # Customize appearance\n",
    "            ax.set_xlabel('Distribution Shift', fontweight='bold')\n",
    "            ax.set_ylabel(f'Δ {metric.replace(\"test/\", \"\").upper()}', fontweight='bold')\n",
    "            ax.set_title(f'{group_name}: Performance Changes\\n{metric.replace(\"test/\", \"\").upper()} (Lower is Better)', \n",
    "                        fontweight='bold', pad=20)\n",
    "            \n",
    "            # Set x-axis labels\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(metric_data['shift_name'], rotation=45, ha='right')\n",
    "            \n",
    "            # Add value labels with significance\n",
    "            for j, (bar, row) in enumerate(zip(bars, metric_data.itertuples())):\n",
    "                height = bar.get_height()\n",
    "                y_pos = height + row.se_difference + 0.01 if height >= 0 else height - row.se_difference - 0.01\n",
    "                \n",
    "                # Add significance stars\n",
    "                significance = \"\"\n",
    "                if row.p_value < 0.001:\n",
    "                    significance = \"***\"\n",
    "                elif row.p_value < 0.01:\n",
    "                    significance = \"**\"\n",
    "                elif row.p_value < 0.05:\n",
    "                    significance = \"*\"\n",
    "                \n",
    "                label = f'{row.mean_difference:.3f}±{row.se_difference:.3f}\\n{significance}'\n",
    "                \n",
    "                ax.text(bar.get_x() + bar.get_width()/2., y_pos,\n",
    "                    label,\n",
    "                    ha='center', va='bottom' if height >= 0 else 'top', \n",
    "                    fontweight='bold', fontsize=9)\n",
    "            \n",
    "            # Styling\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Set y-limits\n",
    "            if len(metric_data) > 0:\n",
    "                abs_values = abs(metric_data['mean_difference'] + metric_data['se_difference'])\n",
    "                if len(abs_values) > 0 and max(abs_values) > 0:\n",
    "                    y_max = max(abs_values) * 1.3\n",
    "                    ax.set_ylim([-y_max, y_max])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            model_name = self.model_df.iloc[0]['config_model']['model_name']\n",
    "            filename = f\"triangle_{str(group_name).replace('(', '').replace(')', '').replace(', ', '_')}_shift_differences_{model_name.lower()}\"\n",
    "            plt.savefig(f\"{filename}.png\", bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def run_triangle_shift_evaluation(shift_configs, model_df, group_name):\n",
    "    \"\"\"Run comprehensive shift evaluation for triangle counting.\"\"\"\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = TriangleShiftEvaluator(model_df)\n",
    "    \n",
    "    # Evaluate all shifts\n",
    "    results_df = evaluator.evaluate_multiple_shifts(\n",
    "        shift_configs, \n",
    "        metrics=['test/mae']\n",
    "    )\n",
    "    \n",
    "    # Check if we have any results before plotting\n",
    "    if results_df.empty:\n",
    "        print(f\"No successful evaluations for group {group_name} - skipping plots\")\n",
    "        fig = None\n",
    "    else:\n",
    "        # Create comparison plot\n",
    "        fig = evaluator.plot_shift_comparison(results_df, metrics=['test/mae'], group_name=group_name)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"TRIANGLE SHIFT EVALUATION SUMMARY for {group_name}\")\n",
    "        print(\"=\"*50)\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"{row['shift_name']:30} | {row['metric']:12} | \"\n",
    "                  f\"Diff: {row['mean_difference']:.3f} ± {row['se_difference']:.3f} \"\n",
    "                  f\"(p={row['p_value']:.3f}, n={row['n_pairs']})\")\n",
    "    \n",
    "    return evaluator, results_df, fig\n",
    "\n",
    "def add_checkpoint_paths(best_models_dict, checkpoint_base_dir):\n",
    "    \"\"\"Add local checkpoint paths to the model dataframes.\"\"\"\n",
    "    for model_name, model_df in best_models_dict.items():\n",
    "        checkpoint_paths = []\n",
    "        \n",
    "        for idx, row in model_df.iterrows():\n",
    "            # Construct checkpoint path based on your directory structure\n",
    "            # This is just an example - adapt to your actual structure\n",
    "            checkpoint_dir = os.path.join(checkpoint_base_dir, f\"degree_separation_range_(0, 0.1)/{model_name}\")\n",
    "            checkpoint_filename = f\"{row['checkpoint'].split('/')[-1]}\"  # or however your checkpoints are named\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "            checkpoint_paths.append(checkpoint_path)\n",
    "        \n",
    "        # Add checkpoint paths to the dataframe\n",
    "        model_df = model_df.copy()\n",
    "        model_df['checkpoint_local'] = checkpoint_paths\n",
    "        best_models_dict[model_name] = model_df\n",
    "    \n",
    "    return best_models_dict\n",
    "\n",
    "# Uncomment and modify this section when you have checkpoint paths set up:\n",
    "best_models_with_checkpoints = add_checkpoint_paths(best_models, CHECKPOINT_BASE_DIR)\n",
    "\n",
    "# Run evaluation for each model type\n",
    "for model_name, model_df in best_models.items():\n",
    "    print(f\"  {model_name}: {len(model_df)} runs\")\n",
    "\n",
    "print(\"\\nShift configurations to test:\")\n",
    "for i, shift_config in enumerate(SHIFT_CONFIGS):\n",
    "    evaluator_temp = TriangleShiftEvaluator(list(best_models.values())[0])\n",
    "    shift_name = evaluator_temp.get_shift_title(shift_config)\n",
    "    print(f\"  {i+1}. {shift_name}\")\n",
    "\n",
    "# Run evaluation for each model type\n",
    "all_results_df = pd.DataFrame()\n",
    "for model_name, model_df in best_models_with_checkpoints.items():\n",
    "    print(f'\\nEvaluating {model_name} models...')\n",
    "    print(f'Number of runs: {len(model_df)}')\n",
    "  \n",
    "    # Check if checkpoints exist before running evaluation\n",
    "    missing_checkpoints = []\n",
    "    for idx, row in model_df.iterrows():\n",
    "        if not os.path.exists(row['checkpoint_local']):\n",
    "            missing_checkpoints.append(row['checkpoint_local'])\n",
    "  \n",
    "    if missing_checkpoints:\n",
    "        print(f'Warning: {len(missing_checkpoints)} checkpoints missing for {model_name}')\n",
    "        print(f'First few missing: {missing_checkpoints[:3]}')\n",
    "        # You can choose to skip this model or continue with available checkpoints\n",
    "  \n",
    "    try:\n",
    "        evaluator, results_df, fig = run_triangle_shift_evaluation(\n",
    "            SHIFT_CONFIGS, model_df, f'{model_name}_degree_sep_0.0_0.1'\n",
    "        )\n",
    "      \n",
    "        if not results_df.empty:\n",
    "            results_df['model_name'] = model_name\n",
    "            all_results_df = pd.concat([all_results_df, results_df])\n",
    "            print(f'✓ Added results for {model_name}')\n",
    "          \n",
    "            # Display results for this model\n",
    "            print(f'\\nResults for {model_name}:')\n",
    "            for _, row in results_df.iterrows():\n",
    "                print(f'  {row[\"shift_name\"]}: MAE change = {row[\"mean_difference\"]:.4f} ± {row[\"se_difference\"]:.4f}')\n",
    "        else:\n",
    "            print(f'✗ No results for {model_name}')\n",
    "          \n",
    "    except Exception as e:\n",
    "        print(f'Error evaluating {model_name}: {e}')\n",
    "        continue\n",
    "# Summary across all models\n",
    "if not all_results_df.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "  \n",
    "    for model in all_results_df['model_name'].unique():\n",
    "        model_results = all_results_df[all_results_df['model_name'] == model]\n",
    "        print(f\"\\n{model}:\")\n",
    "        for _, row in model_results.iterrows():\n",
    "            significance = \"\"\n",
    "            if row['p_value'] < 0.001:\n",
    "                significance = \"***\"\n",
    "            elif row['p_value'] < 0.01:\n",
    "                significance = \"**\"\n",
    "            elif row['p_value'] < 0.05:\n",
    "                significance = \"*\"\n",
    "          \n",
    "            print(f\"  {row['shift_name']:20}: Δ{row['mean_difference']:+.4f} ± {row['se_difference']:.4f} {significance}\")\n",
    "  \n",
    "    # Save all results\n",
    "    all_results_df.to_csv('triangle_shift_evaluation_results.csv', index=False)\n",
    "    print(f\"\\n✓ Results saved to 'triangle_shift_evaluation_results.csv'\")\n",
    "else:\n",
    "    print(\"No successful evaluations completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
