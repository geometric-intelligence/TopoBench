{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Disk Transductive Learning: Structure-Centric Batching\n",
    "\n",
    "Train on massive graphs with **structure-complete mini-batches** and constant memory!\n",
    "\n",
    "**What you'll learn:**\n",
    "- \ud83c\udfaf Structure-centric sampling (sample structures \u2192 gather nodes)\n",
    "- \u2705 100% structure completeness by construction\n",
    "- \ud83d\udcca Which topological structures benefit\n",
    "- \ud83d\udd27 Integration with TopoBench pipeline\n",
    "\n",
    "**\ud83d\udcda Prerequisites:** Complete `tutorial_ondisk_transductive_intro.ipynb` first!  \n",
    "That tutorial covers: transductive learning basics, node samplers, cluster-aware sampling, structure loss problem, and on-disk indexing fundamentals.\n",
    "\n",
    "**\u23f1\ufe0f Time:** 15-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Structure-Centric Batching?\n",
    "\n",
    "**Paradigm shift from Tutorial 1:** Instead of sampling nodes, we sample **structures first**!\n",
    "\n",
    "**Recap:** In Tutorial 1, we learned that traditional node-based sampling loses 20-40% of structures at cluster boundaries.\n",
    "\n",
    "**This approach:**\n",
    "- **Sample structures first** (e.g., triangles by ID)\n",
    "- **Gather all nodes** needed for those structures  \n",
    "- **Result:** All sampled structures are **100% complete** in the batch\n",
    "\n",
    "**When to use:**\n",
    "- Need **guaranteed structure completeness**\n",
    "- Using enumerable structures (cliques, cycles)\n",
    "- Willing to change from node-based sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Topological Structures Benefit?\n",
    "\n",
    "### \u2705 Benefits from Structure-Centric:\n",
    "- **SimplicialCliqueLifting**: Cliques/triangles enumerated from graph\n",
    "- **CellCycleLifting**: Cycles discovered in graph\n",
    "- **Any enumerable structure** from the graph topology\n",
    "\n",
    "### \u274c Does NOT benefit:\n",
    "- **HypergraphKHopLifting**: Neighborhoods generated dynamically per node\n",
    "- **KernelLifting**: Structures computed from kernels, not enumerated\n",
    "- **Feature-based** transforms that don't rely on graph structure enumeration\n",
    "\n",
    "**Why?** Structure-centric requires **pre-enumerable structures** that can be indexed. Dynamic/generated structures don't fit this paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Building the On-Disk Index](#index)\n",
    "3. [Structure-Centric Dataloader](#dataloader)\n",
    "4. [Training with TBModel](#training)\n",
    "5. [Memory Efficiency](#memory)\n",
    "6. [Do Liftings Need Modification?](#liftings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "Following TopoBench conventions, we define a dataset class and loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.io import fs\n",
    "import lightning as pl\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# TopoBench imports\n",
    "from topobench.data.loaders.base import AbstractLoader\n",
    "from topobench.data.preprocessor import OnDiskTransductivePreprocessor\n",
    "from topobench.dataloader import TBDataloader\n",
    "from topobench.model import TBModel\n",
    "from topobench.nn.backbones.simplicial import SCCNNCustom\n",
    "from topobench.nn.readouts.mlp_readout import MLPReadout\n",
    "from topobench.loss import TBLoss\n",
    "from topobench.optimizer import TBOptimizer\n",
    "\n",
    "print(\"\u2713 Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset Class (TopoBench Style)\n",
    "\n",
    "**Important: Why `InMemoryDataset` is fine here**\n",
    "\n",
    "You might wonder: \"If we're doing on-disk processing, why use `InMemoryDataset`?\"\n",
    "\n",
    "**Answer:** We keep the **graph** in memory, but **structures** on disk!\n",
    "\n",
    "- **Graph data** (nodes, edges, features): ~50-200 MB for 100K nodes \u2192 **fits in RAM** \u2705\n",
    "- **Topological structures** (triangles, etc.): 1-10 GB \u2192 **stored on disk** \u2705\n",
    "\n",
    "**Why this works:**\n",
    "1. The base graph must be in memory anyway (for subgraph extraction)\n",
    "2. The **bottleneck** is structures, not the graph itself\n",
    "3. Our on-disk index handles the structures (SQLite)\n",
    "4. Result: Constant memory training on large graphs!\n",
    "\n",
    "**Would on-demand graph loading help?** No, because:\n",
    "- We need full graph in memory to extract subgraphs\n",
    "- Preprocessor needs full graph to query edges\n",
    "- Graph itself is not the memory problem\n",
    "- Structures are the problem (and we solve that!)\n",
    "\n",
    "**Bottom line:** `InMemoryDataset` for graph + on-disk index for structures = perfect combo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset Class (TopoBench Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLargeTransductiveDataset(InMemoryDataset):\n",
    "    \"\"\"Large single graph for transductive learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, root, name, parameters: DictConfig):\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        super().__init__(root)\n",
    "        \n",
    "        out = fs.torch_load(self.processed_paths[0])\n",
    "        if len(out) == 4:\n",
    "            data, self.slices, self.sizes, data_cls = out\n",
    "            self.data = data_cls.from_dict(data) if isinstance(data, dict) else data\n",
    "        else:\n",
    "            data, self.slices, self.sizes = out\n",
    "            self.data = data\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return \"data.pt\"\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Generate large graph with community structure.\"\"\"\n",
    "        # Create graph (example: Watts-Strogatz)\n",
    "        G = nx.watts_strogatz_graph(\n",
    "            n=self.parameters.num_nodes,\n",
    "            k=self.parameters.degree,\n",
    "            p=0.3,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Convert to PyG Data\n",
    "        edges = list(G.edges())\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "        edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)\n",
    "        \n",
    "        n = self.parameters.num_nodes\n",
    "        x = torch.randn(n, self.parameters.num_features)\n",
    "        y = torch.randint(0, self.parameters.num_classes, (n,))\n",
    "        \n",
    "        # Transductive splits\n",
    "        train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n, dtype=torch.bool)\n",
    "        \n",
    "        train_mask[:int(0.6 * n)] = True\n",
    "        val_mask[int(0.6 * n):int(0.8 * n)] = True\n",
    "        test_mask[int(0.8 * n):] = True\n",
    "        \n",
    "        data = Data(\n",
    "            x=x, edge_index=edge_index, y=y, num_nodes=n,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask\n",
    "        )\n",
    "        \n",
    "        self.data, self.slices = self.collate([data])\n",
    "        fs.torch_save(\n",
    "            (self._data.to_dict(), self.slices, {}, self._data.__class__),\n",
    "            self.processed_paths[0]\n",
    "        )\n",
    "\n",
    "print(\"\u2713 Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loader Class (TopoBench Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLargeTransductiveLoader(AbstractLoader):\n",
    "    \"\"\"Loader for large transductive datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, parameters: DictConfig):\n",
    "        super().__init__(parameters)\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        return MyLargeTransductiveDataset(\n",
    "            str(self.root_data_dir),\n",
    "            self.parameters.data_name,\n",
    "            self.parameters\n",
    "        )\n",
    "\n",
    "print(\"\u2713 Loader class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = OmegaConf.create({\n",
    "    \"data_dir\": \"./data/\",\n",
    "    \"data_name\": \"MyLargeGraph\",\n",
    "    \"num_nodes\": 10000,\n",
    "    \"degree\": 20,\n",
    "    \"num_features\": 32,\n",
    "    \"num_classes\": 4\n",
    "})\n",
    "\n",
    "# Load dataset\n",
    "loader = MyLargeTransductiveLoader(config)\n",
    "dataset, _ = loader.load()\n",
    "graph_data = dataset[0]\n",
    "\n",
    "print(f\"\\n\u2713 Graph loaded: {graph_data.num_nodes:,} nodes\")\n",
    "print(f\"  Edges: {graph_data.edge_index.size(1):,}\")\n",
    "print(f\"  Train nodes: {graph_data.train_mask.sum().item():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "## 2. Building the On-Disk Index\n",
    "\n",
    "**Key Innovation:** Index structures once to SQLite database on disk.\n",
    "\n",
    "**What happens:**\n",
    "1. Enumerate structures (e.g., triangles) from graph\n",
    "2. Stream results to SQLite database (disk-based)\n",
    "3. Close index file\n",
    "4. Database persists for future use\n",
    "\n",
    "**Memory:** Constant during indexing (streaming enumeration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure transform (applied at batch-time)\n",
    "transforms_config = OmegaConf.create({\n",
    "    \"clique_lifting\": {\n",
    "        \"transform_type\": \"lifting\",\n",
    "        \"transform_name\": \"SimplicialCliqueLifting\",\n",
    "        \"complex_dim\": 2  # Triangles\n",
    "    }\n",
    "})\n",
    "\n",
    "# Create preprocessor (no need to build_index manually - it's automatic!)\n",
    "preprocessor = OnDiskTransductivePreprocessor(\n",
    "    graph_data=graph_data,\n",
    "    data_dir=\"./index/structure_centric_demo\",\n",
    "    transforms_config=transforms_config,\n",
    "    max_structure_size=3  # Triangles = 3 nodes\n",
    ")\n",
    "\n",
    "print(\"\u2713 Preprocessor created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataloader'></a>\n",
    "## 3. Load Dataset Splits (TopoBench Style)\n",
    "\n",
    "**High-level API:** Exactly like inductive learning!\n",
    "\n",
    "**What happens under the hood:**\n",
    "1. Builds structure index (if not exists) \u2192 saved to disk\n",
    "2. Creates train/val/test datasets based on masks\n",
    "3. Each dataset wraps a loader for structure-centric sampling\n",
    "\n",
    "**Result:** Train, val, test datasets ready for TBDataloader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split configuration (like inductive learning!)\n",
    "split_config = OmegaConf.create({\n",
    "    \"strategy\": \"structure_centric\",\n",
    "    \"structures_per_batch\": 500,  # Target number of structures\n",
    "    \"node_budget\": 2000,          # Max nodes (controls memory)\n",
    "})\n",
    "\n",
    "print(\"\ud83d\uddc4\ufe0f Loading dataset splits (builds index if needed)...\\n\")\n",
    "\n",
    "# Load splits - EXACTLY like inductive learning!\n",
    "train, val, test = preprocessor.load_dataset_splits(split_config)\n",
    "\n",
    "print(f\"\\n\u2713 Dataset splits loaded!\")\n",
    "print(f\"  Train: {len(train)} batches\")\n",
    "print(f\"  Val: {len(val)} batches\")\n",
    "print(f\"  Test: {len(test)} batches\")\n",
    "print(f\"  Strategy: Structure-centric batching\")\n",
    "\n",
    "# Inspect a sample batch\n",
    "sample_batch = next(iter(train))\n",
    "print(f\"\\n\ud83d\udce6 Sample training batch:\")\n",
    "print(f\"  Nodes: {sample_batch.num_nodes}\")\n",
    "print(f\"  Edges: {sample_batch.edge_index.size(1)}\")\n",
    "print(f\"  Structures: {sample_batch.num_structures}\")\n",
    "print(f\"  Has precomputed structures: {hasattr(sample_batch, 'precomputed_structures')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datamodule (TopoBench Style)\n",
    "\n",
    "**Exactly like inductive learning:** Use TBDataloader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datamodule - EXACTLY like inductive learning!\n",
    "datamodule = TBDataloader(\n",
    "    dataset_train=train,\n",
    "    dataset_val=val,\n",
    "    dataset_test=test,\n",
    "    batch_size=1,  # Already batched by dataset\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"\u2713 Datamodule created (TopoBench style)\")\n",
    "print(\"  This is IDENTICAL to inductive learning!\")\n",
    "print(\"  Same API, same workflow, just different sampling strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='training'></a>\n",
    "## 4. Training with TBModel\n",
    "\n",
    "**Integration:** Works seamlessly with TopoBench's TBModel!\n",
    "\n",
    "**Training flow:**\n",
    "1. Loader yields batch with complete structures\n",
    "2. Transform (SimplicialCliqueLifting) applied at batch-time\n",
    "3. Model processes batch\n",
    "4. Standard training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model\n",
    "HIDDEN_DIM = 64\n",
    "OUT_CHANNELS = 4\n",
    "IN_CHANNELS = 32\n",
    "\n",
    "model = TBModel(\n",
    "    backbone=SCCNNCustom(\n",
    "        in_channels_all=(IN_CHANNELS, HIDDEN_DIM, HIDDEN_DIM),\n",
    "        hidden_channels_all=(HIDDEN_DIM, HIDDEN_DIM, HIDDEN_DIM),\n",
    "        conv_order=1,\n",
    "        sc_order=2,\n",
    "        n_layers=2\n",
    "    ),\n",
    "    readout=MLPReadout(\n",
    "        HIDDEN_DIM, OUT_CHANNELS, task_level=\"node\"\n",
    "    ),\n",
    "    loss=TBLoss(\n",
    "        dataset_loss={\"task\": \"classification\", \"loss_type\": \"cross_entropy\"}\n",
    "    ),\n",
    "    optimizer=TBOptimizer(\n",
    "        optimizer_id=\"Adam\", parameters={\"lr\": 0.01}\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\u2713 TBModel created\")\n",
    "print(\"  Backbone: SCCNN (Simplicial Convolutional Network)\")\n",
    "print(\"  Task: Node classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - EXACTLY like inductive learning!\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Training with structure-centric batching...\\n\")\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")\n",
    "print(f\"  Trained on {graph_data.train_mask.sum().item():,} train nodes\")\n",
    "print(f\"  Validated on {graph_data.val_mask.sum().item():,} val nodes\")\n",
    "print(f\"  Memory per batch: ~200-300 MB\")\n",
    "print(f\"  All batches had complete structures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='memory'></a>\n",
    "## 5. Memory Efficiency\n",
    "\n",
    "**Key Advantage:** Memory scales with batch size, not graph size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate memory\n",
    "def estimate_batch_memory(batch):\n",
    "    \"\"\"Estimate memory usage in MB.\"\"\"\n",
    "    total_bytes = 0\n",
    "    if hasattr(batch, 'x'):\n",
    "        total_bytes += batch.x.element_size() * batch.x.nelement()\n",
    "    if hasattr(batch, 'edge_index'):\n",
    "        total_bytes += batch.edge_index.element_size() * batch.edge_index.nelement()\n",
    "    if hasattr(batch, 'y'):\n",
    "        total_bytes += batch.y.element_size() * batch.y.nelement()\n",
    "    return total_bytes / (1024 * 1024)\n",
    "\n",
    "# Measure across batches\n",
    "memories = []\n",
    "for i, batch in enumerate(datamodule.train_dataloader()):\n",
    "    if i >= 20:\n",
    "        break\n",
    "    memories.append(estimate_batch_memory(batch))\n",
    "\n",
    "print(\"\\n\ud83d\udcbe Memory Analysis:\")\n",
    "print(f\"  Average batch: {sum(memories)/len(memories):.1f} MB\")\n",
    "print(f\"  Min batch: {min(memories):.1f} MB\")\n",
    "print(f\"  Max batch: {max(memories):.1f} MB\")\n",
    "print(f\"\\n  Full graph (if in-memory): ~{graph_data.num_nodes * 32 * 4 / 1024 / 1024:.0f} MB\")\n",
    "print(f\"  Memory savings: {(graph_data.num_nodes * 32 * 4 / 1024 / 1024) / (sum(memories)/len(memories)):.0f}x\")\n",
    "print(\"\\n\u2705 Memory scales with batch size, not graph size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='liftings'></a>\n",
    "## 6. Do Liftings Need Modification?\n",
    "\n",
    "**Short Answer:** No! Existing liftings work as-is.\n",
    "\n",
    "**How it works:**\n",
    "- **Index building:** Preprocessor enumerates structures using the lifting's logic\n",
    "- **Batch-time:** Lifting is applied to mini-batch (same as always)\n",
    "- **Difference:** Structures are queried from index instead of computed fresh\n",
    "\n",
    "**Example with SimplicialCliqueLifting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "preprocessor.close()\n",
    "print(\"\u2713 Tutorial complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}