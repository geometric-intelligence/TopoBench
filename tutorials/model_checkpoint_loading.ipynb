{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f552bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbg141/.conda/envs/tb/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from experiment: cluster_variance\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "# CONFIGURATION\n",
    "EXPERIMENTS_TO_LOAD = ['cluster_variance']#, 'homophily_range']  # Multiple experiments\n",
    "METRIC = 'val/accuracy'  # Options: 'test/accuracy', 'test/auroc', 'test/precision', 'test/recall', etc.\n",
    "\n",
    "run_to_api_dict = {'avg_degree_range': 'graphuniverse/final_degree_experiments',\n",
    "                   'homophily_range': 'graphuniverse/final_homophily_experiments',\n",
    "                   'num_nodes_range': 'graphuniverse/final_num_nodes_experiments',\n",
    "                   'degree_separation_range': 'graphuniverse/final_triangle_experiments',\n",
    "                   'cluster_variance': 'graphuniverse/final_cluster_variance_experiments',}\n",
    "\n",
    "# Initialize wandb API\n",
    "api = wandb.Api(timeout=100)\n",
    "\n",
    "# Function to safely extract nested values\n",
    "def safe_get_nested(obj, keys, default=None):\n",
    "    \"\"\"Safely get nested dictionary values\"\"\"\n",
    "    try:\n",
    "        for key in keys:\n",
    "            obj = obj[key]\n",
    "        return obj\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        return default\n",
    "\n",
    "def dict_to_sorted_string(d):\n",
    "    \"\"\"Convert nested dict to consistent sorted string\"\"\"\n",
    "    if d is None:\n",
    "        return \"None\"\n",
    "    return json.dumps(d, sort_keys=True, separators=(',', ':'))\n",
    "\n",
    "def dict_to_sorted_string_no_seed(d, seed_keys=['seed', 'random_seed', 'random_state']):\n",
    "    \"\"\"Convert nested dict to consistent sorted string, excluding seed-related keys\"\"\"\n",
    "    if d is None:\n",
    "        return \"None\"\n",
    "    \n",
    "    return json.dumps(d, sort_keys=True, separators=(',', ':'))\n",
    "\n",
    "# Collect data from all experiments\n",
    "all_data = []\n",
    "for experiment_name in EXPERIMENTS_TO_LOAD:\n",
    "    print(f\"Loading data from experiment: {experiment_name}\")\n",
    "    \n",
    "    # Get all runs from this experiment\n",
    "    runs = api.runs(run_to_api_dict[experiment_name])\n",
    "    \n",
    "    for run in runs:\n",
    "        # Get summary metrics\n",
    "        summary = run.summary._json_dict\n",
    "        summary['run_name'] = run.name\n",
    "        summary['run_id'] = run.id\n",
    "        summary['experiment_type'] = experiment_name  # Add experiment identifier\n",
    "        \n",
    "        # Add config parameters with 'config_' prefix\n",
    "        config = run.config\n",
    "        for key, value in config.items():\n",
    "            summary[f'config_{key}'] = value\n",
    "        \n",
    "        all_data.append(summary)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9636c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing experiment: cluster_variance\n",
      "\n",
      "Selected best models based on metric: val/accuracy\n",
      "Processed experiments: ['cluster_variance']\n",
      "\n",
      "Found 2 data configurations with best models\n",
      "\n",
      "cluster_variance_0.2:\n",
      "  DeepSet: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_299-v27.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_433-v19.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_452-v5.ckpt\n",
      "  GPS: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_281-v25.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_130-v113.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_245-v23.ckpt\n",
      "  GraphMLP: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_339-v25.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_467-v5.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_340-v32.ckpt\n",
      "  GraphSAGE: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_320-v21.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_238-v27.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_246-v30.ckpt\n",
      "  gat: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_206-v21.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_146-v124.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_199-v60.ckpt\n",
      "  gcn: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_194-v70.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_205-v25.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_163-v42.ckpt\n",
      "  gin: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_347-v32.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_190-v49.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_226-v23.ckpt\n",
      "  nsd: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_172-v77.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_242-v22.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_161-v53.ckpt\n",
      "  topotune: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_332-v27.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_383-v26.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_364-v17.ckpt\n",
      "\n",
      "cluster_variance_0.8:\n",
      "  DeepSet: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_332-v25.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_427-v15.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_297-v34.ckpt\n",
      "  GPS: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_061-v127.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_075-v141.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_077-v166.ckpt\n",
      "  GraphMLP: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_360-v16.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_375-v15.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_315-v11.ckpt\n",
      "  GraphSAGE: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_183-v63.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_155-v77.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_137-v111.ckpt\n",
      "  gat: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_116-v93.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_146-v126.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_160-v53.ckpt\n",
      "  gcn: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_132-v86.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_131-v95.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_197-v59.ckpt\n",
      "  gin: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_105-v110.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_116-v92.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_143-v129.ckpt\n",
      "  nsd: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_150-v57.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_141-v108.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_230-v36.ckpt\n",
      "  topotune: 3 checkpoint(s)\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_251-v8.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_255-v16.ckpt\n",
      "    - /data/gbg141/TB/outputs/checkpoints/epoch_422-v11.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Extract parameters for each experiment type\n",
    "def extract_params_multi(gen_params_str, experiment_type):\n",
    "    try:\n",
    "        parsed = json.loads(gen_params_str)\n",
    "        n_graphs = parsed['family_parameters']['n_graphs']\n",
    "        \n",
    "        # Get the appropriate parameter based on experiment type\n",
    "        if experiment_type == 'avg_degree_range':\n",
    "            varied_param_value = tuple(parsed['family_parameters']['avg_degree_range'])\n",
    "        elif experiment_type == 'homophily_range':\n",
    "            varied_param_value = tuple(parsed['family_parameters']['homophily_range'])\n",
    "        elif experiment_type == 'degree_separation_range':\n",
    "            varied_param_value = tuple(parsed['family_parameters']['degree_separation_range'])\n",
    "        elif experiment_type == 'num_nodes_range':\n",
    "            varied_param_value = tuple(parsed['num_nodes_range'])\n",
    "        elif experiment_type == 'cluster_variance':\n",
    "            varied_param_value = parsed['universe_parameters']['cluster_variance']\n",
    "        else:\n",
    "            return None, None, None\n",
    "            \n",
    "        return n_graphs, varied_param_value, experiment_type\n",
    "    except:\n",
    "        return None, None, None\n",
    "\n",
    "def extract_transform_info(config_transforms, model_name):\n",
    "    \"\"\"Extract relevant transform information based on model type\"\"\"\n",
    "    if pd.isna(config_transforms) or config_transforms is None:\n",
    "        return \"no_transform\"\n",
    "    \n",
    "    if model_name in ['GPS', 'nsd']:\n",
    "        # Extract encodings for GPS and NSD\n",
    "        if 'CombinedPSEs' in config_transforms:\n",
    "            encodings = config_transforms['CombinedPSEs'].get('encodings', [])\n",
    "            if encodings:\n",
    "                return '_'.join(sorted(encodings))  # Sort for consistency\n",
    "        return \"no_encoding\"\n",
    "    \n",
    "    elif model_name == 'topotune':\n",
    "        return \"cell_lifting\"\n",
    "    \n",
    "    else:\n",
    "        return \"no_transform\"\n",
    "\n",
    "\n",
    "# Create string representations for sorting\n",
    "df['generation_params_str'] = df['config_dataset'].apply(\n",
    "    lambda x: dict_to_sorted_string(safe_get_nested(x, ['loader', 'parameters', 'generation_parameters']))\n",
    ")\n",
    "\n",
    "\n",
    "df['model_name'] = df['config_model'].apply(\n",
    "    lambda x: safe_get_nested(x, ['model_name'])\n",
    ")\n",
    "\n",
    "# Sort by generation parameters first, then model name second\n",
    "df_sorted = df.sort_values(['generation_params_str', 'model_name'])\n",
    "\n",
    "# Extract transform info separately\n",
    "df_sorted['transform_info'] = df_sorted.apply(\n",
    "    lambda row: extract_transform_info(row.get('config_transforms'), row['model_name']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create enhanced model config string that includes PE info for GPS/NSD\n",
    "df_sorted['model_config_str'] = df_sorted.apply(\n",
    "    lambda row: (dict_to_sorted_string_no_seed(row['config_model']) + f\"_PE_{row['transform_info']}\") \n",
    "                if row['model_name'] in ['GPS', 'nsd'] \n",
    "                else dict_to_sorted_string_no_seed(row['config_model']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filter out rows with missing essential data\n",
    "df_clean = df_sorted.dropna(subset=['generation_params_str', 'model_config_str', METRIC, 'checkpoint'])\n",
    "df_clean = df_clean[df_clean['generation_params_str'] != \"None\"]\n",
    "\n",
    "df_clean[['n_graphs', 'varied_param_value', 'experiment_type_check']] = df_clean.apply(\n",
    "    lambda row: pd.Series(extract_params_multi(row['generation_params_str'], row['experiment_type'])), axis=1\n",
    ")\n",
    "\n",
    "df_clean = df_clean.dropna(subset=['n_graphs', 'varied_param_value'])\n",
    "\n",
    "# Get unique values\n",
    "unique_models = sorted(df_clean['model_name'].unique())\n",
    "\n",
    "# Dictionary to store the best models for ALL experiments\n",
    "best_models_dict = {}\n",
    "\n",
    "# Process each experiment type separately\n",
    "for experiment_type in EXPERIMENTS_TO_LOAD:\n",
    "    print(f\"\\nProcessing experiment: {experiment_type}\")\n",
    "    \n",
    "    # Filter data for this experiment\n",
    "    experiment_data = df_clean[df_clean['experiment_type'] == experiment_type]\n",
    "    \n",
    "    if len(experiment_data) == 0:\n",
    "        print(f\"No data found for experiment: {experiment_type}\")\n",
    "        continue\n",
    "    \n",
    "    unique_varied_param_values = sorted(experiment_data['varied_param_value'].unique())\n",
    "    \n",
    "    # Process each varied parameter value\n",
    "    for varied_param_value in unique_varied_param_values:\n",
    "        subset = experiment_data[experiment_data['varied_param_value'] == varied_param_value]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            # Create data config key that includes experiment type\n",
    "            data_config_key = f\"{experiment_type}_{varied_param_value}\"\n",
    "            best_models_dict[data_config_key] = {}\n",
    "            \n",
    "            # For each model architecture, find the best configuration\n",
    "            for model_name in unique_models:\n",
    "                model_subset = subset[subset['model_name'] == model_name]\n",
    "                \n",
    "                if len(model_subset) > 0:\n",
    "                    # Calculate mean performance for each unique model configuration\n",
    "                    model_config_performance = model_subset.groupby('model_config_str').agg({\n",
    "                        METRIC: ['mean', 'std', 'count'],\n",
    "                        'checkpoint': lambda x: list(x)  # Collect all checkpoint paths\n",
    "                    }).reset_index()\n",
    "                    \n",
    "                    # Flatten column names\n",
    "                    model_config_performance.columns = [\n",
    "                        'model_config_str', \n",
    "                        f'{METRIC}_mean', \n",
    "                        f'{METRIC}_std', \n",
    "                        f'{METRIC}_count',\n",
    "                        'checkpoints'\n",
    "                    ]\n",
    "                    \n",
    "                    # Find the configuration with the best mean performance\n",
    "                    if len(model_config_performance) > 0:\n",
    "                        if \"accuracy\" in METRIC:\n",
    "                            best_config_idx = model_config_performance[f'{METRIC}_mean'].idxmax()\n",
    "                        else:\n",
    "                            best_config_idx = model_config_performance[f'{METRIC}_mean'].idxmin()\n",
    "                        best_checkpoints = model_config_performance.loc[best_config_idx, 'checkpoints']\n",
    "                        \n",
    "                        # Store the checkpoint locations for this model architecture\n",
    "                        best_models_dict[data_config_key][model_name] = best_checkpoints\n",
    "\n",
    "# Clean up empty entries\n",
    "best_models_dict = {k: v for k, v in best_models_dict.items() if v}\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nSelected best models based on metric: {METRIC}\")\n",
    "print(f\"Processed experiments: {EXPERIMENTS_TO_LOAD}\")\n",
    "print(f\"\\nFound {len(best_models_dict)} data configurations with best models\")\n",
    "\n",
    "for data_config, models in best_models_dict.items():\n",
    "    print(f\"\\n{data_config}:\")\n",
    "    for model_name, checkpoints in models.items():\n",
    "        print(f\"  {model_name}: {len(checkpoints)} checkpoint(s)\")\n",
    "        for checkpoint in checkpoints:\n",
    "            print(f\"    - {checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd213d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_299-v27.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/DeepSet\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_433-v19.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/DeepSet\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_452-v5.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/DeepSet\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_281-v25.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GPS\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_130-v113.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GPS\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_245-v23.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GPS\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_339-v25.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GraphMLP\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_467-v5.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GraphMLP\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_340-v32.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GraphMLP\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_320-v21.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GraphSAGE\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_238-v27.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GraphSAGE\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_246-v30.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/GraphSAGE\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_206-v21.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gat\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_146-v124.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gat\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_199-v60.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gat\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_194-v70.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gcn\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_205-v25.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gcn\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_163-v42.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gcn\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_347-v32.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gin\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_190-v49.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gin\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_226-v23.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/gin\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_172-v77.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/nsd\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_242-v22.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/nsd\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_161-v53.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/nsd\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_332-v27.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/topotune\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_383-v26.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/topotune\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_364-v17.ckpt to cluster_variance_checkpoints/cluster_variance_0.2/topotune\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_332-v25.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/DeepSet\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_427-v15.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/DeepSet\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_297-v34.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/DeepSet\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_061-v127.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GPS\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_075-v141.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GPS\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_077-v166.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GPS\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_360-v16.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GraphMLP\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_375-v15.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GraphMLP\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_315-v11.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GraphMLP\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_183-v63.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GraphSAGE\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_155-v77.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GraphSAGE\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_137-v111.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/GraphSAGE\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_116-v93.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gat\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_146-v126.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gat\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_160-v53.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gat\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_132-v86.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gcn\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_131-v95.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gcn\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_197-v59.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gcn\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_105-v110.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gin\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_116-v92.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gin\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_143-v129.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/gin\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_150-v57.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/nsd\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_141-v108.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/nsd\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_230-v36.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/nsd\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_251-v8.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/topotune\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_255-v16.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/topotune\n",
      "Copied /data/gbg141/TB/outputs/checkpoints/epoch_422-v11.ckpt to cluster_variance_checkpoints/cluster_variance_0.8/topotune\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the output directory where you want to organize the checkpoints\n",
    "OUTPUT_DIR = f\"{EXPERIMENTS_TO_LOAD[0]}_checkpoints\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through the best_models_dict and copy files\n",
    "for data_config, models in best_models_dict.items():\n",
    "    for model_name, checkpoints in models.items():\n",
    "        # Create a subfolder for each data_config and model_name\n",
    "        subfolder = os.path.join(OUTPUT_DIR, str(data_config), str(model_name))\n",
    "        os.makedirs(subfolder, exist_ok=True)\n",
    "        for checkpoint_path in checkpoints:\n",
    "            if os.path.isfile(checkpoint_path):\n",
    "                # Copy the checkpoint file into the subfolder\n",
    "                shutil.copy2(checkpoint_path, subfolder)\n",
    "                print(f\"Copied {checkpoint_path} to {subfolder}\")\n",
    "            else:\n",
    "                print(f\"WARNING: Checkpoint file not found: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05bc2dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_models_dict to cluster_variance_checkpoints/best_models_dict.json\n",
      "Compressed folder created: cluster_variance_checkpoints.zip\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save the best_models_dict as a JSON file in the output directory\n",
    "with open(os.path.join(OUTPUT_DIR, \"best_models_dict.json\"), \"w\") as f:\n",
    "    json.dump(best_models_dict, f, indent=2)\n",
    "print(f\"Saved best_models_dict to {os.path.join(OUTPUT_DIR, 'best_models_dict.json')}\")\n",
    "\n",
    "# Compress the organized checkpoints folder after copying\n",
    "shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)\n",
    "print(f\"Compressed folder created: {OUTPUT_DIR}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e56261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp -R gbg141@bobby.ece.ucsb.edu:/home/gbg141/TopoBench-1/tutorials/cluster_variance_checkpoints.zip .\n",
      "scp -R gbg141@daisy.ece.ucsb.edu:/home/gbg141/TopoBench/tutorials/cluster_variance_checkpoints.zip .\n"
     ]
    }
   ],
   "source": [
    "print(f\"scp -R gbg141@bobby.ece.ucsb.edu:/home/gbg141/TopoBench-1/tutorials/{OUTPUT_DIR}.zip .\")\n",
    "print(f\"scp -R gbg141@daisy.ece.ucsb.edu:/home/gbg141/TopoBench/tutorials/{OUTPUT_DIR}.zip .\")\n",
    "# Run scp -R gbg141@daisy.ece.ucsb.edu:/home/gbg141/TopoBench/tutorials/selected_checkpoints.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34ffef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3b7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
