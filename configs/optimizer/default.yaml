_target_: topobench.optimizer.TBOptimizer
# Full compatibility with all available torch optimizers and schedulers

optimizer_id: Adam # torch id of the optimizer
# optimizer params
parameters:
  lr: 0.001
  weight_decay: 0.0

# scheduler: # Comment or delete these lines to disable the scheduler
#   scheduler_id: StepLR # torch id of the scheduler. Set to null to disable.
#   # scheduler params
#   scheduler_params:
#     step_size: 50
#     gamma: 0.5



scheduler: # Comment or delete these lines to disable the scheduler
  scheduler_id: ReduceLROnPlateau # torch id of the scheduler. Set to null to disable.
  monitor: ${get_monitor_metric:${dataset.parameters.task},${dataset.parameters.monitor_metric}} # metric to monitor for ReduceLROnPlateau
  mode: ${get_monitor_mode:${dataset.parameters.task}}"
  interval: epoch      # Optional: usually defaults to epoch for Plateau
  frequency: ${trainer.check_val_every_n_epoch}         # Optional
  # scheduler params
  scheduler_params:
    patience: 3
    factor: 0.9
